<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>公务员行测-常识-人文与历史</title>
    <url>/2020/04/24/%E5%85%AC%E5%8A%A1%E5%91%98/%E5%85%AC%E5%8A%A1%E5%91%98%E8%A1%8C%E6%B5%8B-%E5%B8%B8%E8%AF%86-%E4%BA%BA%E6%96%87%E4%B8%8E%E5%8E%86%E5%8F%B2/</url>
    <content><![CDATA[<h3 id="诸子百家"><a href="#诸子百家" class="headerlink" title="诸子百家"></a>诸子百家</h3><h4 id="儒家"><a href="#儒家" class="headerlink" title="儒家"></a>儒家</h4><p><font class="attention">仁、礼、德、义、教学</font></p>
<h5 id="孔子"><a href="#孔子" class="headerlink" title="孔子"></a>孔子</h5><p>春秋时期，尊称有：至圣、万世师表，郁郁不得志——育人</p>
<ol>
<li>思想</li>
</ol>
<ul>
<li>礼——西周之礼等级有序——“克己复礼”，“君君、臣臣、父父、子子”</li>
<li>任——“爱人”，“因民之所利而利之”，“己所不欲勿施于人”</li>
<li>中庸——不偏不倚，折中调和——“过犹不及”，“和而不同”</li>
<li>德——“为政以德，譬如北辰，居其所而众星共之”<br>利于统治，汉武帝开始成为正统思想</li>
</ul>
<ol start="2">
<li>教育</li>
</ol>
<ul>
<li>对象——有教无类（不问出身）；兴办私学（民间教育）</li>
<li>态度——不耻下问；知之为知之，不知为不知</li>
<li>方法——学（韦编三绝，勤学）、思、习、行；循循善诱；因材施教；不愤不启、不悱不发（启发式教育）；举一反三</li>
<li>内容——<font class="strong">《诗》《书》《礼》《易》《春秋》</font>《乐》（后来《乐》失传，只剩下五经）</li>
</ul>
<ol start="3">
<li>著作<br>《论语》——记录孔子<font class="attention">及其弟子</font>言行的一本书，<font class="attention">孔子弟子、在传弟子</font>编著。<br>宋代<font class="attention">朱熹</font>将它与<font class="strong">《大学》《中庸》《孟子》</font>合称为四书。</li>
</ol>
<h5 id="孟子"><a href="#孟子" class="headerlink" title="孟子"></a>孟子</h5><p>战国时期，尊称：亚圣，儒家思想就是孔孟之道，（孟母三迁）</p>
<ol>
<li>思想</li>
</ol>
<ul>
<li>仁政——“威天下不以兵革之利”，“乐以天下，忧以天下”，“天时不如地利，地利不如人和”</li>
<li>义——舍生取义——道义，“得道者多助”（对官），“富贵不能淫，贫贱不能移，威武不能屈”（对民）</li>
<li>民贵君轻——“民为贵，社稷次之，君为轻”</li>
<li>性善论——“恻隐之心，人皆有之”</li>
<li>以德服人——“以德服人者，中心悦诚服也”</li>
</ul>
<ol start="2">
<li>著作<br>《孟子》 ——作者：<font class="attention">孟子及其弟子</font></li>
</ol>
<h5 id="荀子"><a href="#荀子" class="headerlink" title="荀子"></a>荀子</h5><p>战国时期，儒家另类，弟子李斯和韩非子最后成为法家代表人物</p>
<ol>
<li>思想</li>
</ol>
<ul>
<li>性恶论——礼法并用，青出于蓝而胜于蓝《劝学》</li>
<li>“君者，舟也；庶人者，水也。水则载舟，水则覆舟”</li>
<li>先义后利——“先义而后利者荣，先利而后义者辱”</li>
<li>“天行有常（规律，唯物主义）”，“制天命而用之”</li>
</ul>
<ol start="2">
<li>著作<br>《荀子》 ——作者：<font class="attention">荀子及其弟子</font><br>《劝学》</li>
</ol>
<h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><ul>
<li>董仲舒——西汉时期，“罢黜百家独尊儒术”；“大一统”，汉武帝采用他的建议</li>
<li>朱熹——南宋时期，程朱理学（二程，程颢，陈颐-程门立雪），“存天理，灭人欲”</li>
<li>王阳明——明代，“知行合一”，心学</li>
</ul>
<h4 id="道家"><a href="#道家" class="headerlink" title="道家"></a>道家</h4><p><font class="attention">道、辩证、无为、自然</font></p>
<h5 id="老子"><a href="#老子" class="headerlink" title="老子"></a>老子</h5><p>春秋时期，哲学家，<font class="attention">消极避世</font>，道家创始人，和道教不相等，道教东汉才产生，将道家神秘化，称老子为“太上老君”“道祖”。</p>
<ol>
<li>思想</li>
</ol>
<ul>
<li>“道”是世界本源</li>
<li>朴素的辩证法——祸兮福所倚，福兮祸所伏</li>
<li>无为而治——“我无为，而民自化”、“小国寡民”、道法自然（让百姓自己顺其自然）</li>
<li>“上善若水”</li>
</ul>
<ol start="2">
<li>著作<br>《道德经》——作者：老子。</li>
</ol>
<h5 id="庄子"><a href="#庄子" class="headerlink" title="庄子"></a>庄子</h5><p>战国时期，善于讲寓言故事（东施效颦，邯郸学步，庄周梦蝶，庖丁解牛，朝三暮四）</p>
<ol>
<li>思想</li>
</ol>
<ul>
<li>政治观：无为</li>
<li>齐物论：‘天地与我并生，而万物与我为一’</li>
<li>认识关：“逍遥” —— 《逍遥游》<br>“吾生也有涯，而知也无涯”——量力而行</li>
</ul>
<ol start="2">
<li>著作<br>《庄子》，亦称《南华经》——作者：庄子及其后学。   后人称庄子“南华真人”</li>
</ol>
<h5 id="魏晋玄学"><a href="#魏晋玄学" class="headerlink" title="魏晋玄学"></a>魏晋玄学</h5><p>玄学又称新道家，是对《老子》、《庄子》和《周易》的研究和解说，产生于<font class="strong">魏晋</font>。玄学是中国魏晋时期到宋朝中叶之间出现的一种崇尚老庄的思潮。</p>
<h4 id="法家"><a href="#法家" class="headerlink" title="法家"></a>法家</h4><p><font class="attention">法治、变法、君主集权</font></p>
<h5 id="商鞅"><a href="#商鞅" class="headerlink" title="商鞅"></a>商鞅</h5><p>秦国，秦孝公支持</p>
<ul>
<li>“治世不一道，便国不法古”</li>
<li>典故：立木为信——把金子从南门拿到北门就给金子</li>
</ul>
<h5 id="韩非子"><a href="#韩非子" class="headerlink" title="韩非子"></a>韩非子</h5><p>法家集大成者</p>
<ul>
<li>中央集权：“事在四方，要在中央”</li>
<li>法不阿贵：“刑过不避大臣，赏善不遗匹夫”</li>
<li>“法（法制）”“术（统治手段）”“势（君主权势）”结合（三个流派）</li>
</ul>
<h4 id="墨家"><a href="#墨家" class="headerlink" title="墨家"></a>墨家</h4><p><font class="attention">平民、兼爱、非攻、节俭</font></p>
<h5 id="墨子"><a href="#墨子" class="headerlink" title="墨子"></a>墨子</h5><p>门徒很厉害，侠客，组织严明</p>
<ol>
<li>思想<br><font class="attention">兼爱</font>、非攻、节用、节葬、尚贤、非乐等。（利天下）（逆势而动）</li>
<li>著作<br>《墨子》——作者：墨子及其弟子<br>《墨经》——小孔成像，杠杆原理，光学</li>
</ol>
<h4 id="兵家"><a href="#兵家" class="headerlink" title="兵家"></a>兵家</h4><p><font class="attention">打赢就行</font><br>兵武：春秋时期，兵圣，《孙子兵法》（最早兵书）。（区分于《三十六计》）<br>孙斌：战国时期，《孙膑兵法》。桂林之战（围魏救赵）</p>
<h4 id="杂家"><a href="#杂家" class="headerlink" title="杂家"></a>杂家</h4><p><font class="attention">啥都研究</font><br>“兼儒墨，合名法”<br>吕不韦：战国时期，辅佐秦王，《吕氏春秋》，（一字千金：看谁可以改一个《吕氏春秋》的字）</p>
<p><font class="attention"></font><br><font class="strong"></font></p>
<style>
.attention{color: #f55d42;}
.strong{color: #5a5a96;font-weight: bold;}
.exerise{color: #8d81a6;}
.answer{color: #a295bf;}
</style>]]></content>
      <categories>
        <category>公务员行测</category>
        <category>常识</category>
      </categories>
      <tags>
        <tag>公务员笔试</tag>
        <tag>常识</tag>
      </tags>
  </entry>
  <entry>
    <title>公务员行测-常识-国情和地理</title>
    <url>/2020/04/23/%E5%85%AC%E5%8A%A1%E5%91%98/%E5%85%AC%E5%8A%A1%E5%91%98%E8%A1%8C%E6%B5%8B-%E5%B8%B8%E8%AF%86-%E5%9B%BD%E6%83%85%E5%92%8C%E5%9C%B0%E7%90%86/</url>
    <content><![CDATA[<h3 id="地表塑造"><a href="#地表塑造" class="headerlink" title="地表塑造"></a>地表塑造</h3><h4 id="地球的内部圈层"><a href="#地球的内部圈层" class="headerlink" title="地球的内部圈层"></a>地球的内部圈层</h4><ol>
<li>地壳： 地球球层结构的<font class="strong">最外</font>层。人类生存的场所，厚度约为35-45千米。<br><font class="attention">元素 - 氧&gt;硅&gt;铝</font></li>
<li>地幔： 地壳和地核之间的<font class="strong">中间</font>层，平均厚度为2800多千米。 在地幔和地壳中间有一个<font class="attention">软流层</font>（与大陆板块运动、地震、火山有关，含有矿产）</li>
<li>地核： 地球的<font class="strong">核心</font>部分，主要由铁、镍元素组成，半径为3480千米。<br><font class="attention">三个里面密度最大</font></li>
</ol>
<h4 id="地表的外部圈层"><a href="#地表的外部圈层" class="headerlink" title="地表的外部圈层"></a>地表的外部圈层</h4><p>从外到内包括大气圈、生物圈、水圈。 最活跃的为生物圈。</p>
<h4 id="地壳的物质组成和循环"><a href="#地壳的物质组成和循环" class="headerlink" title="地壳的物质组成和循环"></a>地壳的物质组成和循环</h4><h5 id="岩石的组成"><a href="#岩石的组成" class="headerlink" title="岩石的组成"></a>岩石的组成</h5><p>岩石由矿物组成。（考的很少）</p>
<ol>
<li>矿物：地壳中的化学元素，在一定的地质条件下结合而成的化合物或单质。</li>
<li>矿产： 有用矿物在地壳或地表富集，并且能够被人们开采利用。</li>
<li>岩石： 地壳中的<font class="strong">矿物</font>很少单独存在，他们按照一定的规律<font class="strong">聚集</font>在一起，就形成了岩石。</li>
</ol>
<h5 id="岩石的分类"><a href="#岩石的分类" class="headerlink" title="岩石的分类"></a>岩石的分类</h5><ol>
<li>岩浆岩： 岩浆在压力作用下入侵地壳上部或喷出地表，冷却凝固而形成的岩石。（e.g.玄武岩-搓脚石,花岗岩-颗粒）<br><font class="attention">比例最大</font></li>
<li>沉积岩： 岩石碎屑物质被风、流水等搬运后沉积起来，经过<font class="strong">压紧固结</font>作用而形成的岩石。（e.g.页岩-考古学依据，石灰岩-白色）<br>大的块先沉积</li>
<li>变质岩： 地壳中已经生成的岩石，在岩浆活动、地壳运动中产生的高温、高压条件下使得原来的岩石成分、性质发生变化而形成新的岩石。（e.g.大理岩-石灰岩变质来的，也叫汉白玉）</li>
</ol>
<h5 id="岩石圈的物质循环"><a href="#岩石圈的物质循环" class="headerlink" title="岩石圈的物质循环"></a>岩石圈的物质循环</h5><p><font class="strong">地表形态的塑造也是岩石圈的物质循环过程</font>，他们存在的基础是岩石圈三大岩石之间的相互转化，今天看到的山系、盆地、流水等地貌就是岩石圈物质循环在地表留下的痕迹。<br><font class="attention">同一时期，三种岩石相互转换</font></p>
<h4 id="内力作用和外力作用"><a href="#内力作用和外力作用" class="headerlink" title="内力作用和外力作用"></a>内力作用和外力作用</h4><p>引起地壳变化的是地质作用。按能量来源，可以分为内力作用和外力作用。</p>
<h5 id="内力作用（来自地球内部）"><a href="#内力作用（来自地球内部）" class="headerlink" title="内力作用（来自地球内部）"></a>内力作用（来自地球内部）</h5><ol>
<li>地壳运动<br>塑造地表的主要方式。按照地壳运动的性质和方向，可将其分为水平运动和垂直运动。 </li>
</ol>
<hr>
<p><font class="strong">板块构造学说</font><br><font class="attention">魏格纳</font>提出，地球的岩石不是整体一块而是被一些断裂构造分别成许多单元，这些单元叫做板块（大陆+大洋，一共6大板块），板块相对移动而发生的彼此碰撞或者张裂形成了地球表面的基本面貌。<br><font class="attention">亚欧板块&lt;-印度洋板块导致青藏高原经常地震</font><br>非洲板块、印度洋板块张裂形成有一个东非大裂谷</p>
<hr>
<p><font class="strong">火山分类</font><br>死火山、活火山、休眠火山</p>
<hr>
<p><font class="strong">地震</font>：地球内能以地震波的形式强烈释放，引起一系列振动的现象，地震大小用里氏震级来表示。<br><font class="attention">地震的成因</font>——构造地震、火山地震、塌陷地震、诱发地震<br>震级——用来衡量地震大小的一种度量。每一次地震只有一个震级。震级增加一级，能力释放增加<font class="attention">30</font>倍。<br>烈度——地震影响破坏的程度，可以有多个（对不同实物破坏程度不同）<br>震源——位于地底下，震源越浅，对地面的破坏程度越大，影响范围变小<br>震中——就是震源在地表面的投影<br><font class="attention">三大地震带</font>（有人说是两大）：地中海——喜马拉雅山（也叫欧亚地震带）；环太平洋地震带；海岭地震带<br>（中国位于喜马拉雅和地中海之间）</p>
<hr>
<ol start="2">
<li><p>岩浆活动<br>处于地下深处的岩浆，在巨大的压力作用下，（地热能供能），有时候会沿着地壳的薄弱地带喷出地表，若沿地壳中央喷出或者管道喷出，往往会形成火山。火山死后可能会形成堰塞湖（长白山天池，五大连池）</p>
</li>
<li><p>变质作用<br>岩浆岩、沉积岩或变质岩在地下深入、岩浆侵入的外接触带或地下热液体经过地段，原岩所承受的温度、压力等外部环境发生变化，原岩中的矿物质产生重结晶作用，使其结构、构造和矿物成分发生改变形成新的岩石的过程。</p>
</li>
</ol>
<h5 id="外力作用"><a href="#外力作用" class="headerlink" title="外力作用"></a>外力作用</h5><ol>
<li><p>风化作用<br>地表或者接近地表的岩石、矿物与大气、水及生物接触过程中产生物理、化学变化而在原地形成松散堆积物的全过程。<br><font class="attention">土壤就是风化作用的结果</font><br>有物理风化（热胀冷缩）、化学风化（CO2溶解）、生物风化（植物的根，苔藓）</p>
</li>
<li><p>侵蚀作用（重要）<br><font class="attention">风力、流水</font>等外力在运动状态下改变地面岩石及其风化物的过程。比风化速度快很多。<br>（风蚀蘑菇-雅丹地貌，黄土高原-流水导致，喀斯特地貌-流水侵蚀+CO2-云南石林）</p>
</li>
<li><p>搬运作用<br>地表和近地表的岩屑和溶解质等风化物被外营力搬往他出的过程，是自然界塑造地球表面的重要作用之一。</p>
</li>
<li><p>沉积作用<br>被运动介质搬运的物质到达适宜的场所后，由于条件发生改变而发生的沉积、堆积的过程。<br><font class="attention">黄河下游沉积形成华北平原</font></p>
</li>
</ol>
<h4 id="地表与海底形态"><a href="#地表与海底形态" class="headerlink" title="地表与海底形态"></a>地表与海底形态</h4><p>陆地地形：山地（500米以上）、丘陵（200米）、高原、平原、盆地<br>海底地形（3种）：大陆架、大陆坡、大洋底（洋盆、海沟、海岭）<br><img src="/images/%E6%B5%B7%E5%BA%95%E5%9C%B0%E5%BD%A2%E5%9B%BE.jpg" alt="海底地形图"></p>
<style>
.attention{color: #f55d42;}
.strong{color: #5a5a96;font-weight: bold;}
.exerise{color: #8d81a6;}
.answer{color: #a295bf;}
</style>]]></content>
      <categories>
        <category>公务员行测</category>
        <category>常识</category>
      </categories>
      <tags>
        <tag>公务员笔试</tag>
        <tag>常识</tag>
      </tags>
  </entry>
  <entry>
    <title>北京公务员行测-数量关系数学运算</title>
    <url>/2020/04/21/%E5%85%AC%E5%8A%A1%E5%91%98/%E5%8C%97%E4%BA%AC%E5%85%AC%E5%8A%A1%E5%91%98%E8%A1%8C%E6%B5%8B-%E6%95%B0%E9%87%8F%E5%85%B3%E7%B3%BB%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/</url>
    <content><![CDATA[<p>数量运算的题量为15道（北京市），考察主要体现在基础知识的侧重技巧、高频考点必考、扩展考点隔年考察。</p>
<h3 id="数学运算基础必备"><a href="#数学运算基础必备" class="headerlink" title="数学运算基础必备"></a>数学运算基础必备</h3><h4 id="算术基础知识"><a href="#算术基础知识" class="headerlink" title="算术基础知识"></a>算术基础知识</h4><h5 id="整数的特性"><a href="#整数的特性" class="headerlink" title="整数的特性"></a>整数的特性</h5><ol>
<li>质合性<br>质数：只可以被1和自身整除，不能被其他整数整除。2是唯一的偶质数。<br>合数：除了1和自身外，还能被其他整数整除。</li>
<li>整除的判断<br><font class="exerise">例题：小张的孩子的出生月份乘上29，出生的日期乘上24，所得的两个乘积和为900.问孩子出生在哪一个季度？</font><br><font class="answer"><em>解析： 列方程式为29x+24y=900,24、900都可以被12整除，由整除的加减性可知，29x也可以被12整除，那x只能是12，在第四季度。</em></font></li>
<li>公因数和公倍数<br>公共的因数、公共的倍数</li>
</ol>
<h5 id="比例性质"><a href="#比例性质" class="headerlink" title="比例性质"></a>比例性质</h5><p>自己分份数后记得不能随便约分。　　<br><font class="exerise">例题：某公司年终分红，董事会发放奖励，原本打算依据职位高低按甲乙丙比例为3:2:1的方案进行分配，最终决定根据实际贡献按甲乙丙比例为4:3:2分配。最终方案中（ ）得到的奖金比原有方案有所提升。</font>　　　<br><font class="answer"><em>解析： 第一次总份数为6，第二次总份数为9，最小公倍数为18。换成9:6:3和8:6:4，所以是丙得到的奖金提高了。</em></font></p>
<h4 id="代数工具"><a href="#代数工具" class="headerlink" title="代数工具"></a>代数工具</h4><h5 id="不等式"><a href="#不等式" class="headerlink" title="不等式"></a>不等式</h5><p>两个正数的算术平均数不小于几何平均数，即(a+b)/2 ≥ √(ab)，当且仅当a = b时，等号成立。左边是两个数的算术平均数，右边是几何平均数，这个不等式为<font class="strong">均值不等式</font>。</p>
<ol>
<li>两个正数，当他们和为一个确定的数时，他们相等时乘积最大。</li>
<li>两个正数，当他们积为一个确定的数时，他们相等时和最小。<br><font class="exerise">例题：用40cmX60cm的砖铺一个房间的地，在不破坏方砖的情况下，正好需要用60块。假设该长方形地面的周长的最小值为X米，那么X的值在以下哪个范围内？<br>A.X&lt;15           　　　　B.15≤X&lt;16<br>C.16≤X&lt;17        　　D.X≥17</font><br><font class="answer"><em>长方形地面的面积是 60x0.4x0.6=14.4平方米，根据不等式，长宽和最小值为2√14.4，周长最小值为4√14.4，选项为B。</em></font></li>
</ol>
<h5 id="数列"><a href="#数列" class="headerlink" title="数列"></a>数列</h5><p>等差数列：如果一个数列从第二项起，每一项与它的前一项的差等于同一个常数，这个数列就叫做等差数列。<br>an = a1 + (n-1)d<br>Sn = n*a1+n(n-1)d/2 或 Sn = n(a1+an)/2<br>等比数列：从第二项起，每一项与它的前一项的比值等于同一个常数的一种数列。<br>an=a₁*qⁿ⁻¹<br>Sn = a1(1-qⁿ)/(1-q), q≠1<br>Sn = n*a1, q=1<br><em>注：首项！ 有时候两者混考，等比数列但是要算加法（e.g.微博关注数）</em></p>
<h4 id="实战技巧"><a href="#实战技巧" class="headerlink" title="实战技巧"></a>实战技巧</h4><ol>
<li>特值法<br><font class="exerise">例题：A、B两个容器装有质量相同的酒精溶液，若从A、B中各取一半的溶液，混合后浓度为45%；若从A取出1/2，B取出1/4溶液，混合后浓度为40%。若从A取出1/5，B取出4/5溶液，混合后浓度为？</font><br><font class="answer"><em>解析：设A、B两个容器中的酒精溶液质量都是100，溶液的浓度分别是x、y。则有100x+100y=45%*200, 50x+25y=40%\</em>(50+25),解得x=30%，y=60%。所求为(20*0.3+80*0.6)/100=54% *</font></li>
<li>排除法<br><font class="exerise">例题：有A、B两瓶混合液，A中水、油、醋的比例为3:8:5，B中水、油、醋的比例为1:2：3，将A、B两瓶混合液倒在一起后，得到的混合液中水、油、醋的比例可能为（）。<br>A、4：5：2　　B、2：3：5<br>C、3：7：7　　D、1：3：3</font><br><font class="answer"><em>解析：在A、B中，水的比重都小于醋，混合后也符合，排除A、D；在A、B两瓶混合液中，油的比重分别是1/2，1/3，混合后大于1/3，排除B。</em></font></li>
</ol>
<h3 id="数学运算高频考点"><a href="#数学运算高频考点" class="headerlink" title="数学运算高频考点"></a>数学运算高频考点</h3><h4 id="几何问题"><a href="#几何问题" class="headerlink" title="几何问题"></a>几何问题</h4><h5 id="平行图行周长和面积"><a href="#平行图行周长和面积" class="headerlink" title="平行图行周长和面积"></a>平行图行周长和面积</h5><p><img src="/images/%E5%B9%B3%E8%A1%8C%E5%9B%BE%E5%BD%A2%E5%91%A8%E9%95%BF%E9%9D%A2%E7%A7%AF%E5%85%AC%E5%BC%8F.jpg" alt="平行图行周长和面积"></p>
<h5 id="立体图形表面积和体积"><a href="#立体图形表面积和体积" class="headerlink" title="立体图形表面积和体积"></a>立体图形表面积和体积</h5><p><img src="/images/%E7%AB%8B%E4%BD%93%E5%87%A0%E4%BD%95%E5%B8%B8%E7%94%A8%E5%85%AC%E5%BC%8F.gif" alt="立体图形表面积和体积"></p>
<h5 id="勾股定理"><a href="#勾股定理" class="headerlink" title="勾股定理"></a>勾股定理</h5><p>勾股定理是一个基本的几何定理，指直角三角形的两条直角边的平方和等于斜边的平方。a²+b²=c²</p>
<h5 id="相似三角形"><a href="#相似三角形" class="headerlink" title="相似三角形"></a>相似三角形</h5><p>对应边成比例，对应角相等（出现在平行，对顶角图形）</p>
<h5 id="距离最短问题"><a href="#距离最短问题" class="headerlink" title="距离最短问题"></a>距离最短问题</h5><p>直线距离最短，立体图形转成平面图画直线。</p>
<h4 id="工程问题"><a href="#工程问题" class="headerlink" title="工程问题"></a>工程问题</h4><p>找<font class="strong">公倍数</font>&gt;或是工作效率，较为简单。</p>
<h4 id="行程问题"><a href="#行程问题" class="headerlink" title="行程问题"></a>行程问题</h4><p>路程 = 速度 * 时间　　s = v * t<br>平均速度 = 总路程 / 总时间</p>
<h5 id="相遇问题"><a href="#相遇问题" class="headerlink" title="相遇问题"></a>相遇问题</h5><ol>
<li><p>简单相遇问题。 相遇路程 = 速度和 * 相遇时间<br><font class="exerise">例题： A、B从甲地开往乙地，C从乙到甲地，它们同时出发，C与A相遇20分钟后与B相遇。已知A、B、C的速度分别为75千米/小时、60千米/小时、50千米/小时，则甲乙两地的距离是： </font><br><font class="answer"><em>解析： 利用甲乙两地的距离列等式，C与A相遇的总路程为甲乙两地的距离，C与B相遇的总路程也是甲乙的距离，（75 + 60）t = （60 + 50）（t + 1/3），t = 22 / 15，再代入，距离为198千米。</em></font></p>
</li>
<li><p>直线多次相遇。<br>总路程S = （2n - 1）* s , n表示第n次相遇。</p>
</li>
</ol>
<h5 id="牛吃草问题"><a href="#牛吃草问题" class="headerlink" title="牛吃草问题"></a>牛吃草问题</h5><style>
table th:first-of-type {width: 10%;}
table th:nth-of-type(2) {width: 90%;}
.attention{color: #f55d42;}
.strong{color: #5a5a96;font-weight: bold;}
.exerise{color: #8d81a6;}
.answer{color: #a295bf;}
</style>]]></content>
      <categories>
        <category>公务员行测</category>
      </categories>
      <tags>
        <tag>公务员笔试</tag>
      </tags>
  </entry>
  <entry>
    <title>北京公务员行测-言语理解与表达</title>
    <url>/2020/04/20/%E5%85%AC%E5%8A%A1%E5%91%98/%E5%8C%97%E4%BA%AC%E5%85%AC%E5%8A%A1%E5%91%98%E8%A1%8C%E6%B5%8B-%E8%A8%80%E8%AF%AD%E7%90%86%E8%A7%A3%E4%B8%8E%E8%A1%A8%E8%BE%BE/</url>
    <content><![CDATA[<p>言语表达和理解总题量约为35题（北京市），分为片段阅读（单个选择）和文章阅读哦（一篇多个选择）。</p>
<h3 id="把握阅读材料"><a href="#把握阅读材料" class="headerlink" title="把握阅读材料"></a>把握阅读材料</h3><h4 id="公文类文章"><a href="#公文类文章" class="headerlink" title="公文类文章"></a>公文类文章</h4><p>　　公文是公文文书的简称，是法定机关与组织在公文活动中，按照特定的体式、经过一定的处理程序和使用的书面材料。公文类文章具有 <em>准确<em>、</em>庄重<em>、</em>精炼<em>、</em>严谨</em> 的特点。     </p>
<p>特定名词：<br>螺丝钉——形容一些看起来微不足道而又不可缺少的小人物<br>桥头堡——泛指作为进攻的据点<br>火车头——比喻起带动作用或领导作用的人或事物<br>发动机——比喻动力之源</p>
<h4 id="社会科学类文章"><a href="#社会科学类文章" class="headerlink" title="社会科学类文章"></a>社会科学类文章</h4><p>　　社会科学类文章是指研究人类社会的各种文化现象与社会科学的文章。涉及知识面广，所论述的内容多为社会共同关注的某一个问题或现象，常见的有文化、历史、社会、哲学、政治、经济、法律、心理学等类别。<br><em>注： 答案选项经常出现  <font class="attention">无中生有 </font> （画关键词）</em></p>
<h4 id="自然科学类文章"><a href="#自然科学类文章" class="headerlink" title="自然科学类文章"></a>自然科学类文章</h4><p>自然科学类文章是指研究自然科学及技术方面的文章。涉及数学、物理学、化学、生物学等基础学科，以天文学、地理学、医学、信息科学等方面。</p>
<h3 id="核心考点"><a href="#核心考点" class="headerlink" title="核心考点"></a>核心考点</h3><h4 id="公文知识"><a href="#公文知识" class="headerlink" title="公文知识"></a>公文知识</h4><h5 id="公文文种"><a href="#公文文种" class="headerlink" title="公文文种"></a>公文文种</h5><p>　　《党政机关公文处理工作条例》第八条将公文分为15类：决议、决定、命令、公报、公告、通告、意见、通知、通报、报告、请示、批复、议案、函、纪要。   </p>
<table>
<thead>
<tr>
<th align="center">文种</th>
<th>适用情况</th>
</tr>
</thead>
<tbody><tr>
<td align="center">决议</td>
<td>会议讨论通过的重大决策事项</td>
</tr>
<tr>
<td align="center">决定</td>
<td>对重要事项作出决策和部署、奖惩有关单位和人员、变更或撤销下级机关不适当的决定事项</td>
</tr>
<tr>
<td align="center">命令</td>
<td>公布行政法规和规章、宣布实施重大强制性措施、批准授权和晋升衔级、嘉奖有关单位和人员</td>
</tr>
<tr>
<td align="center">公报</td>
<td>公布重要决定或者重大事项</td>
</tr>
<tr>
<td align="center">公告</td>
<td>向国内外宣布重要事项或者法定事项</td>
</tr>
<tr>
<td align="center">通告</td>
<td>在一定范围内公布应当遵守或者周知的事项</td>
</tr>
<tr>
<td align="center">意见</td>
<td>对重要问题提出见解和处理方法</td>
</tr>
<tr>
<td align="center">通知</td>
<td>发布、传达要求下级机关执行和有关单位周知或者执行的事项，批转下级机关的公文、转发上级机关和不相隶属机关的公文</td>
</tr>
<tr>
<td align="center">通报</td>
<td>表彰先进、批评错误、传达重要精神和告知重要情况</td>
</tr>
<tr>
<td align="center">报告</td>
<td>向上级机关汇报工作、反映情况，回复上级机关的询问</td>
</tr>
<tr>
<td align="center">请示</td>
<td>向上级机关请求指示、批准</td>
</tr>
<tr>
<td align="center">批复</td>
<td>答复下级机关请示事项</td>
</tr>
<tr>
<td align="center">议案</td>
<td>各级人民政府按照法律程序向同级人民代表大会或者人民代表大会常务委员会提请审议事项</td>
</tr>
<tr>
<td align="center">函</td>
<td>不相隶属机关之间商洽工作、询问和答复问题、请求批准和答复审批事项</td>
</tr>
<tr>
<td align="center">纪要</td>
<td>记载会议主要情况和议定事项</td>
</tr>
</tbody></table>
<h5 id="公文标题和发文字号"><a href="#公文标题和发文字号" class="headerlink" title="公文标题和发文字号"></a>公文标题和发文字号</h5><p>完整规范个公文标题一般由<font class="strong">发文机关名称+事由+文种</font>组成。   </p>
<p>发文字号格式标准： <font class="strong">发文机关代字+年份+发文顺序</font>，括号应问六角括号<br>比如，国发办〔2017〕7号 </p>
<h5 id="主送机关和抄送机关"><a href="#主送机关和抄送机关" class="headerlink" title="主送机关和抄送机关"></a>主送机关和抄送机关</h5><p>　　主送机关，指公文的主要受理机关，即对公文负主办和答复责任的机关，编排于标题下空一行位置，居左顶格。上行文只主送一个负责办理的主管机关，下行文可以主送一个或多个下属机关，一般不主送单位负责人个人。<br>　　抄送机关，是指只需要了解公文内容或者协助办理公文的机关。抄送机关应根据公文发送范围和实际需求来确定，不得随意选择和扩大抄送对象范围。   </p>
<p>主送机关常用的标注形式：</p>
<ol>
<li>全称，如“北京市人民政府”</li>
<li>规范化简称，如“市政府”</li>
<li>统称，即同级或者同类型机关概括性的总称，一般用于下行文</li>
</ol>
<p>统称又具体分为两种：<br>“<font class="strong">泛称主送</font>”，指文件送给同一级的各单位，如“各省、自治区、直辖市人民政府”；<br>“<font class="strong">递降称主送</font>”，指文件主送给垂直几级的同类各单位，如“各行政公署，各市、县人民政府”。   </p>
<p><font class="strong">“泛称主送”是下行文件经常使用的主送形式</font>。它时常涉及同一级不同类的若干机关，排列时一般遵循：</p>
<ol>
<li>“先外后内”，把同是下一级的各地方政府放在前，本机关的职能部门放在后。比如，国务院下行文常用“各省、自治区、直辖市人民政府，国务院各部委、各直属机构”，就是地方政府在前、部委在后。各省政府下行文经常用的“各行政公署，各市、县人民政府，省政府各直属单位”（这个是递降称主送）</li>
<li>“党政军群”，在党的文件中体现最明显。如“各省、自治区、直辖市党委和人民政府，中央和国家机关各部委，军委总政治部，各人民团体”。</li>
</ol>
<p><font class="strong">在泛称主送中，标点符号使用：同级同类机关之间用顿号，同级不同类的机关用逗号，主送机关结束用冒号。</font></p>
<h4 id="选词填空"><a href="#选词填空" class="headerlink" title="选词填空"></a>选词填空</h4><p><font class="attention">不能太相信语感！！！</font></p>
<h5 id="分析语境确定词语含义"><a href="#分析语境确定词语含义" class="headerlink" title="分析语境确定词语含义"></a>分析语境确定词语含义</h5><p>通过<font class="strong">前后文语境</font>分析，甚至文章的主旨、写作背景、生活常识等角度综合考量。<br>解释关系对应：也就是说、即、譬如、因此、可见<br>反对关系对应：但、却、并非、其实<br>并列关系对应：和、与<br>递进关系对应：而且、甚至、更</p>
<h5 id="辨析词语进行选择"><a href="#辨析词语进行选择" class="headerlink" title="辨析词语进行选择"></a>辨析词语进行选择</h5><ol>
<li>词语的适用对象</li>
<li>词意的轻重、侧重点，比如尊崇和推崇</li>
<li>词语的感情色彩：褒义、贬义</li>
<li>词语的语体色彩：书面语、口头语</li>
</ol>
<h4 id="细节判断"><a href="#细节判断" class="headerlink" title="细节判断"></a>细节判断</h4><p>细节信息是承载文章内容的信息载体。最核心的思想就是“<font class="strong">忠于原文</font>&gt;”。陷阱有：</p>
<ol>
<li>偷换概念 ——主、谓、宾、定、状、补</li>
<li>无中生有 ——凭空捏造，添加修饰语</li>
<li>以偏概全 ——“一些”、“几乎”、“绝大多数”、“全部”</li>
<li>因果混乱 ——因果颠倒，强加因果</li>
<li>逻辑错误 ——已然和未然，或然和必然，必要条件和充分条件   </li>
</ol>
<p><em>真题注：文章中未进前几名 ≠ 落选</em></p>
<h4 id="主旨归纳"><a href="#主旨归纳" class="headerlink" title="主旨归纳"></a>主旨归纳</h4><p>一是根据高频词确定材料主题，二是分析行文脉络确定论述重点。</p>
<h4 id="词语理解"><a href="#词语理解" class="headerlink" title="词语理解"></a>词语理解</h4><p>主要考察个别词语、句子含义或指代意理解。遵守就近原则，注意修辞手法。</p>
<h4 id="语句排序"><a href="#语句排序" class="headerlink" title="语句排序"></a>语句排序</h4><ol>
<li>首句：不包含代词，一般不以转折性、总结性词语开头</li>
<li>指代词：会跟在其指代对象之后</li>
<li>关联词：经常配对使用，有固定搭配习惯</li>
<li>逻辑顺序：时间顺序，事物发展先后顺序，不存在倒序</li>
</ol>
<h4 id="标题添加"><a href="#标题添加" class="headerlink" title="标题添加"></a>标题添加</h4><p>原则：紧扣主旨，吸人眼球，研究成果为重点 </p>
<style>
table th:first-of-type {width: 10%;}
table th:nth-of-type(2) {width: 90%;}
.attention{color: #f55d42;}
.strong{color: #5a5a96;font-weight: bold;}
</style>]]></content>
      <categories>
        <category>公务员行测</category>
      </categories>
      <tags>
        <tag>公务员笔试</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫结构</title>
    <url>/2020/02/01/python%E7%88%AC%E8%99%AB/%E7%88%AC%E8%99%AB%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<h1 id="爬虫顺序"><a href="#爬虫顺序" class="headerlink" title="爬虫顺序"></a>爬虫顺序</h1><ol>
<li><p>优化爬虫(静态)</p>
<ol>
<li>user-agent的使用</li>
<li>代理</li>
<li>timeout</li>
<li>请求转码问题</li>
<li>post请求</li>
<li>ajax</li>
<li>异常处理</li>
<li>cookie</li>
</ol>
</li>
<li><p>内容的提取</p>
<ol>
<li>HTML<ol>
<li>re</li>
<li>xpath(DOM)</li>
<li>beautifulSoup</li>
</ol>
</li>
<li>JSON<ol>
<li>json ,jsonpath,dump,dumps</li>
</ol>
</li>
</ol>
</li>
<li><p>动态页面抓取</p>
<ol>
<li>slennium + ChromJS</li>
</ol>
</li>
<li><p>Scrapy框架学习</p>
<ol>
<li>sprider 基础类<ol>
<li>items</li>
<li>pipelines</li>
<li>settings</li>
</ol>
</li>
<li>CrawlSpider 类<ol>
<li>LinkExtractor</li>
<li>Rule</li>
</ol>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>番外-装虚拟</title>
    <url>/2020/02/01/python%E7%88%AC%E8%99%AB/%E7%95%AA%E5%A4%96-%E8%A3%85%E8%99%9A%E6%8B%9F/</url>
    <content><![CDATA[<ol>
<li>装虚拟</li>
<li>创建虚拟机-centos6.5<ol>
<li>网络-自动获取    </li>
<li>使用xshell连接linux<ol>
<li>xshell安装免费版本</li>
<li>xftp安装免费版本</li>
</ol>
</li>
<li>删除网络信息，以方便克隆使用<ol>
<li>cd /etc/dev/network-script</li>
<li>vi ifcfg-eth0</li>
<li>删除UUID，和HDADRESS 使用DD快捷键 esc wq</li>
<li>删除 硬件信息文件 /etc/udev/rules.d/70-dsfs-net.rules</li>
<li>rm -rf 70-dsfs-net.rules</li>
</ol>
</li>
<li>装Python</li>
<li>装Twiested</li>
<li>克隆</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>番外篇 - User-Agent的收集</title>
    <url>/2020/02/01/python%E7%88%AC%E8%99%AB/%E7%95%AA%E5%A4%96%E7%AF%87%20-%20User-Agent%E7%9A%84%E6%94%B6%E9%9B%86/</url>
    <content><![CDATA[<p>```<br>agents = [<br>    “Mozilla/5.0 (Linux; U; Android 2.3.6; en-us; Nexus S Build/GRK39F) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1”,<br>    “Avant Browser/1.2.789rel1 (<a href="http://www.avantbrowser.com)&quot;" target="_blank" rel="noopener">http://www.avantbrowser.com)&quot;</a>,<br>    “Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.5 (KHTML, like Gecko) Chrome/4.0.249.0 Safari/532.5”,<br>    “Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US) AppleWebKit/532.9 (KHTML, like Gecko) Chrome/5.0.310.0 Safari/532.9”,<br>    “Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.7 (KHTML, like Gecko) Chrome/7.0.514.0 Safari/534.7”,<br>    “Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/9.0.601.0 Safari/534.14”,<br>    “Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/10.0.601.0 Safari/534.14”,<br>    “Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.20 (KHTML, like Gecko) Chrome/11.0.672.2 Safari/534.20”,<br>    “Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.27 (KHTML, like Gecko) Chrome/12.0.712.0 Safari/534.27”,<br>    “Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.24 Safari/535.1”,<br>    “Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.874.120 Safari/535.2”<br>]</p>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>番外篇 - 安装分布式环境</title>
    <url>/2020/02/01/python%E7%88%AC%E8%99%AB/%E7%95%AA%E5%A4%96%E7%AF%87%20-%20%E5%AE%89%E8%A3%85%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<h3 id="使用CentOS6-标准版系统"><a href="#使用CentOS6-标准版系统" class="headerlink" title="使用CentOS6 标准版系统"></a>使用CentOS6 标准版系统</h3><p>因为CentOS默认是python2</p>
<ol>
<li><p>安装 python3</p>
<ol>
<li><p>安装装python3的环境</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y gcc,zlib*,openssl*</span><br></pre></td></tr></table></figure></li>
<li><p>解压压缩包<br> tar -xf python3.6.1.tar</p>
<p> (参考Linux安装python3)</p>
</li>
</ol>
</li>
<li><p>安装scrapy</p>
<ol>
<li><p>安装装scrapy的环境</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y wget</span><br><span class="line">wget https:&#x2F;&#x2F;twistedmatrix.com&#x2F;Releases&#x2F;Twisted&#x2F;17.1&#x2F;Twisted-17.1.0.tar.bz2</span><br><span class="line"></span><br><span class="line">tar -jxvf Twisted-17.1.0.tar.bz2  </span><br><span class="line">cd Twisted-17.1.0  </span><br><span class="line">python3 setup.py install</span><br></pre></td></tr></table></figure></li>
<li><p>安装scrapy</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip3.6 install scrapy</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>安装 scrapy-redis</p>
<pre><code>pip3.6 install scrapy-redis

</code></pre></li>
</ol>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>31. Scrapyd的安装及使用</title>
    <url>/2020/01/31/python%E7%88%AC%E8%99%AB/31.%20Scrapyd%E7%9A%84%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h3 id="Scrapyd的安装及使用"><a href="#Scrapyd的安装及使用" class="headerlink" title="Scrapyd的安装及使用"></a>Scrapyd的安装及使用</h3><h4 id="1-安装scrapyd"><a href="#1-安装scrapyd" class="headerlink" title="1. 安装scrapyd"></a>1. 安装scrapyd</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install scrapyd</span><br></pre></td></tr></table></figure>
<h4 id="2-安装setuptools"><a href="#2-安装setuptools" class="headerlink" title="2. 安装setuptools"></a>2. 安装setuptools</h4><blockquote>
<p>为什么要安装这个工具？</p>
</blockquote>
<p>因为部署的应用需要打包成*.egg才能运行</p>
<p>官网地址：<a href="https://pypi.python.org/pypi/setuptools下载" target="_blank" rel="noopener">https://pypi.python.org/pypi/setuptools下载</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install setuptools-38.5.2-py2.py3-none-any</span><br></pre></td></tr></table></figure>
<h4 id="3-部署工程"><a href="#3-部署工程" class="headerlink" title="3. 部署工程"></a>3. 部署工程</h4><h5 id="3-1-创建项目"><a href="#3-1-创建项目" class="headerlink" title="3.1 创建项目"></a>3.1 创建项目</h5><blockquote>
<p>工程下会有一个叫scrapy.cfg的文件，文件的内容如下：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[settings]</span><br><span class="line">default &#x3D; my_spider.settings</span><br><span class="line"></span><br><span class="line">[deploy:demo]  # demo是指这个deploy的名称，自己命名，可以多个。（后面有用到） </span><br><span class="line">#url &#x3D; http:&#x2F;&#x2F;localhost:6800&#x2F;</span><br><span class="line">project &#x3D; my_spider  # 工程的名称</span><br></pre></td></tr></table></figure>
<h5 id="3-2-启动scrapyd"><a href="#3-2-启动scrapyd" class="headerlink" title="3.2 启动scrapyd"></a>3.2 启动scrapyd</h5><blockquote>
<p>在本工程下命令行下启动scrapyd</p>
</blockquote>
<p><strong>注意：</strong> 如果不先启动scrapyd就会无法部署工程</p>
<h5 id="3-3-部署项目"><a href="#3-3-部署项目" class="headerlink" title="3.3 部署项目"></a>3.3 部署项目</h5><blockquote>
<p>通过scrapyd-deploy部署，要求装一个scrapyd-client</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install scrapyd-client</span><br></pre></td></tr></table></figure>
<h5 id="3-4-配置scrapyd-deploy"><a href="#3-4-配置scrapyd-deploy" class="headerlink" title="3.4 配置scrapyd-deploy"></a>3.4 配置scrapyd-deploy</h5><p>在 %python_home%\Scripts下增加一个scrapyd-deploy.bat文件，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@echo off </span><br><span class="line">&quot;%python_home%\python.exe&quot; &quot;%python_home%\Scripts\scrapyd-deploy&quot; %1 %2 %3 %4 %5 %6 %7 %8 %9</span><br></pre></td></tr></table></figure>
<h5 id="3-5-使用scrapyd-deploy"><a href="#3-5-使用scrapyd-deploy" class="headerlink" title="3.5 使用scrapyd-deploy"></a>3.5 使用scrapyd-deploy</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy-deploy demo  #demo就是scrapy.cfg中的名字</span><br></pre></td></tr></table></figure>

<h5 id="4-运行Spider"><a href="#4-运行Spider" class="headerlink" title="4 运行Spider"></a>4 运行Spider</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:6800&#x2F;schedule.json -d project&#x3D;项目名 -d spider&#x3D;爬虫名</span><br></pre></td></tr></table></figure>

<h5 id="5-查看效果"><a href="#5-查看效果" class="headerlink" title="5 查看效果"></a>5 查看效果</h5><p>在浏览器输入localhost:6800</p>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>30. Scrapy-Redis 其他</title>
    <url>/2020/01/30/python%E7%88%AC%E8%99%AB/30.%20Scrapy-Redis%20%E5%85%B6%E4%BB%96/</url>
    <content><![CDATA[<p>setting文件配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#启用Redis调度存储请求队列</span><br><span class="line">SCHEDULER &#x3D; &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line"> </span><br><span class="line">#确保所有的爬虫通过Redis去重</span><br><span class="line">DUPEFILTER_CLASS &#x3D; &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line"> </span><br><span class="line">#默认请求序列化使用的是pickle 但是我们可以更改为其他类似的。PS：这玩意儿2.X的可以用。3.X的不能用</span><br><span class="line">#SCHEDULER_SERIALIZER &#x3D; &quot;scrapy_redis.picklecompat&quot;</span><br><span class="line"> </span><br><span class="line">#不清除Redis队列、这样可以暂停&#x2F;恢复 爬取</span><br><span class="line">#SCHEDULER_PERSIST &#x3D; True</span><br><span class="line"> </span><br><span class="line">#使用优先级调度请求队列 （默认使用）</span><br><span class="line">#SCHEDULER_QUEUE_CLASS &#x3D; &#39;scrapy_redis.queue.PriorityQueue&#39;</span><br><span class="line">#可选用的其它队列</span><br><span class="line">#SCHEDULER_QUEUE_CLASS &#x3D; &#39;scrapy_redis.queue.FifoQueue&#39;</span><br><span class="line">#SCHEDULER_QUEUE_CLASS &#x3D; &#39;scrapy_redis.queue.LifoQueue&#39;</span><br><span class="line"> </span><br><span class="line">#最大空闲时间防止分布式爬虫因为等待而关闭</span><br><span class="line">#这只有当上面设置的队列类是SpiderQueue或SpiderStack时才有效</span><br><span class="line">#并且当您的蜘蛛首次启动时，也可能会阻止同一时间启动（由于队列为空）</span><br><span class="line">#SCHEDULER_IDLE_BEFORE_CLOSE &#x3D; 10</span><br><span class="line"> </span><br><span class="line">#将清除的项目在redis进行处理</span><br><span class="line">ITEM_PIPELINES &#x3D; &#123;</span><br><span class="line">    &#39;scrapy_redis.pipelines.RedisPipeline&#39;: 300</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">#序列化项目管道作为redis Key存储</span><br><span class="line">#REDIS_ITEMS_KEY &#x3D; &#39;%(spider)s:items&#39;</span><br><span class="line"> </span><br><span class="line">#默认使用ScrapyJSONEncoder进行项目序列化</span><br><span class="line">#You can use any importable path to a callable object.</span><br><span class="line">#REDIS_ITEMS_SERIALIZER &#x3D; &#39;json.dumps&#39;</span><br><span class="line"> </span><br><span class="line">#指定连接到redis时使用的端口和地址（可选）</span><br><span class="line">#REDIS_HOST &#x3D; &#39;localhost&#39;</span><br><span class="line">#REDIS_PORT &#x3D; 6379</span><br><span class="line"> </span><br><span class="line">#指定用于连接redis的URL（可选）</span><br><span class="line">#如果设置此项，则此项优先级高于设置的REDIS_HOST 和 REDIS_PORT</span><br><span class="line">#REDIS_URL &#x3D; &#39;redis:&#x2F;&#x2F;user:pass@hostname:9001&#39;</span><br><span class="line"> </span><br><span class="line">#自定义的redis参数（连接超时之类的）</span><br><span class="line">#REDIS_PARAMS  &#x3D; &#123;&#125;</span><br><span class="line"> </span><br><span class="line">#自定义redis客户端类</span><br><span class="line">#REDIS_PARAMS[&#39;redis_cls&#39;] &#x3D; &#39;myproject.RedisClient&#39;</span><br><span class="line"> </span><br><span class="line">#如果为True，则使用redis的&#39;spop&#39;进行操作。</span><br><span class="line">#如果需要避免起始网址列表出现重复，这个选项非常有用。开启此选项urls必须通过sadd添加，否则会出现类型错误。</span><br><span class="line">#REDIS_START_URLS_AS_SET &#x3D; False</span><br><span class="line"> </span><br><span class="line">#RedisSpider和RedisCrawlSpider默认 start_usls 键</span><br><span class="line">#REDIS_START_URLS_KEY &#x3D; &#39;%(name)s:start_urls&#39;</span><br><span class="line"> </span><br><span class="line">#设置redis使用utf-8之外的编码</span><br><span class="line">#REDIS_ENCODING &#x3D; &#39;latin1&#39;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>29. Scrapy 框架-分布式</title>
    <url>/2020/01/29/python%E7%88%AC%E8%99%AB/29.%20Scrapy%20%E6%A1%86%E6%9E%B6-%E5%88%86%E5%B8%83%E5%BC%8F/</url>
    <content><![CDATA[<h3 id="1-介绍scrapy-redis框架"><a href="#1-介绍scrapy-redis框架" class="headerlink" title="1. 介绍scrapy-redis框架"></a>1. 介绍scrapy-redis框架</h3><p>scrapy-redis</p>
<blockquote>
<p>一个三方的基于redis的分布式爬虫框架，配合scrapy使用，让爬虫具有了分布式爬取的功能。</p>
</blockquote>
<p>github地址：<br><a href="https://github.com/darkrho/scrapy-redis">https://github.com/darkrho/scrapy-redis</a></p>
<h3 id="2-分布式原理"><a href="#2-分布式原理" class="headerlink" title="2. 分布式原理"></a>2. 分布式原理</h3><p>　scrapy-redis实现分布式，其实从原理上来说很简单，这里为描述方便，我们把自己的<strong>核心服务器</strong>称为<strong>master</strong>，而把用于<strong>跑爬虫程序</strong>的机器称为<strong>slave</strong></p>
<p>我们知道，采用scrapy框架抓取网页，我们需要首先给定它一些start_urls，爬虫首先访问start_urls里面的url，再根据我们的具体逻辑，对里面的元素、或者是其他的二级、三级页面进行抓取。而要实现分布式，我们只需要在这个starts_urls里面做文章就行了</p>
<p>我们在<strong>master</strong>上搭建一个<strong>redis数据库</strong>`（注意这个数据库只用作url的存储)，并对每一个需要爬取的网站类型，都开辟一个单独的列表字段。通过设置slave上scrapy-redis获取url的地址为master地址。这样的结果就是，<strong>尽管有多个slave，然而大家获取url的地方只有一个，那就是服务器master上的redis数据库</strong></p>
<p>并且，由于scrapy-redis<strong>自身的队列机制</strong>，slave获取的链接不会相互冲突。这样各个slave在完成抓取任务之后，再把获取的结果汇总到服务器上</p>
<p><strong>好处</strong></p>
<p>程序移植性强，只要处理好路径问题，把slave上的程序移植到另一台机器上运行，基本上就是复制粘贴的事情</p>
<h3 id="3-分布式爬虫的实现"><a href="#3-分布式爬虫的实现" class="headerlink" title="3.分布式爬虫的实现"></a>3.分布式爬虫的实现</h3><ol>
<li><p>使用三台机器，一台是win10，两台是centos6，分别在两台机器上部署scrapy来进行分布式抓取一个网站</p>
</li>
<li><p>win10的ip地址为192.168.31.245，用来作为redis的master端，centos的机器作为slave</p>
</li>
<li><p>master的爬虫运行时会把提取到的url封装成request放到redis中的数据库：“dmoz:requests”，并且从该数据库中提取request后下载网页，再把网页的内容存放到redis的另一个数据库中“dmoz:items”</p>
</li>
<li><p>slave从master的redis中取出待抓取的request，下载完网页之后就把网页的内容发送回master的redis</p>
</li>
<li><p>重复上面的3和4，直到master的redis中的“dmoz:requests”数据库为空，再把master的redis中的“dmoz:items”数据库写入到mongodb中</p>
</li>
<li><p>master里的reids还有一个数据“dmoz:dupefilter”是用来存储抓取过的url的指纹（使用哈希函数将url运算后的结果），是防止重复抓取的</p>
</li>
</ol>
<h3 id="4-scrapy-redis框架的安装"><a href="#4-scrapy-redis框架的安装" class="headerlink" title="4. scrapy-redis框架的安装"></a>4. scrapy-redis框架的安装</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install scrapy-redis</span><br></pre></td></tr></table></figure>

<h3 id="5-部署scrapy-redis"><a href="#5-部署scrapy-redis" class="headerlink" title="5. 部署scrapy-redis"></a>5. 部署scrapy-redis</h3><h4 id="5-1-slave端"><a href="#5-1-slave端" class="headerlink" title="5.1 slave端"></a>5.1 slave端</h4><blockquote>
<p>在windows上的settings.py文件的最后增加如下一行</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">REDIS_HOST &#x3D; &#39;localhost&#39; #master IP</span><br><span class="line"></span><br><span class="line">REDIS_PORT &#x3D; 6379</span><br></pre></td></tr></table></figure>

<p>配置好了远程的redis地址后启动两个爬虫（启动爬虫没有顺序限制）</p>
<h4 id="6-给爬虫增加配置信息"><a href="#6-给爬虫增加配置信息" class="headerlink" title="6 给爬虫增加配置信息"></a>6 给爬虫增加配置信息</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DUPEFILTER_CLASS &#x3D; &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line">SCHEDULER &#x3D; &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line">SCHEDULER_PERSIST &#x3D; True</span><br><span class="line">#SCHEDULER_QUEUE_CLASS &#x3D; &quot;scrapy_redis.queue.SpiderPriorityQueue&quot;</span><br><span class="line">#SCHEDULER_QUEUE_CLASS &#x3D; &quot;scrapy_redis.queue.SpiderQueue&quot;</span><br><span class="line">#SCHEDULER_QUEUE_CLASS &#x3D; &quot;scrapy_redis.queue.SpiderStack&quot;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES &#x3D; &#123;</span><br><span class="line">    &#39;example.pipelines.ExamplePipeline&#39;: 300,</span><br><span class="line">    &#39;scrapy_redis.pipelines.RedisPipeline&#39;: 400,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="7-运行程序"><a href="#7-运行程序" class="headerlink" title="7 运行程序"></a>7 运行程序</h4><h5 id="7-1-运行slave"><a href="#7-1-运行slave" class="headerlink" title="7.1 运行slave"></a>7.1 运行slave</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy runspider 文件名.py</span><br></pre></td></tr></table></figure>
<p>开起没有先后顺序</p>
<h5 id="7-2-运行master"><a href="#7-2-运行master" class="headerlink" title="7.2 运行master"></a>7.2 运行master</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lpush (redis_key)  url #括号不用写</span><br></pre></td></tr></table></figure>
<p><strong>说明</strong></p>
<ul>
<li>这个命令是在redis-cli中运行</li>
<li>redis_key 是 spider.py文件中的redis_key的值</li>
<li>url 开始爬取地址，不加双引号</li>
</ul>
<h4 id="8-数据导入到mongodb中"><a href="#8-数据导入到mongodb中" class="headerlink" title="8 数据导入到mongodb中"></a>8 数据导入到mongodb中</h4><p>等到爬虫结束后,如果要把数据存储到mongodb中，就应该修改master端process_items.py文件，如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import redis</span><br><span class="line"></span><br><span class="line">import pymongo</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line"></span><br><span class="line">    r &#x3D; redis.Redis(host&#x3D;&#39;192.168.31.245&#39;,port&#x3D;6379,db&#x3D;0)</span><br><span class="line"></span><br><span class="line">    client &#x3D; pymongo.MongoClient(host&#x3D;&#39;localhost&#39;, port&#x3D;27017)</span><br><span class="line"></span><br><span class="line">    db &#x3D; client.dmoz</span><br><span class="line"></span><br><span class="line">    sheet &#x3D; db.sheet</span><br><span class="line"></span><br><span class="line">    while True:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        source, data &#x3D; r.blpop([&quot;dmoz:items&quot;])</span><br><span class="line"></span><br><span class="line">        item &#x3D; json.loads(data)</span><br><span class="line"></span><br><span class="line">        sheet.insert(item)</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line"></span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h4 id="9-数据导入到MySQL中"><a href="#9-数据导入到MySQL中" class="headerlink" title="9 数据导入到MySQL中"></a>9 数据导入到MySQL中</h4><p>等到爬虫结束后,如果要把数据存储到mongodb中，就应该修改master端process_items.py文件，如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import redis</span><br><span class="line">import pymysql</span><br><span class="line">import json</span><br><span class="line">def process_item():</span><br><span class="line">    r_client &#x3D; redis.Redis(host&#x3D;&quot;127.0.0.1&quot;,port&#x3D;6379,db &#x3D;0)</span><br><span class="line">    m_client &#x3D; pymysql.connect(host&#x3D;&quot;127.0.0.1&quot;,port&#x3D;3306,user&#x3D;&quot;root&quot;,passowrd&#x3D;&quot;123456&quot;,db&#x3D;&quot;lianjia&quot;)</span><br><span class="line">    source,data &#x3D;r_client.blpop(&quot;lianjia:item&quot;)</span><br><span class="line">    item &#x3D; json.loads(data)</span><br><span class="line"></span><br><span class="line">    cursor &#x3D; m_client.cursor()</span><br><span class="line">    values &#x3D; []</span><br><span class="line">    cursor.execute(sql,values)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>28. Scrapy 框架-爬取JS生成的动态页面</title>
    <url>/2020/01/28/python%E7%88%AC%E8%99%AB/28.%20Scrapy%20%E6%A1%86%E6%9E%B6-%E7%88%AC%E5%8F%96JS%E7%94%9F%E6%88%90%E7%9A%84%E5%8A%A8%E6%80%81%E9%A1%B5%E9%9D%A2/</url>
    <content><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>有的页面的很多部分都是用JS生成的，而对于用scrapy爬虫来说就是一个很大的问题，因为scrapy没有JS engine，所以爬取的都是静态页面，对于JS生成的动态页面都无法获得</p>
<p><a href="http://splash.readthedocs.io/en/stable/" target="_blank" rel="noopener">官网</a><a href="http://splash.readthedocs.io/en/stable/" target="_blank" rel="noopener">http://splash.readthedocs.io/en/stable/</a></p>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><ul>
<li>利用第三方中间件来提供JS渲染服务： scrapy-splash 等</li>
<li>利用webkit或者基于webkit库</li>
</ul>
<blockquote>
<p>Splash是一个Javascript渲染服务。它是一个实现了HTTP API的轻量级浏览器，Splash是用Python实现的，同时使用Twisted和QT。Twisted（QT）用来让服务具有异步处理能力，以发挥webkit的并发能力</p>
</blockquote>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><ol>
<li><p>pip安装scrapy-splash库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install scrapy-splash</span><br></pre></td></tr></table></figure></li>
<li><p>scrapy-splash使用的是Splash HTTP API， 所以需要一个splash instance，一般采用docker运行splash，所以需要安装docker</p>
</li>
<li><p>安装docker, 安装好后运行docker</p>
</li>
<li><p>拉取镜像</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull scrapinghub&#x2F;splash</span><br></pre></td></tr></table></figure></li>
<li><p>用docker运行scrapinghub/splash</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -p 8050:8050 scrapinghub&#x2F;splash</span><br></pre></td></tr></table></figure></li>
<li><p>配置splash服务（以下操作全部在settings.py）:</p>
<ol>
<li><p>使用splash解析，要在配置文件中设置splash服务器地址：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SPLASH_URL &#x3D; &#39;http:&#x2F;&#x2F;192.168.99.100:8050&#x2F;&#39;</span><br></pre></td></tr></table></figure></li>
<li><p>将splash middleware添加到DOWNLOADER_MIDDLEWARE中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES &#x3D; &#123;</span><br><span class="line">&#39;scrapy_splash.SplashCookiesMiddleware&#39;: 723,</span><br><span class="line">&#39;scrapy_splash.SplashMiddleware&#39;: 725,</span><br><span class="line">&#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;: 810,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>Enable SplashDeduplicateArgsMiddleware</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SPIDER_MIDDLEWARES &#x3D; &#123;</span><br><span class="line">  &#39;scrapy_splash.SplashDeduplicateArgsMiddleware&#39;: 100</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个中间件需要支持cache_args功能; 它允许通过不在磁盘请求队列中多次存储重复的Splash参数来节省磁盘空间。如果使用Splash 2.1+，则中间件也可以通过不将这些重复的参数多次发送到Splash服务器来节省网络流量</p>
</li>
<li><p>配置消息队列所使用的过滤类</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DUPEFILTER_CLASS &#x3D; &#39;scrapy_splash.SplashAwareDupeFilter&#39;</span><br></pre></td></tr></table></figure></li>
<li><p>配置消息队列需要使用的类</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">HTTPCACHE_STORAGE &#x3D; &#39;scrapy_splash.SplashAwareFSCacheStorage&#39;</span><br></pre></td></tr></table></figure>
<h3 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy_splash import SplashRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class DoubanSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &#39;douban&#39;</span><br><span class="line"></span><br><span class="line">    allowed_domains &#x3D; [&#39;douban.com&#39;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def start_requests(self):</span><br><span class="line">    yield SplashRequest(&#39;https:&#x2F;&#x2F;movie.douban.com&#x2F;typerank?type_name&#x3D;剧情&amp;type&#x3D;11&amp;interval_id&#x3D;100:90&#39;, args&#x3D;&#123;&#39;wait&#39;: 0.5&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def parse(self, response):</span><br><span class="line">    print(response.text)</span><br></pre></td></tr></table></figure>



</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>27. Splash 的使用</title>
    <url>/2020/01/27/python%E7%88%AC%E8%99%AB/27.%20Splash%20%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h3 id="1-Splash介绍"><a href="#1-Splash介绍" class="headerlink" title="1. Splash介绍"></a>1. Splash介绍</h3><blockquote>
<p>Splash是一个JavaScript渲染服务，是一个带有HTTP API的轻量级浏览器，同时它对接了Python中的Twisted和QT库。利用它，我们同样可以实现动态渲染页面的抓取</p>
</blockquote>
<h3 id="2-安装"><a href="#2-安装" class="headerlink" title="2. 安装"></a>2. 安装</h3><h4 id="2-1-安装docker"><a href="#2-1-安装docker" class="headerlink" title="2.1 安装docker"></a>2.1 安装docker</h4><h4 id="2-2-拉取镜像"><a href="#2-2-拉取镜像" class="headerlink" title="2.2 拉取镜像"></a>2.2 拉取镜像</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull scrapinghub&#x2F;splash</span><br></pre></td></tr></table></figure>
<h4 id="2-3-用docker运行scrapinghub-splash"><a href="#2-3-用docker运行scrapinghub-splash" class="headerlink" title="2.3 用docker运行scrapinghub/splash"></a>2.3 用docker运行scrapinghub/splash</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -p 8050:8050 scrapinghub&#x2F;splash</span><br></pre></td></tr></table></figure>
<h4 id="2-4-查看效果"><a href="#2-4-查看效果" class="headerlink" title="2.4 查看效果"></a>2.4 查看效果</h4><blockquote>
<p>我们在8050端口上运行了Splash服务，打开<a href="http://192.168.99.100:8050/即可看到其Web页面" target="_blank" rel="noopener">http://192.168.99.100:8050/即可看到其Web页面</a><br><img src="https://note.youdao.com/yws/api/personal/file/366AEA0862FF4B77B584F99F058FD0FE?method=download&shareKey=1becb4e3fd74346d3e247a6cf7d8406d" alt="image"></p>
</blockquote>
<h3 id="3-Splash对象属性"><a href="#3-Splash对象属性" class="headerlink" title="3 Splash对象属性"></a>3 Splash对象属性</h3><blockquote>
<p>上图中main()方法的第一个参数是splash，这个对象非常重要，它类似于Selenium中的WebDriver对象</p>
</blockquote>
<h4 id="3-1-images-enabled"><a href="#3-1-images-enabled" class="headerlink" title="3.1 images_enabled"></a>3.1 images_enabled</h4><blockquote>
<p>设置图片是否加载，默认情况下是加载的。禁用该属性后，可以节省网络流量并提高网页加载速度</p>
</blockquote>
<blockquote>
<p>注意的是，禁用图片加载可能会影响JavaScript渲染。因为禁用图片之后，它的外层DOM节点的高度会受影响，进而影响DOM节点的位置</p>
</blockquote>
<blockquote>
<p>因此，如果JavaScript对图片节点有操作的话，其执行就会受到影响</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash, args)</span><br><span class="line">  splash.images_enabled &#x3D; false</span><br><span class="line">  splash:go(&#39;https:&#x2F;&#x2F;www.baidu.com&#39;)</span><br><span class="line">  return &#123;html&#x3D;splash:html()&#125;</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h4 id="3-2-plugins-enabled"><a href="#3-2-plugins-enabled" class="headerlink" title="3.2 plugins_enabled"></a>3.2 plugins_enabled</h4><blockquote>
<p>可以控制浏览器插件（如Flash插件）是否开启</p>
</blockquote>
<blockquote>
<p>默认情况下，此属性是false，表示不开启</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">splash.plugins_enabled &#x3D; true&#x2F;false</span><br></pre></td></tr></table></figure>

<h4 id="3-3-scroll-position"><a href="#3-3-scroll-position" class="headerlink" title="3.3 scroll_position"></a>3.3 scroll_position</h4><blockquote>
<p>控制页面上下或左右滚动</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">splash.scroll_position &#x3D; &#123;x&#x3D;100, y&#x3D;200&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-Splash对象的方法"><a href="#4-Splash对象的方法" class="headerlink" title="4. Splash对象的方法"></a>4. Splash对象的方法</h3><h4 id="4-1-go"><a href="#4-1-go" class="headerlink" title="4.1 go()"></a>4.1 go()</h4><blockquote>
<p>该方法用来请求某个链接，而且它可以模拟GET和POST请求，同时支持传入请求头、表单等数据</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ok, reason &#x3D; splash:go&#123;url, baseurl&#x3D;nil, headers&#x3D;nil, http_method&#x3D;&quot;GET&quot;, body&#x3D;nil, formdata&#x3D;nil&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>返回结果是结果ok和原因reason</p>
</blockquote>
<blockquote>
<p>如果ok为空，代表网页加载出现了错误，此时reason变量中包含了错误的原因</p>
</blockquote>
<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>url</td>
<td>请求的URL</td>
</tr>
<tr>
<td>baseurl</td>
<td>可选参数，默认为空，表示资源加载相对路径</td>
</tr>
<tr>
<td>headers</td>
<td>可选参数，默认为空，表示请求头</td>
</tr>
<tr>
<td>http_method</td>
<td>可选参数，默认为GET，同时支持POST</td>
</tr>
<tr>
<td>body</td>
<td>可选参数，默认为空，发POST请求时的表单数据，使用的Content-type为application/json</td>
</tr>
<tr>
<td>formdata</td>
<td>可选参数，默认为空，POST的时候的表单数据，使用的Content-type为application/x-www-form-urlencoded</td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">splash:go&#123;&quot;http:&#x2F;&#x2F;www.sxt.cn&quot;, http_method&#x3D;&quot;POST&quot;, body&#x3D;&quot;name&#x3D;17703181473&quot;&#125;</span><br></pre></td></tr></table></figure>
<h4 id="4-2-wait"><a href="#4-2-wait" class="headerlink" title="4.2 wait()"></a>4.2 wait()</h4><blockquote>
<p>控制页面的等待时间</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">splash:wait&#123;time, cancel_on_redirect&#x3D;false, cancel_on_error&#x3D;true&#125;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>time</td>
<td>等待的秒数</td>
</tr>
<tr>
<td>cancel_on_redirect</td>
<td>可选参数，默认为false，表示如果发生了重定向就停止等待，并返回重定向结果</td>
</tr>
<tr>
<td>cancel_on_error</td>
<td>可选参数，默认为false，表示如果发生了加载错误，就停止等待</td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash)</span><br><span class="line">    splash:go(&quot;https:&#x2F;&#x2F;www.taobao.com&quot;)</span><br><span class="line">    splash:wait(2)</span><br><span class="line">    return &#123;html&#x3D;splash:html()&#125;</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h4 id="4-3-jsfunc"><a href="#4-3-jsfunc" class="headerlink" title="4.3 jsfunc()"></a>4.3 jsfunc()</h4><blockquote>
<p>直接调用JavaScript定义的方法，但是所调用的方法需要用双中括号包围，这相当于实现了JavaScript方法到Lua脚本的转换</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash, args)</span><br><span class="line">  splash:go(&quot;http:&#x2F;&#x2F;www.sxt.cn&quot;)</span><br><span class="line">  local scroll_to &#x3D; splash:jsfunc(&quot;window.scrollTo&quot;)</span><br><span class="line">  scroll_to(0, 300)</span><br><span class="line">  return &#123;png&#x3D;splash:png()&#125;</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h4 id="4-4-evaljs-与-runjs"><a href="#4-4-evaljs-与-runjs" class="headerlink" title="4.4 evaljs()与 runjs()"></a>4.4 evaljs()与 runjs()</h4><ul>
<li>evaljs() 以执行JavaScript代码并返回最后一条JavaScript语句的返回结果</li>
<li>runjs() 以执行JavaScript代码，它与evaljs()的功能类似，但是更偏向于执行某些动作或声明某些方法</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash, args)</span><br><span class="line">  splash:go(&quot;https:&#x2F;&#x2F;www.baidu.com&quot;)</span><br><span class="line">  splash:runjs(&quot;foo &#x3D; function() &#123; return &#39;sxt&#39; &#125;&quot;)</span><br><span class="line">  local result &#x3D; splash:evaljs(&quot;foo()&quot;)</span><br><span class="line">  return result</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h4 id="4-5-html"><a href="#4-5-html" class="headerlink" title="4.5 html()"></a>4.5 html()</h4><blockquote>
<p>获取网页的源代码</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash, args)</span><br><span class="line">  splash:go(&quot;https:&#x2F;&#x2F;www.bjsxt.com&quot;)</span><br><span class="line">  return splash:html()</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h4 id="4-6-png"><a href="#4-6-png" class="headerlink" title="4.6 png()"></a>4.6 png()</h4><blockquote>
<p>获取PNG格式的网页截图</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash, args)</span><br><span class="line">  splash:go(&quot;https:&#x2F;&#x2F;www.bjsxt.com&quot;)</span><br><span class="line">  return splash:png()</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h4 id="4-7-har"><a href="#4-7-har" class="headerlink" title="4.7 har()"></a>4.7 har()</h4><blockquote>
<p>获取页面加载过程描述</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash, args)</span><br><span class="line">  splash:go(&quot;https:&#x2F;&#x2F;www.bjsxt.com&quot;)</span><br><span class="line">  return splash:har()</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h4 id="4-8-url"><a href="#4-8-url" class="headerlink" title="4.8 url()"></a>4.8 url()</h4><blockquote>
<p>获取当前正在访问的URL</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash, args)</span><br><span class="line">  splash:go(&quot;https:&#x2F;&#x2F;www.bjsxt.com&quot;)</span><br><span class="line">  return splash:url()</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h4 id="4-9-get-cookies"><a href="#4-9-get-cookies" class="headerlink" title="4.9 get_cookies()"></a>4.9 get_cookies()</h4><blockquote>
<p>获取当前页面的Cookies</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash, args)</span><br><span class="line">  splash:go(&quot;https:&#x2F;&#x2F;www.bjsxt.com&quot;)</span><br><span class="line">  return splash:get_cookies()</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h4 id="4-10-add-cookie"><a href="#4-10-add-cookie" class="headerlink" title="4.10 add_cookie()"></a>4.10 add_cookie()</h4><blockquote>
<p>当前页面添加Cookie</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cookies &#x3D; splash:add_cookie&#123;name, value, path&#x3D;nil, domain&#x3D;nil, expires&#x3D;nil, httpOnly&#x3D;nil, secure&#x3D;nil&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash)</span><br><span class="line">    splash:add_cookie&#123;&quot;sessionid&quot;, &quot;123456abcdef&quot;, &quot;&#x2F;&quot;, domain&#x3D;&quot;http:&#x2F;&#x2F;bjsxt.com&quot;&#125;</span><br><span class="line">    splash:go(&quot;http:&#x2F;&#x2F;bjsxt.com&#x2F;&quot;)</span><br><span class="line">    return splash:html()</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h4 id="4-11-clear-cookies"><a href="#4-11-clear-cookies" class="headerlink" title="4.11 clear_cookies()"></a>4.11 clear_cookies()</h4><blockquote>
<p>可以清除所有的Cookies</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash)</span><br><span class="line">    splash:go(&quot;https:&#x2F;&#x2F;www.bjsxt.com&#x2F;&quot;)</span><br><span class="line">    splash:clear_cookies()</span><br><span class="line">    return splash:get_cookies()</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h4 id="4-12-set-user-agent"><a href="#4-12-set-user-agent" class="headerlink" title="4.12 set_user_agent()"></a>4.12 set_user_agent()</h4><blockquote>
<p>设置浏览器的User-Agent</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash)</span><br><span class="line">  splash:set_user_agent(&#39;Splash&#39;)</span><br><span class="line">  splash:go(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;get&quot;)</span><br><span class="line">  return splash:html()</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h4 id="4-13-set-custom-headers"><a href="#4-13-set-custom-headers" class="headerlink" title="4.13 set_custom_headers()"></a>4.13 set_custom_headers()</h4><blockquote>
<p>设置请求头</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash)</span><br><span class="line">  splash:set_custom_headers(&#123;</span><br><span class="line">     [&quot;User-Agent&quot;] &#x3D; &quot;Splash&quot;,</span><br><span class="line">     [&quot;Site&quot;] &#x3D; &quot;Splash&quot;,</span><br><span class="line">  &#125;)</span><br><span class="line">  splash:go(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;get&quot;)</span><br><span class="line">  return splash:html()</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h4 id="4-14-select"><a href="#4-14-select" class="headerlink" title="4.14 select()"></a>4.14 select()</h4><blockquote>
<p>选中符合条件的第一个节点</p>
</blockquote>
<blockquote>
<p>如果有多个节点符合条件，则只会返回一个</p>
</blockquote>
<blockquote>
<p>其参数是CSS选择器</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash)</span><br><span class="line">  splash:go(&quot;https:&#x2F;&#x2F;www.baidu.com&#x2F;&quot;)</span><br><span class="line">  input &#x3D; splash:select(&quot;#kw&quot;)</span><br><span class="line">  splash:wait(3)</span><br><span class="line">  return splash:png()</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h4 id="4-15-send-text"><a href="#4-15-send-text" class="headerlink" title="4.15 send_text()"></a>4.15 send_text()</h4><blockquote>
<p>填写文本</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash)</span><br><span class="line">  splash:go(&quot;https:&#x2F;&#x2F;www.baidu.com&#x2F;&quot;)</span><br><span class="line">  input &#x3D; splash:select(&quot;#kw&quot;)</span><br><span class="line">  input:send_text(&#39;Splash&#39;)</span><br><span class="line">  splash:wait(3)</span><br><span class="line">  return splash:png()</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h4 id="4-16-mouse-click"><a href="#4-16-mouse-click" class="headerlink" title="4.16 mouse_click()"></a>4.16 mouse_click()</h4><blockquote>
<p>模拟鼠标点击操作</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function main(splash)</span><br><span class="line">  splash:go(&quot;https:&#x2F;&#x2F;www.baidu.com&#x2F;&quot;)</span><br><span class="line">  input &#x3D; splash:select(&quot;#kw&quot;)</span><br><span class="line">  input:send_text(&#39;Splash&#39;)</span><br><span class="line">  submit &#x3D; splash:select(&#39;#su&#39;)</span><br><span class="line">  submit:mouse_click()</span><br><span class="line">  splash:wait(3)</span><br><span class="line">  return splash:png()</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<h3 id="5-Splash与Python结合"><a href="#5-Splash与Python结合" class="headerlink" title="5 Splash与Python结合"></a>5 Splash与Python结合</h3><h4 id="5-1-render-html"><a href="#5-1-render-html" class="headerlink" title="5.1 render.html"></a>5.1 render.html</h4><blockquote>
<p>此接口用于获取JavaScript渲染的页面的HTML代码，接口地址就是Splash的运行地址加此接口名称，例如<code>http://192.168.99.100:8050/render.html</code></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">url &#x3D; &#39;http:&#x2F;&#x2F;192.168.99.100:8050&#x2F;render.html?url&#x3D;https:&#x2F;&#x2F;www.bjsxt.com&amp;wait&#x3D;3&#39;</span><br><span class="line">response &#x3D; requests.get(url)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>
<h4 id="5-2-render-png"><a href="#5-2-render-png" class="headerlink" title="5.2 render.png"></a>5.2 render.png</h4><blockquote>
<p>此接口可以获取网页截图</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line"> </span><br><span class="line">url &#x3D; &#39;http:&#x2F;&#x2F;192.168.99.100:8050&#x2F;render.png?url&#x3D;https:&#x2F;&#x2F;www.jd.com&amp;wait&#x3D;5&amp;width&#x3D;1000&amp;height&#x3D;700&#39;</span><br><span class="line">response &#x3D; requests.get(url)</span><br><span class="line">with open(&#39;taobao.png&#39;, &#39;wb&#39;) as f:</span><br><span class="line">    f.write(response.content)</span><br></pre></td></tr></table></figure>

<h4 id="5-3-execute"><a href="#5-3-execute" class="headerlink" title="5.3 execute"></a>5.3 execute</h4><blockquote>
<p>最为强大的接口。前面说了很多Splash Lua脚本的操作，用此接口便可实现与Lua脚本的对接</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">from urllib.parse import quote</span><br><span class="line"> </span><br><span class="line">lua &#x3D; &#39;&#39;&#39;</span><br><span class="line">function main(splash)</span><br><span class="line">    return &#39;hello&#39;</span><br><span class="line">end</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"> </span><br><span class="line">url &#x3D; &#39;http:&#x2F;&#x2F;192.168.99.100:8050&#x2F;execute?lua_source&#x3D;&#39; + quote(lua)</span><br><span class="line">response &#x3D; requests.get(url)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>26. Scrapy 框架-模拟登录-Request、Response</title>
    <url>/2020/01/26/python%E7%88%AC%E8%99%AB/26.%20Scrapy%20%E6%A1%86%E6%9E%B6-%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95-Request%E3%80%81Response/</url>
    <content><![CDATA[<h3 id="1-Scrapy-Request和Response（请求和响应）"><a href="#1-Scrapy-Request和Response（请求和响应）" class="headerlink" title="1. Scrapy-Request和Response（请求和响应）"></a>1. Scrapy-Request和Response（请求和响应）</h3><p> Scrapy的Request和Response对象用于爬网网站。</p>
<p> 通常，Request对象在爬虫程序中生成并传递到系统，直到它们到达下载程序，后者执行请求并返回一个Response对象，该对象返回到发出请求的爬虫程序。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">爬虫-&gt;&gt;Request: 创建</span><br><span class="line">Request-&gt;&gt;Response:获取下载数据</span><br><span class="line">Response-&gt;&gt;爬虫:数据</span><br></pre></td></tr></table></figure>

<h3 id="2-Request对象"><a href="#2-Request对象" class="headerlink" title="2. Request对象"></a>2. Request对象</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class scrapy.http.Request(url[, callback, method&#x3D;&#39;GET&#39;, headers, body, cookies, meta, encoding&#x3D;&#39;utf-8&#39;, priority&#x3D;0, dont_filter&#x3D;False, errback])</span><br></pre></td></tr></table></figure>

<p>一个Request对象表示一个HTTP请求，它通常是在爬虫生成，并由下载执行，从而生成Response</p>
<ul>
<li><p>参数</p>
<ul>
<li><p>url（string） - 此请求的网址</p>
</li>
<li><p>callback（callable） - 将使用此请求的响应（一旦下载）作为其第一个参数调用的函数。有关更多信息，请参阅下面的将附加数据传递给回调函数。如果请求没有指定回调，parse()将使用spider的 方法。请注意，如果在处理期间引发异常，则会调用errback。</p>
</li>
<li><p>method（string） - 此请求的HTTP方法。默认为’GET’。可设置为”GET”, “POST”, “PUT”等，且保证字符串大写</p>
</li>
<li><p>meta（dict） - 属性的初始值Request.meta,在不同的请求之间传递数据使用</p>
</li>
<li><p>body（str或unicode） - 请求体。如果unicode传递了a，那么它被编码为 str使用传递的编码（默认为utf-8）。如果 body没有给出，则存储一个空字符串。不管这个参数的类型，存储的最终值将是一个str（不会是unicode或None）。</p>
</li>
<li><p>headers（dict） - 这个请求的头。dict值可以是字符串（对于单值标头）或列表（对于多值标头）。如果 None作为值传递，则不会发送HTTP头.一般不需要</p>
</li>
<li><p>encoding: 使用默认的 ‘utf-8’ 就行。</p>
</li>
<li><p>cookie（dict或list） - 请求cookie。这些可以以两种形式发送。</p>
<ul>
<li>使用dict：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">request_with_cookies &#x3D; Request(url&#x3D;&quot;http:&#x2F;&#x2F;www.sxt.cn&#x2F;index&#x2F;login&#x2F;login.html&quot;,)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>使用列表：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">request_with_cookies &#x3D; Request(url&#x3D;&quot;http:&#x2F;&#x2F;www.example.com&quot;,</span><br><span class="line">                             cookies&#x3D;[&#123;&#39;name&#39;: &#39;currency&#39;,</span><br><span class="line">                                      &#39;value&#39;: &#39;USD&#39;,</span><br><span class="line">                                      &#39;domain&#39;: &#39;example.com&#39;,</span><br><span class="line">                                      &#39;path&#39;: &#39;&#x2F;currency&#39;&#125;])</span><br></pre></td></tr></table></figure>
<p>后一种形式允许定制 cookie的属性domain和path属性。这只有在保存Cookie用于以后的请求时才有用</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">request_with_cookies &#x3D; Request(url&#x3D;&quot;http:&#x2F;&#x2F;www.example.com&quot;,</span><br><span class="line">                               cookies&#x3D;&#123;&#39;currency&#39;: &#39;USD&#39;, &#39;country&#39;: &#39;UY&#39;&#125;,</span><br><span class="line">                               meta&#x3D;&#123;&#39;dont_merge_cookies&#39;: True&#125;)</span><br></pre></td></tr></table></figure>

<h4 id="将附加数据传递给回调函数"><a href="#将附加数据传递给回调函数" class="headerlink" title="将附加数据传递给回调函数"></a>将附加数据传递给回调函数</h4><p>请求的回调是当下载该请求的响应时将被调用的函数。将使用下载的Response对象作为其第一个参数来调用回调函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def parse_page1(self, response):</span><br><span class="line">    item &#x3D; MyItem()</span><br><span class="line">    item[&#39;main_url&#39;] &#x3D; response.url</span><br><span class="line">    request &#x3D; scrapy.Request(&quot;http:&#x2F;&#x2F;www.example.com&#x2F;some_page.html&quot;,</span><br><span class="line">                             callback&#x3D;self.parse_page2)</span><br><span class="line">    request.meta[&#39;item&#39;] &#x3D; item</span><br><span class="line">    return request</span><br><span class="line"></span><br><span class="line">def parse_page2(self, response):</span><br><span class="line">    item &#x3D; response.meta[&#39;item&#39;]</span><br><span class="line">    item[&#39;other_url&#39;] &#x3D; response.url</span><br><span class="line">    return item</span><br></pre></td></tr></table></figure>
<h3 id="3-请求子类-FormRequest对象"><a href="#3-请求子类-FormRequest对象" class="headerlink" title="3 请求子类 FormRequest对象"></a>3 请求子类 FormRequest对象</h3><p>FormRequest类扩展了Request具有处理HTML表单的功能的基础。它使用lxml.html表单 从Response对象的表单数据预填充表单字段</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class scrapy.http.FormRequest(url[, formdata, ...])</span><br></pre></td></tr></table></figure>
<p>本FormRequest类增加了新的构造函数的参数。其余的参数与Request类相同，这里没有记录</p>
<ul>
<li>参数：formdata（元组的dict或iterable） - 是一个包含HTML Form数据的字典（或（key，value）元组的迭代），它将被url编码并分配给请求的主体。</li>
</ul>
<p>该FormRequest对象支持除标准以下类方法Request的方法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">classmethod from_response(response[, formname&#x3D;None, formid&#x3D;None, formnumber&#x3D;0, formdata&#x3D;None, formxpath&#x3D;None, formcss&#x3D;None, clickdata&#x3D;None, dont_click&#x3D;False, ...])</span><br></pre></td></tr></table></figure>

<p>返回一个新FormRequest对象，其中的表单字段值已预先<code>&lt;form&gt;</code>填充在给定响应中包含的HTML 元素中.</p>
<p>参数：</p>
<ul>
<li>response（Responseobject） - 包含将用于预填充表单字段的HTML表单的响应</li>
<li>formname（string） - 如果给定，将使用name属性设置为此值的形式</li>
<li>formid（string） - 如果给定，将使用id属性设置为此值的形式</li>
<li>formxpath（string） - 如果给定，将使用匹配xpath的第一个表单</li>
<li>formcss（string） - 如果给定，将使用匹配css选择器的第一个形式</li>
<li>formnumber（integer） - 当响应包含多个表单时要使用的表单的数量。第一个（也是默认）是0</li>
<li>formdata（dict） - 要在表单数据中覆盖的字段。如果响应元素中已存在字段，则其值将被在此参数中传递的值覆盖</li>
<li>clickdata（dict） - 查找控件被点击的属性。如果没有提供，表单数据将被提交，模拟第一个可点击元素的点击。除了html属性，控件可以通过其相对于表单中其他提交表输入的基于零的索引，通过nr属性来标识</li>
<li>dont_click（boolean） - 如果为True，表单数据将在不点击任何元素的情况下提交</li>
</ul>
<h4 id="3-1-请求使用示例"><a href="#3-1-请求使用示例" class="headerlink" title="3.1 请求使用示例"></a>3.1 请求使用示例</h4><p>使用FormRequest通过HTTP POST发送数据</p>
<p>如果你想在你的爬虫中模拟HTML表单POST并发送几个键值字段，你可以返回一个FormRequest对象（从你的爬虫）像这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">return [FormRequest(url&#x3D;&quot;http:&#x2F;&#x2F;www.example.com&#x2F;post&#x2F;action&quot;,</span><br><span class="line">                    formdata&#x3D;&#123;&#39;name&#39;: &#39;John Doe&#39;, &#39;age&#39;: &#39;27&#39;&#125;,</span><br><span class="line">                    callback&#x3D;self.after_post)]</span><br></pre></td></tr></table></figure>
<p>使用FormRequest.from_response（）来模拟用户登录</p>
<p>网站通常通过元素（例如会话相关数据或认证令牌（用于登录页面））提供预填充的表单字段。进行剪贴时，您需要自动预填充这些字段，并且只覆盖其中的一些，例如用户名和密码。您可以使用 此作业的方法。这里有一个使用它的爬虫示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;input type&#x3D;&quot;hidden&quot;&gt; FormRequest.from_response()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class LoginSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &#39;example.com&#39;</span><br><span class="line">    start_urls &#x3D; [&#39;http:&#x2F;&#x2F;www.example.com&#x2F;users&#x2F;login.php&#39;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        return scrapy.FormRequest.from_response(</span><br><span class="line">            response,</span><br><span class="line">            formdata&#x3D;&#123;&#39;username&#39;: &#39;john&#39;, &#39;password&#39;: &#39;secret&#39;&#125;,</span><br><span class="line">            callback&#x3D;self.after_login</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def after_login(self, response):</span><br><span class="line">        # check login succeed before going on</span><br><span class="line">        if &quot;authentication failed&quot; in response.body:</span><br><span class="line">            self.logger.error(&quot;Login failed&quot;)</span><br><span class="line">            return</span><br><span class="line"></span><br><span class="line">        # continue scraping with authenticated session...</span><br></pre></td></tr></table></figure>
<h3 id="4-响应对象"><a href="#4-响应对象" class="headerlink" title="4 响应对象"></a>4 响应对象</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class scrapy.http.Response(url[, status&#x3D;200, headers&#x3D;None, body&#x3D;b&#39;&#39;, flags&#x3D;None, request&#x3D;None])</span><br></pre></td></tr></table></figure>
<p>一个Response对象表示的HTTP响应，这通常是下载（由下载），并供给到爬虫进行处理</p>
<p>参数：</p>
<ul>
<li>url（string） - 此响应的URL</li>
<li>status（integer） - 响应的HTTP状态。默认为200</li>
<li>headers（dict） - 这个响应的头。dict值可以是字符串（对于单值标头）或列表（对于多值标头）</li>
<li>body（str） - 响应体。它必须是str，而不是unicode，除非你使用一个编码感知响应子类，如 TextResponse</li>
<li>flags（list） - 是一个包含属性初始值的 Response.flags列表。如果给定，列表将被浅复制</li>
<li>request（Requestobject） - 属性的初始值Response.request。这代表Request生成此响应</li>
</ul>
<h3 id="5-模拟登录"><a href="#5-模拟登录" class="headerlink" title="5 模拟登录"></a>5 模拟登录</h3><p><strong>用的函数：</strong></p>
<ul>
<li><p>start_requests()可以返回一个请求给爬虫的起始网站，这个返回的请求相当于start_urls，start_requests()返回的请求会替代start_urls里的请求</p>
</li>
<li><p>Request()get请求，可以设置，url、cookie、回调函数</p>
</li>
<li><p>FormRequest.from_response()表单post提交，第一个必须参数，上一次响应cookie的response对象，其他参数，cookie、url、表单内容等</p>
</li>
<li><p>yield Request()可以将一个新的请求返回给爬虫执行</p>
</li>
</ul>
<p><strong>在发送请求时cookie的操作，</strong></p>
<ul>
<li>meta={‘cookiejar’:1}表示开启cookie记录，首次请求时写在Request()里</li>
<li>meta={‘cookiejar’:response.meta[‘cookiejar’]}表示使用上一次response的cookie，写在FormRequest.from_response()里post授权</li>
<li>meta={‘cookiejar’:True}表示使用授权后的cookie访问需要登录查看的页面</li>
</ul>
<p><strong>获取Scrapy框架Cookies</strong></p>
<p><strong>样例代码</strong></p>
<p><code>start_requests()</code>方法，可以返回一个请求给爬虫的起始网站，这个返回的请求相当于start_urls，start_requests()返回的请求会替代start_urls里的请求</p>
<p>在发送请求时cookie的操作</p>
<p><code>meta={&#39;cookiejar&#39;:1}</code>表示开启cookie记录，首次请求时写在Request()里</p>
<p><code>meta={&#39;cookiejar&#39;:response.meta[&#39;cookiejar&#39;]}</code>表示使用上一次response的cookie，写在Request里post授权</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy import Request</span><br><span class="line">from scrapy import FormRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class SxtSpiderSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &#39;sxt1&#39;</span><br><span class="line">    allowed_domains &#x3D; [&#39;sxt.cn&#39;]</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        return [Request(&#39;http:&#x2F;&#x2F;www.sxt.cn&#x2F;index&#x2F;login&#x2F;login.html&#39;, meta&#x3D;&#123;&#39;cookiejar&#39;: 1&#125;, callback&#x3D;self.parse)]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        formdata &#x3D; &#123;</span><br><span class="line">            &quot;user&quot;: &quot;17703181473&quot;, &quot;password&quot;: &quot;123456&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        return FormRequest(                                        formdata&#x3D;formdata,</span><br><span class="line">                                        url&#x3D;&#39;http:&#x2F;&#x2F;www.sxt.cn&#x2F;index&#x2F;login&#x2F;login.html&#39;,</span><br><span class="line">                                        meta&#x3D;&#123;&#39;cookiejar&#39;: response.meta[&#39;cookiejar&#39;]&#125;,</span><br><span class="line">                                        callback&#x3D;self.login_after)</span><br><span class="line"></span><br><span class="line">    def login_after(self, response):</span><br><span class="line">        yield scrapy.Request(&#39;http:&#x2F;&#x2F;www.sxt.cn&#x2F;index&#x2F;user.html&#39;,</span><br><span class="line">                             meta&#x3D;&#123;&quot;cookiejar&quot;: response.meta[&#39;cookiejar&#39;]&#125;,</span><br><span class="line">                             callback&#x3D;self.next)</span><br><span class="line">    def next(self,response):</span><br><span class="line">        print(response.text)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>25. Scrapy 框架-下载中间件Middleware</title>
    <url>/2020/01/25/python%E7%88%AC%E8%99%AB/25.%20Scrapy%20%E6%A1%86%E6%9E%B6-%E4%B8%8B%E8%BD%BD%E4%B8%AD%E9%97%B4%E4%BB%B6Middleware/</url>
    <content><![CDATA[<h3 id="1-Spider-下载中间件-Middleware"><a href="#1-Spider-下载中间件-Middleware" class="headerlink" title="1. Spider 下载中间件(Middleware)"></a>1. Spider 下载中间件(Middleware)</h3><p>Spider 中间件(Middleware) 下载器中间件是介入到 Scrapy 的 spider 处理机制的钩子框架，您可以添加代码来处理发送给 Spiders 的 response 及 spider 产生的 item 和 request</p>
<h3 id="2-激活一个下载DOWNLOADER-MIDDLEWARES"><a href="#2-激活一个下载DOWNLOADER-MIDDLEWARES" class="headerlink" title="2. 激活一个下载DOWNLOADER_MIDDLEWARES"></a>2. 激活一个下载DOWNLOADER_MIDDLEWARES</h3><p>要激活一个下载器中间件组件，将其添加到 <code>DOWNLOADER_MIDDLEWARES</code>设置中，该设置是一个字典，其键是中间件类路径，它们的值是中间件命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES  &#x3D;  &#123; </span><br><span class="line">    &#39;myproject.middlewares.CustomDownloaderMiddleware&#39; ： 543 ，</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该<code>DOWNLOADER_MIDDLEWARES</code>设置与<code>DOWNLOADER_MIDDLEWARES_BASEScrapy</code>中定义的设置（并不意味着被覆盖）合并， 然后按顺序排序，以获得最终的已启用中间件的排序列表：第一个中间件是靠近引擎的第一个中间件，最后一个是靠近引擎的中间件到下载器。换句话说，<code>process_request()</code> 每个中间件的方法将以增加中间件的顺序（100,200,300，…）<code>process_response()</code>被调用，并且每个中间件的方法将以降序调用</p>
<p>要决定分配给中间件的顺序，请参阅 <code>DOWNLOADER_MIDDLEWARES_BASE</code>设置并根据要插入中间件的位置选择一个值。顺序很重要，因为每个中间件都执行不同的操作，而您的中间件可能依赖于之前（或后续）正在使用的中间件</p>
<p>如果要禁用内置中间件（<code>DOWNLOADER_MIDDLEWARES_BASE</code>默认情况下已定义和启用的中间件 ），则必须在项目<code>DOWNLOADER_MIDDLEWARES</code>设置中定义它，并将“ 无” 作为其值。例如，如果您要禁用用户代理中间件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES  &#x3D;  &#123; </span><br><span class="line">    &#39;myproject.middlewares.CustomDownloaderMiddleware&#39; ： 543 ，</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39; ： None ，</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后，请记住，某些中间件可能需要通过特定设置启用</p>
<h3 id="3-编写你自己的下载中间件"><a href="#3-编写你自己的下载中间件" class="headerlink" title="3. 编写你自己的下载中间件"></a>3. 编写你自己的下载中间件</h3><p>每个中间件组件都是一个Python类，它定义了一个或多个以下方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class scrapy.downloadermiddlewares.DownloaderMiddleware</span><br></pre></td></tr></table></figure>
<blockquote>
<p>任何下载器中间件方法也可能返回一个延迟</p>
</blockquote>
<h4 id="3-1-process-request-self-request-spider"><a href="#3-1-process-request-self-request-spider" class="headerlink" title="3.1 process_request(self, request, spider)"></a>3.1 process_request(self, request, spider)</h4><blockquote>
<p>当每个request通过下载中间件时，该方法被调用</p>
</blockquote>
<p>process_request()必须返回其中之一</p>
<ul>
<li>返回 None<ul>
<li>Scrapy 将继续处理该 request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用，该 request 被执行(其 response 被下载)</li>
</ul>
</li>
<li>返回一个 Response 对象<ul>
<li>Scrapy 将不会调用 任何 其他的 process_request()或 process_exception()方法，或相应地下载函数； 其将返回该 response。已安装的中间件的 process_response()方法则会在每个 response 返回时被调用</li>
</ul>
</li>
<li>返回一个 Request 对象<ul>
<li>Scrapy 则停止调用 process_request 方法并重新调度返回的 request。当新返回的 request 被执行后， 相应地中间件链将会根据下载的 response 被调用</li>
</ul>
</li>
<li>raise IgnoreRequest<ul>
<li>如果抛出 一个 IgnoreRequest 异常，则安装的下载中间件的 process_exception() 方法会被调用。如果没有任何一个方法处理该异常， 则 request 的 errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)</li>
</ul>
</li>
</ul>
<p>参数:</p>
<ul>
<li>request (Request 对象) – 处理的request</li>
<li>spider (Spider 对象) – 该request对应的spider<h4 id="3-2-process-response-self-request-response-spider"><a href="#3-2-process-response-self-request-response-spider" class="headerlink" title="3.2 process_response(self, request, response, spider)"></a>3.2 process_response(self, request, response, spider)</h4></li>
</ul>
<blockquote>
<p>当下载器完成http请求，传递响应给引擎的时候调用</p>
</blockquote>
<ul>
<li><p>process_request() 必须返回以下其中之一: 返回一个 Response 对象、 返回一个 Request 对象或raise一个 IgnoreRequest 异常</p>
<ul>
<li><p>如果其返回一个 Response (可以与传入的response相同，也可以是全新的对象)， 该response会被在链中的其他中间件的 process_response() 方法处理。</p>
</li>
<li><p>如果其返回一个 Request 对象，则中间件链停止， 返回的request会被重新调度下载。处理类似于 process_request() 返回request所做的那样。</p>
</li>
<li><p>如果其抛出一个 IgnoreRequest 异常，则调用request的errback(Request.errback)。 如果没有代码处理抛出的异常，则该异常被忽略且不记录(不同于其他异常那样)。</p>
</li>
</ul>
</li>
<li><p>参数:</p>
<ul>
<li>request (Request 对象) – response所对应的request</li>
<li>response (Response 对象) – 被处理的response</li>
<li>spider (Spider 对象) – response所对应的spider</li>
</ul>
</li>
</ul>
<h3 id="4-使用代理"><a href="#4-使用代理" class="headerlink" title="4 使用代理"></a>4 使用代理</h3><p>settings.py</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PROXIES&#x3D;[</span><br><span class="line">    &#123;&quot;ip&quot;:&quot;122.236.158.78:8118&quot;&#125;,</span><br><span class="line">    &#123;&quot;ip&quot;:&quot;112.245.78.90:8118&quot;&#125;</span><br><span class="line">]</span><br><span class="line">DOWNLOADER_MIDDLEWARES &#x3D; &#123;</span><br><span class="line">    #&#39;xiaoshuo.middlewares.XiaoshuoDownloaderMiddleware&#39;: 543,</span><br><span class="line">    &#39;xiaoshuo.proxyMidde.ProxyMidde&#39;:100</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>创建一个midderwares</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from xiaoshuo.settings import PROXIES</span><br><span class="line">import random</span><br><span class="line">class ProxyMidde(object):</span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">            proxy &#x3D; random.choice(PROXIES)</span><br><span class="line">            request.meta[&#39;proxy&#39;]&#x3D;&#39;http:&#x2F;&#x2F;&#39;+proxy[&#39;ip&#39;]</span><br></pre></td></tr></table></figure>
<p>写一个spider测试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from scrapy import Spider</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ProxyIp(Spider):</span><br><span class="line">    name &#x3D; &#39;ip&#39;</span><br><span class="line">    #http:&#x2F;&#x2F;www.882667.com&#x2F;</span><br><span class="line">    start_urls &#x3D; [&#39;http:&#x2F;&#x2F;ip.cn&#39;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        print(response.text)</span><br></pre></td></tr></table></figure>

<h3 id="5-使用动态UA"><a href="#5-使用动态UA" class="headerlink" title="5 使用动态UA"></a>5 使用动态UA</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 随机的User-Agent</span><br><span class="line">class RandomUserAgent(object):</span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        useragent &#x3D; random.choice(USER_AGENTS)</span><br><span class="line"></span><br><span class="line">        request.headers.setdefault(&quot;User-Agent&quot;, useragent)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>24. Scrapy 框架-图片管道使用</title>
    <url>/2020/01/24/python%E7%88%AC%E8%99%AB/24.%20Scrapy%20%E6%A1%86%E6%9E%B6-%E5%9B%BE%E7%89%87%E7%AE%A1%E9%81%93%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h3><p>Scrapy提供了一个 item pipeline ，来下载属于某个特定项目的图片，比如，当你抓取产品时，也想把它们的图片下载到本地。</p>
<p>这条管道，被称作图片管道，在 <code>ImagesPipeline</code> 类中实现，提供了一个方便并具有额外特性的方法，来下载并本地存储图片:</p>
<ul>
<li>将所有下载的图片转换成通用的格式（JPG）和模式（RGB）</li>
<li>避免重新下载最近已经下载过的图片</li>
<li>缩略图生成</li>
<li>检测图像的宽/高，确保它们满足最小限制</li>
</ul>
<p>这个管道也会为那些当前安排好要下载的图片保留一个内部队列，并将那些到达的包含相同图片的项目连接到那个队列中。 这可以避免多次下载几个项目共享的同一个图片</p>
<h3 id="2-使用图片管道"><a href="#2-使用图片管道" class="headerlink" title="2. 使用图片管道"></a>2. 使用图片管道</h3><p>当使用 ImagesPipeline ，典型的工作流程如下所示:</p>
<ol>
<li>在一个爬虫里，你抓取一个项目，把其中图片的URL放入 image_urls 组内</li>
<li>项目从爬虫内返回，进入项目管道</li>
<li>当项目进入 ImagesPipeline，image_urls 组内的URLs将被Scrapy的调度器和下载器（这意味着调度器和下载器的中间件可以复用）安排下载，当优先级更高，会在其他页面被抓取前处理。项目会在这个特定的管道阶段保持“locker”的状态，直到完成图片的下载（或者由于某些原因未完成下载）。</li>
<li>当图片下载完，另一个组(images)将被更新到结构中。这个组将包含一个字典列表，其中包括下载图片的信息，比如下载路径、源抓取地址（从 image_urls 组获得）和图片的校验码。 images 列表中的图片顺序将和源 image_urls 组保持一致。如果某个图片下载失败，将会记录下错误信息，图片也不会出现在 images 组中</li>
</ol>
<h3 id="3-具体流程-此处以zol网站为例"><a href="#3-具体流程-此处以zol网站为例" class="headerlink" title="3. 具体流程(此处以zol网站为例)"></a>3. 具体流程(此处以zol网站为例)</h3><ol>
<li><p>定义item</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ImagedownloadItem(scrapy.Item):</span><br><span class="line">    # define the fields for your item here like:</span><br><span class="line">    img_name &#x3D; scrapy.Field()</span><br><span class="line">    img_urls &#x3D;scrapy.Field()</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写spider</p>
<blockquote>
<p>思路：获取文件地址–&gt;获取图片名称–&gt;推送地址</p>
</blockquote>
</li>
</ol>
<p>此处是一张一张的推送</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class ZolSpiderSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &#39;zol&#39;</span><br><span class="line">    allowed_domains &#x3D; [&#39;zol.com.cn&#39;]</span><br><span class="line">    url &#x3D;&#39;http:&#x2F;&#x2F;desk.zol.com.cn&#39;</span><br><span class="line">    start_urls &#x3D; [url+&#39;&#x2F;bizhi&#x2F;7106_88025_2.html&#39;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        image_url &#x3D; response.xpath(&#39;&#x2F;&#x2F;img[@id&#x3D;&quot;bigImg&quot;]&#x2F;@src&#39;).extract_first()</span><br><span class="line">        image_name &#x3D; response.xpath(&#39;&#x2F;&#x2F;h3&#39;)[0].xpath(&#39;string(.)&#39;).extract_first().strip().replace(&#39;\r\n\t\t&#39;, &#39;&#39;)</span><br><span class="line">        next_image &#x3D; response.xpath(&#39;&#x2F;&#x2F;a[@id&#x3D;&quot;pageNext&quot;]&#x2F;@href&#39;).extract_first()</span><br><span class="line">        item &#x3D; ImagedownloadItem()</span><br><span class="line">        item[&quot;img_name&quot;] &#x3D; image_name</span><br><span class="line">        item[&quot;img_urls&quot;] &#x3D; image_url</span><br><span class="line">        yield item</span><br><span class="line"></span><br><span class="line">        yield scrapy.Request(self.url+next_image,callback&#x3D;self.parse,)</span><br></pre></td></tr></table></figure>
<ol start="3">
<li><p>编写pipline</p>
<p>以下如果不想改文件名，meta属性可以忽略不写</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_media_requests(self, item, info):</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    #如果item[urls]里里面是列表，用下面</span><br><span class="line">    urls&#x3D; item[&#39;urls&#39;]</span><br><span class="line">    for url in urls:</span><br><span class="line">        yield scrapy.Request(url,meta&#x3D;&#123;&quot;item&quot;,item&#125;)</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    # 如果item[urls]里里面是一个图片地址，用这下面的</span><br><span class="line">    yield scrapy.Request(item[&#39;img_urls&#39;], meta&#x3D;&#123;&quot;item&quot;: item&#125;)</span><br></pre></td></tr></table></figure>

<p>因为scrapy里是使用它们URL的 SHA1 hash 作为文件名，所以如果想重命名：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def file_path(self, request, response&#x3D;None, info&#x3D;None):</span><br><span class="line">      item &#x3D; request.meta[&quot;item&quot;]</span><br><span class="line">      #去掉文件里的&#x2F;,避免创建图片文件时出错</span><br><span class="line">      filename &#x3D; item[&quot;img_name&quot;].replace(&quot;&#x2F;&quot;,&quot;-&quot;)+&quot;.jpg&quot;</span><br><span class="line"></span><br><span class="line">      return filename</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>定义图片保存在哪？<br>在settings中增加一句<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">IMAGES_STORE &#x3D; &quot;e:&#x2F;pics&quot;</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>23. Scrapy 框架-CrawlSpider</title>
    <url>/2020/01/23/python%E7%88%AC%E8%99%AB/23.%20Scrapy%20%E6%A1%86%E6%9E%B6-CrawlSpider/</url>
    <content><![CDATA[<h3 id="1-CrawlSpiders"><a href="#1-CrawlSpiders" class="headerlink" title="1. CrawlSpiders"></a>1. CrawlSpiders</h3><h4 id="原理图"><a href="#原理图" class="headerlink" title="原理图"></a>原理图</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">start_urls -&gt;&gt;调度器: 初始化url</span><br><span class="line">调度器-&gt;&gt;下载器: request</span><br><span class="line">下载器-&gt;&gt;rules: response</span><br><span class="line">rules-&gt;&gt;数据提取: response</span><br><span class="line">rules-&gt;&gt;调度器: 新的url</span><br></pre></td></tr></table></figure>

<p>通过下面的命令可以快速创建 CrawlSpider模板 的代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy genspider -t crawl 文件名 (allowed_url)</span><br></pre></td></tr></table></figure>
<p>首先在说下Spider，它是所有爬虫的基类，而CrawSpiders就是Spider的派生类。对于设计原则是只爬取start_url列表中的网页，而从爬取的网页中获取link并继续爬取的工作CrawlSpider类更适合</p>
<h3 id="2-Rule对象"><a href="#2-Rule对象" class="headerlink" title="2. Rule对象"></a>2. Rule对象</h3><p>Rule类与CrawlSpider类都位于scrapy.contrib.spiders模块中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class scrapy.contrib.spiders.Rule (  </span><br><span class="line">link_extractor, callback&#x3D;None,cb_kwargs&#x3D;None,follow&#x3D;None,process_links&#x3D;None,process_request&#x3D;None )</span><br></pre></td></tr></table></figure>
<p>参数含义：</p>
<ul>
<li><p>link_extractor为LinkExtractor，用于定义需要提取的链接</p>
</li>
<li><p>callback参数：当link_extractor获取到链接时参数所指定的值作为回调函数</p>
<ul>
<li>callback参数使用注意：<br>当编写爬虫规则时，请避免使用parse作为回调函数。于CrawlSpider使用parse方法来实现其逻辑，如果您覆盖了parse方法，crawlspider将会运行失败</li>
</ul>
</li>
<li><p>follow：指定了根据该规则从response提取的链接是否需要跟进。当callback为None,默认值为True</p>
</li>
<li><p>process_links：主要用来过滤由link_extractor获取到的链接</p>
</li>
<li><p>process_request：主要用来过滤在rule中提取到的request</p>
</li>
</ul>
<h3 id="3-LinkExtractors"><a href="#3-LinkExtractors" class="headerlink" title="3.LinkExtractors"></a>3.LinkExtractors</h3><h4 id="3-1-概念"><a href="#3-1-概念" class="headerlink" title="3.1 概念"></a>3.1 概念</h4><blockquote>
<p>顾名思义，链接提取器</p>
</blockquote>
<h4 id="3-2-作用"><a href="#3-2-作用" class="headerlink" title="3.2 作用"></a>3.2 作用</h4><p>response对象中获取链接，并且该链接会被接下来爬取<br>每个LinkExtractor有唯一的公共方法是 extract_links()，它接收一个 Response 对象，并返回一个 scrapy.link.Link 对象</p>
<h4 id="3-3-使用"><a href="#3-3-使用" class="headerlink" title="3.3 使用"></a>3.3 使用</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class scrapy.linkextractors.LinkExtractor(</span><br><span class="line">    allow &#x3D; (),</span><br><span class="line">    deny &#x3D; (),</span><br><span class="line">    allow_domains &#x3D; (),</span><br><span class="line">    deny_domains &#x3D; (),</span><br><span class="line">    deny_extensions &#x3D; None,</span><br><span class="line">    restrict_xpaths &#x3D; (),</span><br><span class="line">    tags &#x3D; (&#39;a&#39;,&#39;area&#39;),</span><br><span class="line">    attrs &#x3D; (&#39;href&#39;),</span><br><span class="line">    canonicalize &#x3D; True,</span><br><span class="line">    unique &#x3D; True,</span><br><span class="line">    process_value &#x3D; None</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>主要参数：</p>
<ul>
<li><p>allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。</p>
</li>
<li><p>deny：与这个正则表达式(或正则表达式列表)不匹配的URL一定不提取。</p>
</li>
<li><p>allow_domains：会被提取的链接的domains。</p>
</li>
<li><p>deny_domains：一定不会被提取链接的domains。</p>
</li>
<li><p>restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接(只选到节点，不选到属性)</p>
</li>
</ul>
<h5 id="3-3-1-查看效果（shell中验证"><a href="#3-3-1-查看效果（shell中验证" class="headerlink" title="3.3.1 查看效果（shell中验证)"></a>3.3.1 查看效果（shell中验证)</h5><p>首先运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy shell http:&#x2F;&#x2F;www.fhxiaoshuo.com&#x2F;read&#x2F;33&#x2F;33539&#x2F;17829387.shtml</span><br></pre></td></tr></table></figure>

<p>继续import相关模块：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from scrapy.linkextractors import LinkExtractor</span><br></pre></td></tr></table></figure>
<p>提取当前网页中获得的链接</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">link &#x3D; LinkExtractor(restrict_xpaths&#x3D;(r&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;bottem&quot;]&#x2F;a[4]&#39;)</span><br></pre></td></tr></table></figure>
<p>调用LinkExtractor实例的extract_links()方法查询匹配结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">link.extract_links(response)</span><br></pre></td></tr></table></figure>

<h5 id="3-3-2-查看效果-CrawlSpider版本"><a href="#3-3-2-查看效果-CrawlSpider版本" class="headerlink" title="3.3.2 查看效果 CrawlSpider版本"></a>3.3.2 查看效果 CrawlSpider版本</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line">from xiaoshuo.items import XiaoshuoItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class XiaoshuoSpiderSpider(CrawlSpider):</span><br><span class="line">    name &#x3D; &#39;xiaoshuo_spider&#39;</span><br><span class="line">    allowed_domains &#x3D; [&#39;fhxiaoshuo.com&#39;]</span><br><span class="line">    start_urls &#x3D; [&#39;http:&#x2F;&#x2F;www.fhxiaoshuo.com&#x2F;read&#x2F;33&#x2F;33539&#x2F;17829387.shtml&#39;]</span><br><span class="line"></span><br><span class="line">    rules &#x3D; [</span><br><span class="line">        Rule(LinkExtractor(restrict_xpaths&#x3D;(r&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;bottem&quot;]&#x2F;a[4]&#39;)), callback&#x3D;&#39;parse_item&#39;),]</span><br><span class="line"></span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        info &#x3D; response.xpath(&quot;&#x2F;&#x2F;div[@id&#x3D;&#39;TXT&#39;]&#x2F;text()&quot;).extract()</span><br><span class="line">        it &#x3D; XiaoshuoItem()</span><br><span class="line">        it[&#39;info&#39;] &#x3D; info</span><br><span class="line">        yield it</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rules &#x3D; [</span><br><span class="line">       Rule(LinkExtractor(restrict_xpaths&#x3D;(r&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;bottem&quot;]&#x2F;a[4]&#39;)), callback&#x3D;&#39;parse_item&#39;),]</span><br></pre></td></tr></table></figure>
<ul>
<li>callback后面函数名用引号引起</li>
<li>函数名不能是parse</li>
<li>格式问题</li>
</ul>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>22. Scrapy 框架-案例实现</title>
    <url>/2020/01/22/python%E7%88%AC%E8%99%AB/22.%20Scrapy%20%E6%A1%86%E6%9E%B6-%E6%A1%88%E4%BE%8B%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h3 id="爬取小说"><a href="#爬取小说" class="headerlink" title="爬取小说"></a>爬取小说</h3><p>spider</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from xiaoshuo.items import XiaoshuoItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class XiaoshuoSpiderSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &#39;xiaoshuo_spider&#39;</span><br><span class="line">    allowed_domains &#x3D; [&#39;zy200.com&#39;]</span><br><span class="line">    url &#x3D; &#39;http:&#x2F;&#x2F;www.zy200.com&#x2F;5&#x2F;5943&#x2F;&#39;</span><br><span class="line">    start_urls &#x3D; [url + &#39;11667352.html&#39;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        info &#x3D; response.xpath(&quot;&#x2F;html&#x2F;body&#x2F;div[@id&#x3D;&#39;content&#39;]&#x2F;text()&quot;).extract()</span><br><span class="line">        href &#x3D; response.xpath(&quot;&#x2F;&#x2F;div[@class&#x3D;&#39;zfootbar&#39;]&#x2F;a[3]&#x2F;@href&quot;).extract_first()</span><br><span class="line">        xs_item &#x3D; XiaoshuoItem()</span><br><span class="line">        xs_item[&#39;content&#39;] &#x3D; info</span><br><span class="line">        yield xs_item</span><br><span class="line"></span><br><span class="line">        if href !&#x3D; &#39;index.html&#39;:</span><br><span class="line">            new_url &#x3D; self.url + href</span><br><span class="line">            yield scrapy.Request(new_url, callback&#x3D;self.parse)</span><br></pre></td></tr></table></figure>

<p>items</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class XiaoshuoItem(scrapy.Item):</span><br><span class="line">    # define the fields for your item here like:</span><br><span class="line">    content &#x3D; scrapy.Field()</span><br><span class="line">    href &#x3D; scrapy.Field()</span><br></pre></td></tr></table></figure>

<p>pipeline</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class XiaoshuoPipeline(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.filename &#x3D; open(&quot;dp1.txt&quot;, &quot;w&quot;, encoding&#x3D;&quot;utf-8&quot;)</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        content &#x3D; item[&quot;title&quot;] + item[&quot;content&quot;] + &#39;\n&#39;</span><br><span class="line">        self.filename.write(content)</span><br><span class="line">        self.filename.flush()</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.filename.close()</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>21. Scrapy 框架 - settings</title>
    <url>/2020/01/21/python%E7%88%AC%E8%99%AB/21.%20Scrapy%20%E6%A1%86%E6%9E%B6%20-%20settings/</url>
    <content><![CDATA[<h3 id="Scrapy内置设置"><a href="#Scrapy内置设置" class="headerlink" title="Scrapy内置设置"></a>Scrapy内置设置</h3><p>下面给出scrapy提供的常用内置设置列表,你可以在settings.py文件里面修改这些设置，以应用或者禁用这些设置项</p>
<ul>
<li><p>BOT_NAME</p>
<p>  默认: ‘scrapybot’</p>
<p>  Scrapy项目实现的bot的名字。用来构造默认 User-Agent，同时也用来log。<br>  当你使用 startproject 命令创建项目时其也被自动赋值。</p>
</li>
<li><p>CONCURRENT_ITEMS</p>
<p>  默认: 100</p>
<p>  Item Processor(即 Item Pipeline) 同时处理(每个response的)item的最大值</p>
</li>
<li><p>CONCURRENT_REQUESTS</p>
<p>  默认: 16</p>
<p>  Scrapy downloader 并发请求(concurrent requests)的最大值。</p>
</li>
<li><p>CONCURRENT_REQUESTS_PER_DOMAIN</p>
<p>  默认: 8</p>
<p>  对单个网站进行并发请求的最大值。</p>
</li>
<li><p>CONCURRENT_REQUESTS_PER_IP</p>
<p>  默认: 0</p>
<p>  对单个IP进行并发请求的最大值。如果非0，则忽略 CONCURRENT_REQUESTS_PER_DOMAIN 设定， 使用该设定。 也就是说，并发限制将针对IP，而不是网站。</p>
<p>  该设定也影响 DOWNLOAD_DELAY: 如果 CONCURRENT_REQUESTS_PER_IP 非0，下载延迟应用在IP而不是网站上。</p>
</li>
<li><p>DEFAULT_ITEM_CLASS</p>
<p>  默认: ‘scrapy.item.Item’</p>
<p>  the Scrapy shell 中实例化item使用的默认类</p>
</li>
<li><p>DEFAULT_REQUEST_HEADERS</p>
<p>  默认:</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &#39;Accept&#39;: &#39;text&#x2F;html,application&#x2F;xhtml+xml,application&#x2F;xml;q&#x3D;0.9,*&#x2F;*;q&#x3D;0.8&#39;,</span><br><span class="line">    &#39;Accept-Language&#39;: &#39;en&#39;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  Scrapy HTTP Request使用的默认header。由 DefaultHeadersMiddleware 产生。</p>
</li>
<li><p>DOWNLOADER</p>
<p>  默认: ‘scrapy.core.downloader.Downloader’</p>
<p>  用于crawl的downloader.</p>
</li>
<li><p>DOWNLOADER_MIDDLEWARES</p>
<p>  默认:: {}</p>
<p>  保存项目中启用的下载中间件及其顺序的字典</p>
</li>
<li><p>DOWNLOAD_DELAY</p>
<p>  默认: 0</p>
<p>  下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度， 减轻服务器压力。同时也支持小数</p>
</li>
<li><p>DOWNLOAD_HANDLERS</p>
<p>  默认: {}</p>
<p>  保存项目中启用的下载处理器(request downloader handler)的字典</p>
</li>
<li><p>DOWNLOAD_TIMEOUT</p>
<p>  默认: 180</p>
<p>  下载器超时时间(单位: 秒)</p>
</li>
<li><p>EXTENSIONS</p>
<p>  默认:{}</p>
<p>  保存项目中启用的插件及其顺序的字典</p>
</li>
<li><p>ITEM_PIPELINES</p>
<p>  默认: {}</p>
<p>  保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意。 不过值(value)习惯设定在0-1000范围内</p>
</li>
<li><p>ITEM_PIPELINES_BASE</p>
<p>  默认: {}</p>
<p>  保存项目中默认启用的pipeline的字典。 永远不要在项目中修改该设定，而是修改 ITEM_PIPELINES</p>
</li>
<li><p>LOG_ENABLED</p>
<p>  默认: True</p>
<p>  是否启用logging</p>
</li>
<li><p>LOG_ENCODING</p>
<p>  默认: ‘utf-8’</p>
<p>  logging使用的编码。</p>
</li>
<li><p>LOG_FILE</p>
<p>  默认: None</p>
<p>  logging输出的文件名。如果为None，则使用标准错误输出(standard error)。</p>
</li>
<li><p>LOG_FORMAT</p>
<p>  默认: ‘%(asctime)s [%(name)s] %(levelname)s: %(message)s’</p>
<p>  日志的数据格式</p>
</li>
<li><p>LOG_DATEFORMAT</p>
<p>  默认: ‘%Y-%m-%d %H:%M:%S’</p>
<p>  日志的日期格式</p>
</li>
<li><p>LOG_LEVEL</p>
<p>  默认: ‘DEBUG’</p>
<p>  log的最低级别。可选的级别有: CRITICAL、 ERROR、WARNING、INFO、DEBUG</p>
</li>
<li><p>LOG_STDOUT</p>
<p>  默认: False</p>
<p>  如果为 True ，进程所有的标准输出(及错误)将会被重定向到log中</p>
</li>
<li><p>RANDOMIZE_DOWNLOAD_DELAY</p>
<p>  默认: True</p>
<p>  如果启用，当从相同的网站获取数据时，Scrapy将会等待一个随机的值 (0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)</p>
<p>  该随机值降低了crawler被检测到(接着被block)的机会。某些网站会分析请求， 查找请求之间时间的相似性</p>
</li>
<li><p>REDIRECT_MAX_TIMES</p>
<p>  默认: 20</p>
<p>  定义request允许重定向的最大次数。超过该限制后该request直接返回获取到的结果。 对某些任务我们使用Firefox默认值</p>
</li>
<li><p>ROBOTSTXT_OBEY</p>
<p>  默认: False</p>
<p>  是否遵循robots协议</p>
</li>
<li><p>SCHEDULER<br>  默认: ‘scrapy.core.scheduler.Scheduler’</p>
<p>  用于爬取的调度器</p>
</li>
<li><p>SPIDER_MIDDLEWARES</p>
<p>  默认: {}</p>
<p>  保存项目中启用的下载中间件及其顺序的字典</p>
</li>
<li><p>USER_AGENT</p>
<p>  默认: “Scrapy/VERSION (+<a href="http://scrapy.org)&quot;" target="_blank" rel="noopener">http://scrapy.org)&quot;</a></p>
<p>  爬取的默认User-Agent，除非被覆盖</p>
</li>
</ul>
<h4 id="Scrapy默认BASE设置"><a href="#Scrapy默认BASE设置" class="headerlink" title="Scrapy默认BASE设置"></a>Scrapy默认BASE设置</h4><blockquote>
<p>scrapy对某些内部组件进行了默认设置，这些组件通常情况下是不能被修改的，但是我们在自定义了某些组件以后，比如我们设置了自定义的middleware中间件，需要按照一定的顺序把他添加到组件之中，这个时候需要参考scrapy的默认设置，因为这个顺序会影响scrapy的执行，下面列出了scrapy的默认基础设置</p>
</blockquote>
<p>注意：如果你想要修改以下的某些设置，应该避免直接修改下列内容，而是修改其对应的自定义内容，例如，你想修改下面的<code>DOWNLOADER_MIDDLEWARES_BASE</code>的内容，你应该去修改<code>DOWNLOADER_MIDDLEWARES</code>这个内容，只是去掉了_BASE而已，其他的也是类似这样</p>
<ul>
<li>DOWNLOADER_MIDDLEWARES_BASE</li>
</ul>
<p>默认:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#39;: 100,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#39;: 300,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#39;: 350,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;: 400,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;: 500,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#39;: 550,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#39;: 580,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;: 590,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#39;: 600,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#39;: 700,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&#39;: 750,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware&#39;: 830,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.stats.DownloaderStats&#39;: 850,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&#39;: 900,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>包含Scrapy默认启用的下载中间件的字典。 永远不要在项目中修改该设定，而是修改 DOWNLOADER_MIDDLEWARES 。</p>
<ul>
<li>SPIDER_MIDDLEWARES_BASE</li>
</ul>
<p>默认:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &#39;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&#39;: 50,</span><br><span class="line">    &#39;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&#39;: 500,</span><br><span class="line">    &#39;scrapy.spidermiddlewares.referer.RefererMiddleware&#39;: 700,</span><br><span class="line">    &#39;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&#39;: 800,</span><br><span class="line">    &#39;scrapy.spidermiddlewares.depth.DepthMiddleware&#39;: 900,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>保存项目中默认启用的spider中间件的字典。 永远不要在项目中修改该设定，而是修改 SPIDER_MIDDLEWARES 。<br>EXTENSIONS_BASE</p>
<p>默认:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &#39;scrapy.extensions.corestats.CoreStats&#39;: 0,</span><br><span class="line">    &#39;scrapy.telnet.TelnetConsole&#39;: 0,</span><br><span class="line">    &#39;scrapy.extensions.memusage.MemoryUsage&#39;: 0,</span><br><span class="line">    &#39;scrapy.extensions.memdebug.MemoryDebugger&#39;: 0,</span><br><span class="line">    &#39;scrapy.extensions.closespider.CloseSpider&#39;: 0,</span><br><span class="line">    &#39;scrapy.extensions.feedexport.FeedExporter&#39;: 0,</span><br><span class="line">    &#39;scrapy.extensions.logstats.LogStats&#39;: 0,</span><br><span class="line">    &#39;scrapy.extensions.spiderstate.SpiderState&#39;: 0,</span><br><span class="line">    &#39;scrapy.extensions.throttle.AutoThrottle&#39;: 0,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可用的插件列表。需要注意，有些插件需要通过设定来启用。默认情况下， 该设定包含所有稳定(stable)的内置插件。</p>
<ul>
<li>DOWNLOAD_HANDLERS_BASE</li>
</ul>
<p>默认:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &#39;file&#39;: &#39;scrapy.core.downloader.handlers.file.FileDownloadHandler&#39;,</span><br><span class="line">    &#39;http&#39;: &#39;scrapy.core.downloader.handlers.http.HttpDownloadHandler&#39;,</span><br><span class="line">    &#39;https&#39;: &#39;scrapy.core.downloader.handlers.http.HttpDownloadHandler&#39;,</span><br><span class="line">    &#39;s3&#39;: &#39;scrapy.core.downloader.handlers.s3.S3DownloadHandler&#39;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>保存项目中默认启用的下载处理器(request downloader handler)的字典。 永远不要在项目中修改该设定，而是修改 DOWNLOADER_HANDLERS 。</p>
<p>如果需要关闭上面的下载处理器，您必须在项目中的 DOWNLOAD_HANDLERS 设定中设置该处理器，并为其赋值为 None 。</p>
<p><strong>说明</strong></p>
<p>即使我们添加了一些我们自定义的组件，scrapy默认的base设置依然会被应用，这样说可能会一头雾水，简单地例子：</p>
<p>假如我们在middlewares.py文件中定义了一个中间件，名称为MyMiddleware，我们把它添加到settings.py文件里面的<code>DOWNLOADER_MIDDLEWARES</code>，且他的执行顺序我们设置为450，最终的设置内容就是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES &#x3D; &#123;</span><br><span class="line">    &#39;cnblog.middlewares.MyMiddleware&#39;:450,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们再来看一下默认的<code>DOWNLOADER_MIDDLEWARES_BASE</code>的内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES_BASE &#x3D;&#123;</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#39;: 100,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#39;: 300,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#39;: 350,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;: 400,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;: 500,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#39;: 550,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#39;: 580,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;: 590,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#39;: 600,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#39;: 700,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&#39;: 750,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware&#39;: 830,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.stats.DownloaderStats&#39;: 850,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&#39;: 900,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个时候，scrapy下载中间件的最终的执行顺序就是，把<code>DOWNLOADER_MIDDLEWARES</code>和<code>DOWNLOADER_MIDDLEWARES_BASE</code>里面的中间件按照顺序执行，<code>100&gt;300&gt;350&gt;400&gt;450&gt;500&gt;550&gt;580&gt;590&gt;600&gt;700&gt;750&gt;830&gt;850&gt;900</code>且全部执行，并不会因为我们定义了一个中间件，而使默认的中间件失效，也就是说，最终的结果其实是合并执行。</p>
<p>如果我们不想应用某一个默认的中间件，假如<code>&#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;: 500,</code>那么，就应该在<code>DOWNLOADER_MIDDLEWARES</code>里面把它的值设置为None，像下面这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES &#x3D; &#123;</span><br><span class="line">    &#39;cnblog.middlewares.MyMiddleware&#39;:450,</span><br><span class="line">    &#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;:None，</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>20. Scrapy 框架 - Pipeline</title>
    <url>/2020/01/20/python%E7%88%AC%E8%99%AB/20.%20Scrapy%20%E6%A1%86%E6%9E%B6%20-%20Pipeline/</url>
    <content><![CDATA[<h3 id="1-Item-Pipeline-介绍"><a href="#1-Item-Pipeline-介绍" class="headerlink" title="1. Item Pipeline 介绍"></a>1. Item Pipeline 介绍</h3><p>当Item 在Spider中被收集之后，就会被传递到Item Pipeline中进行处理</p>
<p>每个item pipeline组件是实现了简单的方法的python类，负责接收到item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline,或者被丢弃而不再进行处理</p>
<p>item pipeline的主要作用：</p>
<ol>
<li>清理html数据</li>
<li>验证爬取的数据</li>
<li>去重并丢弃</li>
<li>讲爬取的结果保存到数据库中或文件中</li>
</ol>
<h3 id="2-编写自己的item-pipeline"><a href="#2-编写自己的item-pipeline" class="headerlink" title="2. 编写自己的item pipeline"></a>2. 编写自己的item pipeline</h3><h4 id="2-1-必须实现的函数"><a href="#2-1-必须实现的函数" class="headerlink" title="2.1 必须实现的函数"></a>2.1 必须实现的函数</h4><ul>
<li>process_item(self,item,spider)</li>
</ul>
<p>每个item piple组件是一个独立的pyhton类，必须实现以process_item(self,item,spider)方法</p>
<p>每个item pipeline组件都需要调用该方法，这个方法必须返回一个具有数据的dict,或者item对象，或者抛出DropItem异常，被丢弃的item将不会被之后的pipeline组件所处理</p>
<h4 id="2-2-可以选择实现"><a href="#2-2-可以选择实现" class="headerlink" title="2.2 可以选择实现"></a>2.2 可以选择实现</h4><ul>
<li><p>open_spider(self,spider)<br>表示当spider被开启的时候调用这个方法</p>
</li>
<li><p>close_spider(self,spider)<br>当spider关闭时候这个方法被调用</p>
</li>
</ul>
<h4 id="2-3-应用到项目"><a href="#2-3-应用到项目" class="headerlink" title="2.3 应用到项目"></a>2.3 应用到项目</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">class MoviePipeline(object):</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        json.dump(dict(item), open(&#39;diban.json&#39;, &#39;a&#39;, encoding&#x3D;&#39;utf-8&#39;), ensure_ascii&#x3D;False)</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>

<h4 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h4><p>写到pipeline后，要在settings中设置才可生效</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ITEM_PIPELINES &#x3D; &#123;</span><br><span class="line">    &#39;spiderdemo1.pipelines.MoviePipeline&#39;: 300</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-4-将项目写入MongoDB"><a href="#2-4-将项目写入MongoDB" class="headerlink" title="2.4 将项目写入MongoDB"></a>2.4 将项目写入MongoDB</h4><p>MongoDB地址和数据库名称在Scrapy设置中指定; MongoDB集合以item类命名</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pymongo import MongoClient</span><br><span class="line">from middle.settings import HOST</span><br><span class="line">from middle.settings import PORT</span><br><span class="line">from middle.settings import DB_NAME</span><br><span class="line">from middle.settings import SHEET_NAME</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MiddlePipeline(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        client &#x3D; MongoClient(host&#x3D;HOST, port&#x3D;PORT)</span><br><span class="line">        my_db &#x3D; client[DB_NAME]</span><br><span class="line">        self.sheet &#x3D; my_db[SHEET_NAME]</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        self.sheet.insert(dict(item))</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>19. Scrapy 数据的保存</title>
    <url>/2020/01/19/python%E7%88%AC%E8%99%AB/19.%20Scrapy%20%E6%95%B0%E6%8D%AE%E7%9A%84%E4%BF%9D%E5%AD%98/</url>
    <content><![CDATA[<h3 id="1-数据的提取"><a href="#1-数据的提取" class="headerlink" title="1. 数据的提取"></a>1. 数据的提取</h3><h4 id="1-1-控制台打印"><a href="#1-1-控制台打印" class="headerlink" title="1.1 控制台打印"></a>1.1 控制台打印</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class DoubanSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &#39;douban&#39;</span><br><span class="line">    allwed_url &#x3D; &#39;douban.com&#39;</span><br><span class="line">    start_urls &#x3D; [</span><br><span class="line">        &#39;https:&#x2F;&#x2F;movie.douban.com&#x2F;top250&#x2F;&#39;</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        movie_name &#x3D; response.xpath(&quot;&#x2F;&#x2F;div[@class&#x3D;&#39;item&#39;]&#x2F;&#x2F;a&#x2F;span[1]&#x2F;text()&quot;).extract()</span><br><span class="line">        movie_core &#x3D; response.xpath(&quot;&#x2F;&#x2F;div[@class&#x3D;&#39;star&#39;]&#x2F;span[2]&#x2F;text()&quot;).extract()</span><br><span class="line">        yield &#123;</span><br><span class="line">            &#39;movie_name&#39;:movie_name,</span><br><span class="line">            &#39;movie_core&#39;:movie_core</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>执行以上代码，我可以在控制看到：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">2018-01-24 15:17:14 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: spiderdemo1)</span><br><span class="line">2018-01-24 15:17:14 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.3.1, w3lib 1.18.0, Twiste</span><br><span class="line">d 17.9.0, Python 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 201</span><br><span class="line">7), cryptography 2.1.4, Platform Windows-10-10.0.10240-SP0</span><br><span class="line">2018-01-24 15:17:14 [scrapy.crawler] INFO: Overridden settings: &#123;&#39;BOT_NAME&#39;: &#39;spiderdemo1&#39;, &#39;NEWSPIDER_MODULE&#39;: &#39;spiderdemo1.spiders&#39;,</span><br><span class="line">&#39;ROBOTSTXT_OBEY&#39;: True, &#39;SPIDER_MODULES&#39;: [&#39;spiderdemo1.spiders&#39;]&#125;</span><br><span class="line">2018-01-24 15:17:14 [scrapy.middleware] INFO: Enabled extensions:</span><br><span class="line">[&#39;scrapy.extensions.corestats.CoreStats&#39;,</span><br><span class="line"> &#39;scrapy.extensions.telnet.TelnetConsole&#39;,</span><br><span class="line"> &#39;scrapy.extensions.logstats.LogStats&#39;]</span><br><span class="line">2018-01-24 15:17:14 [scrapy.middleware] INFO: Enabled downloader middlewares:</span><br><span class="line">[&#39;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.downloadermiddlewares.stats.DownloaderStats&#39;]</span><br><span class="line">2018-01-24 15:17:14 [scrapy.middleware] INFO: Enabled spider middlewares:</span><br><span class="line">[&#39;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.spidermiddlewares.referer.RefererMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&#39;,</span><br><span class="line"> &#39;scrapy.spidermiddlewares.depth.DepthMiddleware&#39;]</span><br><span class="line">2018-01-24 15:17:14 [scrapy.middleware] INFO: Enabled item pipelines:</span><br><span class="line">[]</span><br><span class="line">2018-01-24 15:17:14 [scrapy.core.engine] INFO: Spider opened</span><br><span class="line">2018-01-24 15:17:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages&#x2F;min), scraped 0 items (at 0 items&#x2F;min)</span><br><span class="line">2018-01-24 15:17:14 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023</span><br><span class="line">2018-01-24 15:17:14 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https:&#x2F;&#x2F;movie.douban.com&#x2F;robots.txt&gt; (referer: None)</span><br><span class="line">2018-01-24 15:17:15 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to &lt;GET https:&#x2F;&#x2F;movie.douban.com&#x2F;top250&gt; from &lt;GET</span><br><span class="line"> https:&#x2F;&#x2F;movie.douban.com&#x2F;top250&#x2F;&gt;</span><br><span class="line">2018-01-24 15:17:15 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https:&#x2F;&#x2F;movie.douban.com&#x2F;top250&gt; (referer: None)</span><br><span class="line">2018-01-24 15:17:15 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https:&#x2F;&#x2F;movie.douban.com&#x2F;top250&gt;</span><br><span class="line">&#123;&#39;movie_name&#39;: [&#39;肖申克的救赎&#39;, &#39;霸王别姬&#39;, &#39;这个杀手不太冷&#39;, &#39;阿甘正传&#39;, &#39;美丽人生&#39;, &#39;千与千寻&#39;, &#39;泰坦尼克号&#39;, &#39;辛德勒的名单&#39;, &#39;盗梦空</span><br><span class="line">间&#39;, &#39;机器人总动员&#39;, &#39;海上钢琴师&#39;, &#39;三傻大闹宝莱坞&#39;, &#39;忠犬八公的故事&#39;, &#39;放牛班的春天&#39;, &#39;大话西游之大圣娶亲&#39;, &#39;教父&#39;, &#39;龙猫&#39;, &#39;楚门的世</span><br><span class="line">界&#39;, &#39;乱世佳人&#39;, &#39;熔炉&#39;, &#39;触不可及&#39;, &#39;天堂电影院&#39;, &#39;当幸福来敲门&#39;, &#39;无间道&#39;, &#39;星际穿越&#39;], &#39;movie_core&#39;: [&#39;9.6&#39;, &#39;9.5&#39;, &#39;9.4&#39;, &#39;9.4&#39;, &#39;9</span><br><span class="line">.5&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.4&#39;, &#39;9.3&#39;, &#39;9.3&#39;, &#39;9.2&#39;, &#39;9.1&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.1&#39;, &#39;9.1&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.1&#39;, &#39;9.1&#39;, &#39;8.9&#39;, &#39;9.0</span><br><span class="line">&#39;, &#39;9.1&#39;]&#125;</span><br><span class="line">2018-01-24 15:17:15 [scrapy.core.engine] INFO: Closing spider (finished)</span><br><span class="line">2018-01-24 15:17:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;&#39;downloader&#x2F;request_bytes&#39;: 651,</span><br><span class="line"> &#39;downloader&#x2F;request_count&#39;: 3,</span><br><span class="line"> &#39;downloader&#x2F;request_method_count&#x2F;GET&#39;: 3,</span><br><span class="line"> &#39;downloader&#x2F;response_bytes&#39;: 13900,</span><br><span class="line"> &#39;downloader&#x2F;response_count&#39;: 3,</span><br><span class="line"> &#39;downloader&#x2F;response_status_count&#x2F;200&#39;: 2,</span><br><span class="line"> &#39;downloader&#x2F;response_status_count&#x2F;301&#39;: 1,</span><br><span class="line"> &#39;finish_reason&#39;: &#39;finished&#39;,</span><br><span class="line"> &#39;finish_time&#39;: datetime.datetime(2018, 1, 24, 7, 17, 15, 247183),</span><br><span class="line"> &#39;item_scraped_count&#39;: 1,</span><br><span class="line"> &#39;log_count&#x2F;DEBUG&#39;: 5,</span><br><span class="line"> &#39;log_count&#x2F;INFO&#39;: 7,</span><br><span class="line"> &#39;response_received_count&#39;: 2,</span><br><span class="line"> &#39;scheduler&#x2F;dequeued&#39;: 2,</span><br><span class="line"> &#39;scheduler&#x2F;dequeued&#x2F;memory&#39;: 2,</span><br><span class="line"> &#39;scheduler&#x2F;enqueued&#39;: 2,</span><br><span class="line"> &#39;scheduler&#x2F;enqueued&#x2F;memory&#39;: 2,</span><br><span class="line"> &#39;start_time&#39;: datetime.datetime(2018, 1, 24, 7, 17, 14, 784782)&#125;</span><br><span class="line">2018-01-24 15:17:15 [scrapy.core.engine] INFO: Spider closed (finished)</span><br></pre></td></tr></table></figure>

<h4 id="1-2-以文件的方式输出"><a href="#1-2-以文件的方式输出" class="headerlink" title="1.2 以文件的方式输出"></a>1.2 以文件的方式输出</h4><h5 id="1-2-1-python原生方式"><a href="#1-2-1-python原生方式" class="headerlink" title="1.2.1 python原生方式"></a>1.2.1 python原生方式</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with open(&quot;movie.txt&quot;, &#39;wb&#39;) as f:</span><br><span class="line">    for n, c in zip(movie_name, movie_core):</span><br><span class="line">        str &#x3D; n+&quot;:&quot;+c+&quot;\n&quot;</span><br><span class="line">        f.write(str.encode())</span><br></pre></td></tr></table></figure>

<h5 id="1-2-2-以scrapy内置方式"><a href="#1-2-2-以scrapy内置方式" class="headerlink" title="1.2.2 以scrapy内置方式"></a>1.2.2 以scrapy内置方式</h5><p>scrapy 内置主要有四种：JSON，JSON lines，CSV，XML</p>
<p>我们将结果用最常用的JSON导出，命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy crawl dmoz -o douban.json -t json</span><br></pre></td></tr></table></figure>
<p>-o 后面是导出文件名，-t 后面是导出类型</p>
<h4 id="2-提取内容的封装Item"><a href="#2-提取内容的封装Item" class="headerlink" title="2 提取内容的封装Item"></a>2 提取内容的封装Item</h4><blockquote>
<p>Scrapy进程可通过使用蜘蛛提取来自网页中的数据。Scrapy使用Item类生成输出对象用于收刮数据</p>
</blockquote>
<blockquote>
<p>Item 对象是自定义的python字典，可以使用标准字典语法获取某个属性的值</p>
</blockquote>
<h5 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class InfoItem(scrapy.Item):</span><br><span class="line">    # define the fields for your item here like:</span><br><span class="line">    movie_name &#x3D; scrapy.Field()</span><br><span class="line">    movie_core &#x3D; scrapy.Field()</span><br></pre></td></tr></table></figure>

<h5 id="2-2-使用"><a href="#2-2-使用" class="headerlink" title="2.2 使用"></a>2.2 使用</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def parse(self, response):</span><br><span class="line">    movie_name &#x3D; response.xpath(&quot;&#x2F;&#x2F;div[@class&#x3D;&#39;item&#39;]&#x2F;&#x2F;a&#x2F;span[1]&#x2F;text()&quot;).extract()</span><br><span class="line">    movie_core &#x3D; response.xpath(&quot;&#x2F;&#x2F;div[@class&#x3D;&#39;star&#39;]&#x2F;span[2]&#x2F;text()&quot;).extract()</span><br><span class="line">    </span><br><span class="line">    for n, c in zip(movie_name, movie_core):</span><br><span class="line">        movie &#x3D; InfoItem()</span><br><span class="line">        movie[&#39;movie_name&#39;] &#x3D; n</span><br><span class="line">        movie[&#39;movie_core&#39;] &#x3D; c</span><br><span class="line">        yield movie</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>18. Scrapy 数据的提取</title>
    <url>/2020/01/18/python%E7%88%AC%E8%99%AB/18.%20Scrapy%20%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8F%90%E5%8F%96/</url>
    <content><![CDATA[<h3 id="1-Scrapy提取项目"><a href="#1-Scrapy提取项目" class="headerlink" title="1 Scrapy提取项目"></a>1 Scrapy提取项目</h3><p>从网页中提取数据，Scrapy 使用基于 XPath 和 CSS 表达式的技术叫做选择器。以下是 XPath 表达式的一些例子：</p>
<ul>
<li>这将选择 HTML 文档中的 <code>&lt;head&gt;</code> 元素中的 <code>&lt;title&gt;</code> 元素<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;html&#x2F;head&#x2F;title</span><br></pre></td></tr></table></figure></li>
<li>这将选择 <code>&lt;title&gt;</code> 元素中的文本<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;html&#x2F;head&#x2F;title&#x2F;text()</span><br></pre></td></tr></table></figure></li>
<li>这将选择所有的 <code>&lt;td&gt;</code> 元素<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;td</span><br></pre></td></tr></table></figure></li>
<li>选择 div 包含一个属性 class=”slice” 的所有元素<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;div[@class&#x3D;”slice”]</span><br></pre></td></tr></table></figure>


</li>
</ul>
<p>选择器有四个基本的方法，如下所示：<br>S.N.|方法 &amp; 描述<br>–|–<br>extract()|它返回一个unicode字符串以及所选数据<br>extract_first()|它返回第一个unicode字符串以及所选数据<br>re()|它返回Unicode字符串列表，当正则表达式被赋予作为参数时提取<br>xpath()|它返回选择器列表，它代表由指定XPath表达式参数选择的节点<br>css()|它返回选择器列表，它代表由指定CSS表达式作为参数所选择的节点</p>
<h3 id="2-Scrapy-Shell"><a href="#2-Scrapy-Shell" class="headerlink" title="2 Scrapy Shell"></a>2 Scrapy Shell</h3><p>如果使用选择器想快速的到到效果，我们可以使用Scrapy Shell</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy shell &quot;http:&#x2F;&#x2F;www.163.com&quot;</span><br></pre></td></tr></table></figure>
<p>注意windows系统必须使用双引号</p>
<h4 id="2-1-举例"><a href="#2-1-举例" class="headerlink" title="2.1 举例"></a>2.1 举例</h4><p>从一个普通的HTML网站提取数据，查看该网站得到的 XPath 的源代码。检测后，可以看到数据将在UL标签，并选择 li 标签中的 元素。</p>
<p>代码的下面行显示了不同类型的数据的提取：</p>
<ul>
<li>选择 li 标签内的数据：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">response.xpath(&#39;&#x2F;&#x2F;ul&#x2F;li&#39;)</span><br></pre></td></tr></table></figure></li>
<li>对于选择描述：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">response.xpath(&#39;&#x2F;&#x2F;ul&#x2F;li&#x2F;text()&#39;).extract()</span><br></pre></td></tr></table></figure></li>
<li>对于选择网站标题：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">response.xpath(&#39;&#x2F;&#x2F;ul&#x2F;li&#x2F;a&#x2F;text()&#39;).extract()</span><br></pre></td></tr></table></figure></li>
<li>对于选择网站的链接：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">response.xpath(&#39;&#x2F;&#x2F;ul&#x2F;li&#x2F;a&#x2F;@href&#39;).extract()</span><br></pre></td></tr></table></figure>
</li>
</ul>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>17. Scrapy 框架使用</title>
    <url>/2020/01/17/python%E7%88%AC%E8%99%AB/17.%20Scrapy%20%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h3 id="1-基本使用"><a href="#1-基本使用" class="headerlink" title="1 基本使用"></a>1 基本使用</h3><h4 id="1-1-创建项目"><a href="#1-1-创建项目" class="headerlink" title="1.1 创建项目"></a>1.1 创建项目</h4><p>运行命令:<br><code>scrapy startproject myfrist（your_project_name）</code></p>
<p><img src="https://note.youdao.com/yws/api/personal/file/2A2A0A18562A46C582E9394A5792242A?method=download&shareKey=75190e82788cd47c9c8792593be71114" alt="image"><br>文件说明：<br>名称 | 作用<br>–|–<br>scrapy.cfg | 项目的配置信息，主要为Scrapy命令行工具提供一个基础的配置信息。（真正爬虫相关的配置信息在settings.py文件中）<br>items.py | 设置数据存储模板，用于结构化数据，如：Django的Model<br>pipelines | 数据处理行为，如：一般结构化的数据持久化<br>settings.py | 配置文件，如：递归的层数、并发数，延迟下载等<br>spiders |  爬虫目录，如：创建文件，编写爬虫规则</p>
<p>注意：一般创建爬虫文件时，以网站域名命名</p>
<h4 id="2-编写-spdier"><a href="#2-编写-spdier" class="headerlink" title="2 编写 spdier"></a>2 编写 spdier</h4><p>在spiders目录中新建 daidu_spider.py 文件</p>
<h5 id="2-1-注意"><a href="#2-1-注意" class="headerlink" title="2.1 注意"></a>2.1 注意</h5><ol>
<li>爬虫文件需要定义一个类，并继承scrapy.spiders.Spider</li>
<li>必须定义name，即爬虫名，如果没有name，会报错。因为源码中是这样定义的</li>
</ol>
<h5 id="2-2-编写内容"><a href="#2-2-编写内容" class="headerlink" title="2.2 编写内容"></a>2.2 编写内容</h5><blockquote>
<p>在这里可以告诉 scrapy 。要如何查找确切数据，这里必须要定义一些属性</p>
</blockquote>
<ul>
<li>name: 它定义了蜘蛛的唯一名称</li>
<li>allowed_domains: 它包含了蜘蛛抓取的基本URL；</li>
<li>start-urls: 蜘蛛开始爬行的URL列表；</li>
<li>parse(): 这是提取并解析刮下数据的方法；</li>
</ul>
<p>下面的代码演示了蜘蛛代码的样子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class DoubanSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &#39;douban&#39;</span><br><span class="line">    allwed_url &#x3D; &#39;douban.com&#39;</span><br><span class="line">    start_urls &#x3D; [</span><br><span class="line">        &#39;https:&#x2F;&#x2F;movie.douban.com&#x2F;top250&#x2F;&#39;</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        movie_name &#x3D; response.xpath(&quot;&#x2F;&#x2F;div[@class&#x3D;&#39;item&#39;]&#x2F;&#x2F;a&#x2F;span[1]&#x2F;text()&quot;).extract()</span><br><span class="line">        movie_core &#x3D; response.xpath(&quot;&#x2F;&#x2F;div[@class&#x3D;&#39;star&#39;]&#x2F;span[2]&#x2F;text()&quot;).extract()</span><br><span class="line">        yield &#123;</span><br><span class="line">            &#39;movie_name&#39;:movie_name,</span><br><span class="line">            &#39;movie_core&#39;:movie_core</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<h3 id="其他命令："><a href="#其他命令：" class="headerlink" title="其他命令："></a>其他命令：</h3><ul>
<li>创建爬虫  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy genspider 爬虫名 爬虫的地址</span><br></pre></td></tr></table></figure></li>
<li>运行爬虫  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrapy crawl 爬虫名</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>16. Scrapy 框架介绍与安装</title>
    <url>/2020/01/16/python%E7%88%AC%E8%99%AB/16.%20Scrapy%20%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p><img src="https://note.youdao.com/yws/api/personal/file/3B46CE1A83254E4ABC7CCBC6DA7F8838?method=download&shareKey=2bba5f9fd137f02bc237bfca800e603a" alt="image"></p>
<h3 id="1-Scrapy-框架介绍"><a href="#1-Scrapy-框架介绍" class="headerlink" title="1. Scrapy 框架介绍"></a>1. Scrapy 框架介绍</h3><ul>
<li><p>Scrapy是Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy = Scrach+Python</p>
</li>
<li><p>Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试、信息处理和历史档案等大量应用范围内抽取结构化数据的应用程序框架，广泛用于工业</p>
</li>
<li><p>Scrapy 使用Twisted 这个异步网络库来处理网络通讯，架构清晰，并且包含了各种中间件接口，可以灵活的完成各种需求。Scrapy是由Twisted写的一个受欢迎的Python事件驱动网络框架，它使用的是非堵塞的异步处理</p>
<h4 id="1-1-为什么要使用Scrapy？"><a href="#1-1-为什么要使用Scrapy？" class="headerlink" title="1.1 为什么要使用Scrapy？"></a>1.1 为什么要使用Scrapy？</h4></li>
<li><p>它更容易构建和大规模的抓取项目</p>
</li>
<li><p>它内置的机制被称为选择器，用于从网站（网页）上提取数据</p>
</li>
<li><p>它异步处理请求，速度十分快</p>
</li>
<li><p>它可以使用自动调节机制自动调整爬行速度</p>
</li>
<li><p>确保开发人员可访问性</p>
<h4 id="1-2-Scrapy的特点"><a href="#1-2-Scrapy的特点" class="headerlink" title="1.2 Scrapy的特点"></a>1.2 Scrapy的特点</h4></li>
<li><p>Scrapy是一个开源和免费使用的网络爬虫框架</p>
</li>
<li><p>Scrapy生成格式导出如：JSON，CSV和XML</p>
</li>
<li><p>Scrapy内置支持从源代码，使用XPath或CSS表达式的选择器来提取数据</p>
</li>
<li><p>Scrapy基于爬虫，允许以自动方式从网页中提取数据</p>
</li>
</ul>
<h4 id="1-3-Scrapy的优点"><a href="#1-3-Scrapy的优点" class="headerlink" title="1.3 Scrapy的优点"></a>1.3 Scrapy的优点</h4><ul>
<li>Scrapy很容易扩展，快速和功能强大；</li>
<li>这是一个跨平台应用程序框架（在Windows，Linux，Mac OS和BSD）。</li>
<li>Scrapy请求调度和异步处理；</li>
<li>Scrapy附带了一个名为Scrapyd的内置服务，它允许使用JSON Web服务上传项目和控制蜘蛛。</li>
<li>也能够刮削任何网站，即使该网站不具有原始数据访问API；<h4 id="1-4-整体架构大致如下"><a href="#1-4-整体架构大致如下" class="headerlink" title="1.4 整体架构大致如下:"></a>1.4 整体架构大致如下:</h4><img src="https://images2015.cnblogs.com/blog/918906/201608/918906-20160830220006980-1873919293.png" alt="image"></li>
</ul>
<blockquote>
<p>最简单的单个网页爬取流程是spiders &gt; scheduler &gt; downloader &gt; spiders &gt; item pipeline</p>
</blockquote>
<h4 id="1-5-Scrapy运行流程大概如下："><a href="#1-5-Scrapy运行流程大概如下：" class="headerlink" title="1.5 Scrapy运行流程大概如下："></a>1.5 Scrapy运行流程大概如下：</h4><ol>
<li>引擎从调度器中取出一个链接(URL)用于接下来的抓取</li>
<li>引擎把URL封装成一个请求(Request)传给下载器</li>
<li>下载器把资源下载下来，并封装成应答包(Response)</li>
<li>爬虫解析Response</li>
<li>解析出实体（Item）,则交给实体管道进行进一步的处理</li>
<li>解析出的是链接（URL）,则把URL交给调度器等待抓取</li>
</ol>
<h4 id="1-6-Scrapy主要包括了以下组件："><a href="#1-6-Scrapy主要包括了以下组件：" class="headerlink" title="1.6 Scrapy主要包括了以下组件："></a>1.6 Scrapy主要包括了以下组件：</h4><ul>
<li>引擎(Scrapy)<ul>
<li>用来处理整个系统的数据流处理, 触发事务(框架核心)</li>
</ul>
</li>
<li>调度器(Scheduler)<ul>
<li>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</li>
</ul>
</li>
<li>下载器(Downloader)<ul>
<li>用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
</ul>
</li>
<li>爬虫(Spiders)<ul>
<li>爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</li>
</ul>
</li>
<li>项目管道(Pipeline)<ul>
<li>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
</ul>
</li>
<li>下载器中间件(Downloader Middlewares)<ul>
<li>位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应</li>
</ul>
</li>
<li>爬虫中间件(Spider Middlewares)<ul>
<li>介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出</li>
</ul>
</li>
<li>调度中间件(Scheduler Middewares)<ul>
<li>介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应<h3 id="2-安装"><a href="#2-安装" class="headerlink" title="2 安装"></a>2 安装</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install Scrapy</span><br></pre></td></tr></table></figure>
注：windows平台需要依赖pywin32<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ModuleNotFoundError: No module named &#39;win32api&#39;</span><br></pre></td></tr></table></figure>
<code>pip install pypiwin32</code></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>15. Python下Tesseract Ocr引擎及安装介绍</title>
    <url>/2020/01/15/python%E7%88%AC%E8%99%AB/15.%20Python%E4%B8%8BTesseract%20Ocr%E5%BC%95%E6%93%8E%E5%8F%8A%E5%AE%89%E8%A3%85%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h3 id="1-Tesseract介绍"><a href="#1-Tesseract介绍" class="headerlink" title="1. Tesseract介绍"></a>1. Tesseract介绍</h3><p>tesseract 是一个google支持的开源ocr项目</p>
<blockquote>
<p>其项目地址：<a href="https://github.com/tesseract-ocr/tesseract">https://github.com/tesseract-ocr/tesseract</a></p>
</blockquote>
<p>目前最新的源码可以在这里下载</p>
<h3 id="2-Tesseract安装包下载"><a href="#2-Tesseract安装包下载" class="headerlink" title="2. Tesseract安装包下载"></a>2. Tesseract安装包下载</h3><p>Tesseract的release版本下载地址：<a href="https://github.com/tesseract-ocr/tesseract/wiki/Downloads，这里需要注意这一段话：">https://github.com/tesseract-ocr/tesseract/wiki/Downloads，这里需要注意这一段话：</a></p>
<blockquote>
<p>Currently, there is no official Windows installer for newer versions</p>
</blockquote>
<p>意思就是官方不提供最新版windows平台安装包，只有相对略老的3.02.02版本，其下载地址：<a href="https://sourceforge.net/projects/tesseract-ocr-alt/files/" target="_blank" rel="noopener">https://sourceforge.net/projects/tesseract-ocr-alt/files/</a></p>
<p>最新版3.03和3.05版本，都是三方维护和管理的安装包，有好几个发行机构，分别是：</p>
<ul>
<li><a href="https://www.dropbox.com/s/8t54mz39i58qslh/tesseract-3.05.00dev-win32-vc19.zip?dl=1" target="_blank" rel="noopener">https://www.dropbox.com/s/8t54mz39i58qslh/tesseract-3.05.00dev-win32-vc19.zip?dl=1</a></li>
<li><a href="https://github.com/UB-Mannheim/tesseract/wiki">https://github.com/UB-Mannheim/tesseract/wiki</a></li>
<li><a href="http://domasofan.spdns.eu/tesseract/" target="_blank" rel="noopener">http://domasofan.spdns.eu/tesseract/</a></li>
</ul>
<h3 id="3-小结"><a href="#3-小结" class="headerlink" title="3. 小结"></a>3. 小结</h3><ol>
<li><p>官方发布的3.02版本下载地址</p>
<blockquote>
<p><a href="http://downloads.sourceforge.net/project/tesseract-ocr-alt/tesseract-ocr-setup-3.02.02.exe?r=https%3A%2F%2Fsourceforge.net%2Fprojects%2Ftesseract-ocr-alt%2Ffiles%2F&amp;ts=1464880498&amp;use_mirror=jaist" target="_blank" rel="noopener">http://downloads.sourceforge.net/project/tesseract-ocr-alt/tesseract-ocr-setup-3.02.02.exe?r=https%3A%2F%2Fsourceforge.net%2Fprojects%2Ftesseract-ocr-alt%2Ffiles%2F&amp;ts=1464880498&amp;use_mirror=jaist</a></p>
</blockquote>
</li>
<li><p>德国曼海姆大学发行的3.05版本下载地址</p>
<blockquote>
<p><a href="http://digi.bib.uni-mannheim.de/tesseract/tesseract-ocr-setup-3.05.00dev.exe" target="_blank" rel="noopener">http://digi.bib.uni-mannheim.de/tesseract/tesseract-ocr-setup-3.05.00dev.exe</a></p>
</blockquote>
</li>
<li><p>imon Eigeldinger (@DomasoFan) 维护的另一个版本</p>
<blockquote>
<p><a href="http://3.onj.me/tesseract/" target="_blank" rel="noopener">http://3.onj.me/tesseract/</a><br>值得称道的是，这个网址里还有一个比较详细的说明</p>
</blockquote>
</li>
</ol>
<h3 id="4-Tesseract-ocr使用"><a href="#4-Tesseract-ocr使用" class="headerlink" title="4. Tesseract ocr使用"></a>4. Tesseract ocr使用</h3><p>安装之后，默认目录C:\Program Files (x86)\Tesseract-OCR，你需要把这个路径放到你操作系统的path搜索路径中，否则后面使用起来会不方便。</p>
<p>在安装目录C:\Program Files (x86)\Tesseract-OCR下可以看到 tesseract.exe这个命令行执行程序</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tesseract 1.png output-l eng -psm 7</span><br></pre></td></tr></table></figure>
<p>-psm 7 表示用单行文本识别<br>pagesegmode值：</p>
<ul>
<li><p>0 =定向和脚本检测（OSD）。</p>
</li>
<li><p>1 =带OSD的自动页面分割。</p>
</li>
<li><p>2 =自动页面分割，但没有OSD或OCR</p>
</li>
<li><p>3 =全自动页面分割，但没有OSD。（默认）</p>
</li>
<li><p>4 =假设一列可变大小的文本。</p>
</li>
<li><p>5 =假设一个统一的垂直对齐文本块。</p>
</li>
<li><p>6 =假设一个统一的文本块。</p>
</li>
<li><p>7 =将图像作为单个文本行处理。</p>
</li>
<li><p>8 =把图像当作一个单词。</p>
</li>
<li><p>9 =把图像当作一个圆圈中的一个词来对待。</p>
</li>
<li><p>10 =将图像作为单个字符处理</p>
<p>#-l eng 代表使用英语识别</p>
</li>
</ul>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>14. Selenium 处理滚动条</title>
    <url>/2020/01/14/python%E7%88%AC%E8%99%AB/14.%20Selenium%20%E5%A4%84%E7%90%86%E6%BB%9A%E5%8A%A8%E6%9D%A1/</url>
    <content><![CDATA[<h4 id="Selenium-处理滚动条"><a href="#Selenium-处理滚动条" class="headerlink" title="Selenium 处理滚动条"></a>Selenium 处理滚动条</h4><blockquote>
<p>selenium并不是万能的，有时候页面上操作无法实现的，这时候就需要借助JS来完成了</p>
</blockquote>
<p>　　当页面上的元素超过一屏后，想操作屏幕下方的元素，是不能直接定位到，会报元素不可见的。这时候需要借助滚动条来拖动屏幕，使被操作的元素显示在当前的屏幕上。滚动条是无法直接用定位工具来定位的。selenium里面也没有直接的方法去控制滚动条，这时候只能借助J了，还好selenium提供了一个操作js的方法:execute_script()，可以直接执行js的脚本</p>
<h5 id="一-控制滚动条高度"><a href="#一-控制滚动条高度" class="headerlink" title="一. 控制滚动条高度"></a>一. 控制滚动条高度</h5><p>1 滚动条回到顶部：</p>
<pre><code>js=&quot;var q=document.getElementById(&apos;id&apos;).scrollTop=0&quot;
driver.execute_script(js)</code></pre><p>　　　　<br>2 滚动条拉到底部</p>
<pre><code>js=&quot;var q=document.documentElement.scrollTop=10000&quot;
driver.execute_script(js)</code></pre><p>可以修改scrollTop 的值，来定位右侧滚动条的位置，0是最上面，10000是最底部</p>
<p>以上方法在Firefox和IE浏览器上上是可以的，但是用Chrome浏览器，发现不管用。Chrome浏览器解决办法：</p>
<pre><code>js = &quot;var q=document.body.scrollTop=0&quot;
driver.execute_script(js)</code></pre><h5 id="二-横向滚动条"><a href="#二-横向滚动条" class="headerlink" title="二.横向滚动条"></a>二.横向滚动条</h5><p>1 有时候浏览器页面需要左右滚动（一般屏幕最大化后，左右滚动的情况已经很少见了)</p>
<p>2 通过左边控制横向和纵向滚动条scrollTo(x, y)</p>
<pre><code>js = &quot;window.scrollTo(100,400)&quot;
driver.execute_script(js)</code></pre><h5 id="三-元素聚焦"><a href="#三-元素聚焦" class="headerlink" title="三.元素聚焦"></a>三.元素聚焦</h5><p>虽然用上面的方法可以解决拖动滚动条的位置问题，但是有时候无法确定我需要操作的元素在什么位置，有可能每次打开的页面不一样，元素所在的位置也不一样，怎么办呢？这个时候我们可以先让页面直接跳到元素出现的位置，然后就可以操作了</p>
<p>同样需要借助JS去实现。 具体如下：</p>
<pre><code>target = driver.find_element_by_xxxx()
driver.execute_script(&quot;arguments[0].scrollIntoView();&quot;, target)</code></pre><h5 id="四-参考代码"><a href="#四-参考代码" class="headerlink" title="四. 参考代码"></a>四. 参考代码</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from lxml import etree</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">url &#x3D; &quot;https:&#x2F;&#x2F;search.jd.com&#x2F;Search?keyword&#x3D;%E7%AC%94%E8%AE%B0%E6%9C%AC&amp;enc&#x3D;utf-8&amp;wq&#x3D;%E7%AC%94%E8%AE%B0%E6%9C%AC&amp;pvid&#x3D;845d019c94f6476ca5c4ffc24df6865a&quot;</span><br><span class="line"># 加载浏览器</span><br><span class="line">wd &#x3D; webdriver.Firefox()</span><br><span class="line"># 发送请求</span><br><span class="line">wd.get(url)</span><br><span class="line"># 要执行的js</span><br><span class="line">js &#x3D; &quot;var q &#x3D; document.documentElement.scrollTop&#x3D;10000&quot;</span><br><span class="line"># 执行js</span><br><span class="line">wd.execute_script(js)</span><br><span class="line"></span><br><span class="line">time.sleep(3)</span><br><span class="line"># 解析数据</span><br><span class="line">e &#x3D; etree.HTML(wd.page_source)</span><br><span class="line"># 提取数据的xpath</span><br><span class="line">price_xpath &#x3D; &#39;&#x2F;&#x2F;ul[@class&#x3D;&quot;gl-warp clearfix&quot;]&#x2F;&#x2F;div[@class&#x3D;&quot;p-price&quot;]&#x2F;strong&#x2F;i&#x2F;text()&#39;</span><br><span class="line"># 提取数据的</span><br><span class="line">infos &#x3D; e.xpath(price_xpath)</span><br><span class="line"></span><br><span class="line">print(len(infos))</span><br><span class="line"># 关闭浏览器</span><br><span class="line">wd.quit()</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>13. Selenium与PhantomJS</title>
    <url>/2020/01/13/python%E7%88%AC%E8%99%AB/13.%20Selenium%E4%B8%8EPhantomJS/</url>
    <content><![CDATA[<h3 id="1-Selenium"><a href="#1-Selenium" class="headerlink" title="1. Selenium"></a>1. Selenium</h3><p>Selenium是一个Web的自动化测试工具，最初是为网站自动化测试而开发的，类型像我们玩游戏用的按键精灵，可以按指定的命令自动操作，不同是Selenium 可以直接运行在浏览器上，它支持所有主流的浏览器（包括PhantomJS这些无界面的浏览器）。</p>
<p>Selenium 可以根据我们的指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏，或者判断网站上某些动作是否发生。</p>
<p>Selenium 自己不带浏览器，不支持浏览器的功能，它需要与第三方浏览器结合在一起才能使用。但是我们有时候需要让它内嵌在代码中运行，所以我们可以用一个叫 PhantomJS 的工具代替真实的浏览器。</p>
<p>PyPI网站下载 Selenium库 <a href="https://pypi.python.org/simple/selenium" target="_blank" rel="noopener">https://pypi.python.org/simple/selenium</a> ，也可以用 第三方管理器 </p>
<p>pip用命令安装：<code>pip install selenium</code></p>
<p>Selenium 官方参考文档：<a href="http://selenium-python.readthedocs.io/index.html" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/index.html</a></p>
<h3 id="2-PhantomJS"><a href="#2-PhantomJS" class="headerlink" title="2. PhantomJS"></a>2. PhantomJS</h3><p>PhantomJS 是一个基于Webkit的“无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的 JavaScript，因为不会展示图形界面，所以运行起来比完整的浏览器要高效</p>
<p>如果我们把 Selenium 和 PhantomJS 结合在一起，就可以运行一个非常强大的网络爬虫了，这个爬虫可以处理 JavaScrip、Cookie、headers，以及任何我们真实用户需要做的事情</p>
<h4 id="2-1注意：PhantomJS（python2）"><a href="#2-1注意：PhantomJS（python2）" class="headerlink" title="2.1注意：PhantomJS（python2）"></a>2.1注意：PhantomJS（python2）</h4><p>只能从它的官方网站<a href="http://phantomjs.org/download.html" target="_blank" rel="noopener">http://phantomjs.org/download.html</a>) 下载。 因为 PhantomJS 是一个功能完善(虽然无界面)的浏览器而非一个 Python 库，所以它不需要像 Python 的其他库一样安装，但我们可以通过Selenium调用PhantomJS来直接使用。</p>
<p>PhantomJS 官方参考文档：<a href="http://phantomjs.org/documentation" target="_blank" rel="noopener">http://phantomjs.org/documentation</a></p>
<h4 id="2-2-python3使用的浏览器"><a href="#2-2-python3使用的浏览器" class="headerlink" title="2.2 python3使用的浏览器"></a>2.2 python3使用的浏览器</h4><p>随着Python3的普及，Selenium3也跟上了行程。而Selenium3最大的变化是去掉了Selenium RC，另外就是Webdriver从各自浏览器中脱离，必须单独下载</p>
<h5 id="2-1-1-安装Firefox-geckodriver"><a href="#2-1-1-安装Firefox-geckodriver" class="headerlink" title="2.1.1 安装Firefox geckodriver"></a>2.1.1 安装Firefox geckodriver</h5><p>安装firefox最新版本，添加Firefox可执行程序到系统环境变量。记得关闭firefox的自动更新</p>
<p>firefox下载地下：<a href="https://github.com/mozilla/geckodriver/releases">https://github.com/mozilla/geckodriver/releases</a></p>
<p>将下载的geckodriver.exe 放到path路径下 D:\Python\Python36\</p>
<h5 id="2-1-2-安装ChromeDriver"><a href="#2-1-2-安装ChromeDriver" class="headerlink" title="2.1.2 安装ChromeDriver"></a>2.1.2 安装ChromeDriver</h5><p><a href="https://chromedriver.chromium.org/downloads" target="_blank" rel="noopener">https://chromedriver.chromium.org/downloads</a></p>
<blockquote>
<p>注意版本号要对应</p>
</blockquote>
<blockquote>
<p>下载下来的文件解压到<code>Python36\Scripts</code></p>
</blockquote>
<blockquote>
<p>chrome59版本以后可以变成无头的浏览器，加以下参数</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">options &#x3D; webdriver.ChromeOptions()</span><br><span class="line">options.add_argument(&#39;--headless&#39;)</span><br><span class="line">chrome &#x3D; webdriver.Chrome(chrome_options&#x3D;options)</span><br><span class="line">chrome.get(&quot;http:&#x2F;&#x2F;ww.baidu.com&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="3-使用方式"><a href="#3-使用方式" class="headerlink" title="3. 使用方式"></a>3. 使用方式</h3><p>Selenium 库里有个叫 WebDriver 的 API。WebDriver 有点儿像可以加载网站的浏览器，但是它也可以像 BeautifulSoup 或者其他 Selector 对象一样用来查找页面元素，与页面上的元素进行交互 (发送文本、点击等)，以及执行其他动作来运行网络爬虫</p>
<h4 id="3-1-简单例子"><a href="#3-1-简单例子" class="headerlink" title="3.1 简单例子"></a>3.1 简单例子</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 导入 webdriver</span><br><span class="line">from selenium import webdriver</span><br><span class="line"></span><br><span class="line"># 要想调用键盘按键操作需要引入keys包</span><br><span class="line">from selenium.webdriver.common.keys import Keys</span><br><span class="line"></span><br><span class="line"># 调用环境变量指定的PhantomJS浏览器创建浏览器对象</span><br><span class="line">driver &#x3D; webdriver.PhantomJS()</span><br><span class="line"></span><br><span class="line"># 如果没有在环境变量指定PhantomJS位置</span><br><span class="line"># driver &#x3D; webdriver.PhantomJS(executable_path&#x3D;&quot;.&#x2F;phantomjs&quot;))</span><br><span class="line"></span><br><span class="line"># get方法会一直等到页面被完全加载，然后才会继续程序，通常测试会在这里选择 time.sleep(2)</span><br><span class="line">driver.get(&quot;http:&#x2F;&#x2F;www.baidu.com&#x2F;&quot;)</span><br><span class="line"></span><br><span class="line"># 获取页面名为 wrapper的id标签的文本内容</span><br><span class="line">data &#x3D; driver.find_element_by_id(&quot;wrapper&quot;).text</span><br><span class="line"></span><br><span class="line"># 打印数据内容</span><br><span class="line">print(data)</span><br><span class="line"></span><br><span class="line"># 打印页面标题 &quot;百度一下，你就知道&quot;</span><br><span class="line">print（driver.title）</span><br><span class="line"></span><br><span class="line"># 生成当前页面快照并保存</span><br><span class="line">driver.save_screenshot(&quot;baidu.png&quot;)</span><br><span class="line"></span><br><span class="line"># id&#x3D;&quot;kw&quot;是百度搜索输入框，输入字符串&quot;长城&quot;</span><br><span class="line">driver.find_element_by_id(&quot;kw&quot;).send_keys(&quot;尚学堂&quot;)</span><br><span class="line"></span><br><span class="line"># id&#x3D;&quot;su&quot;是百度搜索按钮，click() 是模拟点击</span><br><span class="line">driver.find_element_by_id(&quot;su&quot;).click()</span><br><span class="line"></span><br><span class="line"># 获取新的页面快照</span><br><span class="line">driver.save_screenshot(&quot;尚学.png&quot;)</span><br><span class="line"></span><br><span class="line"># 打印网页渲染后的源代码</span><br><span class="line">print(driver.page_source)</span><br><span class="line"></span><br><span class="line"># 获取当前页面Cookie</span><br><span class="line">print(driver.get_cookies())</span><br><span class="line"></span><br><span class="line"># ctrl+a 全选输入框内容</span><br><span class="line">driver.find_element_by_id(&quot;kw&quot;).send_keys(Keys.CONTROL,&#39;a&#39;)</span><br><span class="line"></span><br><span class="line"># ctrl+x 剪切输入框内容</span><br><span class="line">driver.find_element_by_id(&quot;kw&quot;).send_keys(Keys.CONTROL,&#39;x&#39;)</span><br><span class="line"></span><br><span class="line"># 输入框重新输入内容</span><br><span class="line">driver.find_element_by_id(&quot;kw&quot;).send_keys(&quot;python爬虫&quot;)</span><br><span class="line"></span><br><span class="line"># 模拟Enter回车键</span><br><span class="line">driver.find_element_by_id(&quot;su&quot;).send_keys(Keys.RETURN)</span><br><span class="line"></span><br><span class="line"># 清除输入框内容</span><br><span class="line">driver.find_element_by_id(&quot;kw&quot;).clear()</span><br><span class="line"></span><br><span class="line"># 生成新的页面快照</span><br><span class="line">driver.save_screenshot(&quot;python爬虫.png&quot;)</span><br><span class="line"></span><br><span class="line"># 获取当前url</span><br><span class="line">print(driver.current_url)</span><br><span class="line"></span><br><span class="line"># 关闭当前页面，如果只有一个页面，会关闭浏览器</span><br><span class="line"># driver.close()</span><br><span class="line"></span><br><span class="line"># 关闭浏览器</span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>

<h3 id="4-页面操作"><a href="#4-页面操作" class="headerlink" title="4 页面操作"></a>4 页面操作</h3><h4 id="4-1-页面交互"><a href="#4-1-页面交互" class="headerlink" title="4.1 页面交互"></a>4.1 页面交互</h4><blockquote>
<p>仅仅抓取页面没有多大卵用，我们真正要做的是做到和页面交互，比如点击，输入等等。那么前提就是要找到页面中的元素。WebDriver提供了各种方法来寻找元素。例如下面有一个表单输入框</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;passwd&quot; id&#x3D;&quot;passwd-id&quot; &#x2F;&gt;</span><br></pre></td></tr></table></figure>
<h5 id="4-1-1-获取"><a href="#4-1-1-获取" class="headerlink" title="4.1.1 获取"></a>4.1.1 <strong>获取</strong></h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">element &#x3D; driver.find_element_by_id(&quot;passwd-id&quot;)</span><br><span class="line">element &#x3D; driver.find_element_by_name(&quot;passwd&quot;)</span><br><span class="line">element &#x3D; driver.find_elements_by_tag_name(&quot;input&quot;)</span><br><span class="line">element &#x3D; driver.find_element_by_xpath(&quot;&#x2F;&#x2F;input[@id&#x3D;&#39;passwd-id&#39;]&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong></p>
<ul>
<li><p>文本必须完全匹配才可以，所以这并不是一个很好的匹配方式</p>
</li>
<li><p>在用 xpath 的时候还需要注意的如果有多个元素匹配了 xpath，它只会返回第一个匹配的元素。如果没有找到，那么会抛出 NoSuchElementException 的异常</p>
</li>
</ul>
<h5 id="4-1-2-输入内容"><a href="#4-1-2-输入内容" class="headerlink" title="4.1.2 输入内容"></a>4.1.2 输入内容</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">element.send_keys(&quot;some text&quot;)</span><br></pre></td></tr></table></figure>
<h5 id="4-1-3-模拟点击某个按键"><a href="#4-1-3-模拟点击某个按键" class="headerlink" title="4.1.3 模拟点击某个按键"></a>4.1.3 模拟点击某个按键</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">element.send_keys(&quot;and some&quot;, Keys.ARROW_DOWN)</span><br></pre></td></tr></table></figure>
<h5 id="4-1-4-清空文本"><a href="#4-1-4-清空文本" class="headerlink" title="4.1.4 清空文本"></a>4.1.4 清空文本</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">element.clear()</span><br></pre></td></tr></table></figure>

<h5 id="4-1-5-元素拖拽"><a href="#4-1-5-元素拖拽" class="headerlink" title="4.1.5 元素拖拽"></a>4.1.5 元素拖拽</h5><blockquote>
<p>要完成元素的拖拽，首先你需要指定被拖动的元素和拖动目标元素，然后利用 ActionChains 类来实现</p>
</blockquote>
<p>以下实现元素从 source 拖动到 target 的操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">element &#x3D; driver.find_element_by_name(&quot;source&quot;)</span><br><span class="line">target &#x3D; driver.find_element_by_name(&quot;target&quot;)</span><br><span class="line"> </span><br><span class="line">from selenium.webdriver import ActionChains</span><br><span class="line">action_chains &#x3D; ActionChains(driver)</span><br><span class="line">action_chains.drag_and_drop(element, target).perform()</span><br></pre></td></tr></table></figure>
<h5 id="4-1-6-历史记录"><a href="#4-1-6-历史记录" class="headerlink" title="4.1.6 历史记录"></a>4.1.6 历史记录</h5><blockquote>
<p>操作页面的前进和后退功能</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">driver.forward()</span><br><span class="line">driver.back()</span><br></pre></td></tr></table></figure>

<h3 id="5-API"><a href="#5-API" class="headerlink" title="5 API"></a>5 API</h3><h4 id="5-1-元素选取"><a href="#5-1-元素选取" class="headerlink" title="5.1 元素选取"></a>5.1 元素选取</h4><h5 id="5-1-1-单个元素选取"><a href="#5-1-1-单个元素选取" class="headerlink" title="5.1.1 单个元素选取"></a>5.1.1 单个元素选取</h5><ul>
<li>find_element_by_id</li>
<li>find_element_by_name</li>
<li>find_element_by_xpath</li>
<li>find_element_by_link_text</li>
<li>find_element_by_partial_link_text</li>
<li>find_element_by_tag_name</li>
<li>find_element_by_class_name</li>
<li>find_element_by_css_selector</li>
</ul>
<h5 id="5-1-2-多个元素选取"><a href="#5-1-2-多个元素选取" class="headerlink" title="5.1.2 多个元素选取"></a>5.1.2 多个元素选取</h5><ul>
<li>find_elements_by_name</li>
<li>find_elements_by_xpath</li>
<li>find_elements_by_link_text</li>
<li>find_elements_by_partial_link_text</li>
<li>find_elements_by_tag_name</li>
<li>find_elements_by_class_name</li>
<li>find_elements_by_css_selector</li>
</ul>
<h5 id="5-1-3-利用-By-类来确定哪种选择方式"><a href="#5-1-3-利用-By-类来确定哪种选择方式" class="headerlink" title="5.1.3 利用 By 类来确定哪种选择方式"></a>5.1.3 利用 By 类来确定哪种选择方式</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from selenium.webdriver.common.by import By</span><br><span class="line"> </span><br><span class="line">driver.find_element(By.XPATH, &#39;&#x2F;&#x2F;button[text()&#x3D;&quot;Some text&quot;]&#39;)</span><br><span class="line">driver.find_elements(By.XPATH, &#39;&#x2F;&#x2F;button&#39;)</span><br></pre></td></tr></table></figure>
<p>By 类的一些属性如下</p>
<ul>
<li>ID = “id”</li>
<li>XPATH = “xpath”</li>
<li>LINK_TEXT = “link text”</li>
<li>PARTIAL_LINK_TEXT = “partial link text”</li>
<li>NAME = “name”</li>
<li>TAG_NAME = “tag name”</li>
<li>CLASS_NAME = “class name”</li>
<li>CSS_SELECTOR = “css selector”</li>
</ul>
<h3 id="6-等待"><a href="#6-等待" class="headerlink" title="6 等待"></a>6 等待</h3><h4 id="6-1-隐式等待"><a href="#6-1-隐式等待" class="headerlink" title="6.1 隐式等待"></a>6.1 隐式等待</h4><blockquote>
<p>到了一定的时间发现元素还没有加载，则继续等待我们指定的时间，如果超过了我们指定的时间还没有加载就会抛出异常，如果没有需要等待的时候就已经加载完毕就会立即执行</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">url &#x3D; &#39;https:&#x2F;&#x2F;www.guazi.com&#x2F;nj&#x2F;buy&#x2F;&#39;</span><br><span class="line">driver &#x3D; webdriver.Chrome()</span><br><span class="line">driver.get(url)</span><br><span class="line">driver.implicitly_wait(100)</span><br><span class="line">print(driver.find_element_by_class_name(&#39;next&#39;))</span><br><span class="line">print(driver.page_source)</span><br></pre></td></tr></table></figure>

<h4 id="6-2-显示等待"><a href="#6-2-显示等待" class="headerlink" title="6.2 显示等待"></a>6.2 显示等待</h4><blockquote>
<p>指定一个等待条件，并且指定一个最长等待时间，会在这个时间内进行判断是否满足等待条件，如果成立就会立即返回，如果不成立，就会一直等待，直到等待你指定的最长等待时间，如果还是不满足，就会抛出异常，如果满足了就会正常返回</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">url &#x3D; &#39;https:&#x2F;&#x2F;www.guazi.com&#x2F;nj&#x2F;buy&#x2F;&#39;</span><br><span class="line">driver &#x3D; webdriver.Chrome()</span><br><span class="line">driver.get(url)</span><br><span class="line">wait &#x3D; WebDriverWait(driver,10)</span><br><span class="line">wait.until(EC.presence_of_element_located((By.CLASS_NAME, &#39;next&#39;)))</span><br><span class="line">print(driver.page_source)</span><br></pre></td></tr></table></figure>

<ul>
<li>presence_of_element_located   <ul>
<li>元素加载出，传入定位元组，如(By.ID, ‘p’)</li>
</ul>
</li>
<li>presence_of_all_elements_located <ul>
<li>所有元素加载出</li>
</ul>
</li>
<li>element_to_be_clickable<ul>
<li>元素可点击</li>
</ul>
</li>
<li>element_located_to_be_selected<ul>
<li>元素可选择，传入定位元组 <h4 id="6-3-强制等待"><a href="#6-3-强制等待" class="headerlink" title="6.3 强制等待"></a>6.3 强制等待</h4><blockquote>
<p>使用 time.sleep</p>
</blockquote>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>12. 爬虫之多线程</title>
    <url>/2020/01/12/python%E7%88%AC%E8%99%AB/12.%20%E7%88%AC%E8%99%AB%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B/</url>
    <content><![CDATA[<h3 id="1-引入"><a href="#1-引入" class="headerlink" title="1. 引入"></a>1. 引入</h3><blockquote>
<p>我们之前写的爬虫都是单个线程的？这怎么够？一旦一个地方卡到不动了，那不就永远等待下去了？为此我们可以使用多线程或者多进程来处理。</p>
</blockquote>
<blockquote>
<p>不建议你用这个，不过还是介绍下了，如果想看可以看看下面，不想浪费时间直接看</p>
</blockquote>
<h3 id="2-如何使用"><a href="#2-如何使用" class="headerlink" title="2. 如何使用"></a>2. 如何使用</h3><blockquote>
<p>爬虫使用多线程来处理网络请求，使用线程来处理URL队列中的url，然后将url返回的结果保存在另一个队列中，其它线程在读取这个队列中的数据，然后写到文件中去</p>
</blockquote>
<h3 id="3-主要组成部分"><a href="#3-主要组成部分" class="headerlink" title="3. 主要组成部分"></a>3. 主要组成部分</h3><h4 id="3-1-URL队列和结果队列"><a href="#3-1-URL队列和结果队列" class="headerlink" title="3.1 URL队列和结果队列"></a>3.1 URL队列和结果队列</h4><p>将将要爬去的url放在一个队列中，这里使用标准库Queue。访问url后的结果保存在结果队列中</p>
<p>初始化一个URL队列</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from queue import Queue</span><br><span class="line">urls_queue &#x3D; Queue()</span><br><span class="line">out_queue &#x3D; Queue()</span><br></pre></td></tr></table></figure>

<h4 id="3-2-请求线程"><a href="#3-2-请求线程" class="headerlink" title="3.2 请求线程"></a>3.2 请求线程</h4><p>使用多个线程，不停的取URL队列中的url，并进行处理：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import threading</span><br><span class="line"></span><br><span class="line">class ThreadCrawl(threading.Thread):</span><br><span class="line">    def __init__(self, queue, out_queue):</span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line">        self.queue &#x3D; queue</span><br><span class="line">        self.out_queue &#x3D; out_queue</span><br><span class="line"></span><br><span class="line">    def run(self):</span><br><span class="line">        while True:</span><br><span class="line">            item &#x3D; self.queue.get()</span><br></pre></td></tr></table></figure>
<p>如果队列为空，线程就会被阻塞，直到队列不为空。处理队列中的一条数据后，就需要通知队列已经处理完该条数据</p>
<h4 id="3-3-处理线程"><a href="#3-3-处理线程" class="headerlink" title="3.3 处理线程"></a>3.3 处理线程</h4><p>处理结果队列中的数据，并保存到文件中。如果使用多个线程的话，必须要给文件加上锁</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lock &#x3D; threading.Lock()</span><br><span class="line">f &#x3D; codecs.open(&#39;out.txt&#39;, &#39;w&#39;, &#39;utf8&#39;)</span><br></pre></td></tr></table></figure>
<p>当线程需要写入文件的时候，可以这样处理：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with lock:</span><br><span class="line">    f.write(something)</span><br></pre></td></tr></table></figure>


<h4 id="4-Queue模块中的常用方法"><a href="#4-Queue模块中的常用方法" class="headerlink" title="4. Queue模块中的常用方法:"></a>4. Queue模块中的常用方法:</h4><p>Python的Queue模块中提供了同步的、线程安全的队列类，包括FIFO（先入先出)队列Queue，LIFO（后入先出）队列LifoQueue，和优先级队列PriorityQueue。这些队列都实现了锁原语，能够在多线程中直接使用。可以使用队列来实现线程间的同步</p>
<ul>
<li>Queue.qsize() 返回队列的大小</li>
<li>Queue.empty() 如果队列为空，返回True,反之False</li>
<li>Queue.full() 如果队列满了，返回True,反之False</li>
<li>Queue.full 与 maxsize 大小对应</li>
<li>Queue.get([block[, timeout]])获取队列，timeout等待时间</li>
<li>Queue.get_nowait() 相当Queue.get(False)</li>
<li>Queue.put(item) 写入队列，timeout等待时间</li>
<li>Queue.put_nowait(item) 相当Queue.put(item, False)</li>
<li>Queue.task_done() 在完成一项工作之后，Queue.task_done()函数向任务已经完成的队列发送一个信号</li>
<li>Queue.join() 实际上意味着等到队列为空，再执行别的操作</li>
</ul>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>11. 数据提取-PyQuery</title>
    <url>/2020/01/11/python%E7%88%AC%E8%99%AB/11.%20%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96-PyQuery/</url>
    <content><![CDATA[<h3 id="1-pyquery"><a href="#1-pyquery" class="headerlink" title="1. pyquery"></a>1. pyquery</h3><h4 id="1-1-介绍"><a href="#1-1-介绍" class="headerlink" title="1.1 介绍"></a>1.1 介绍</h4><blockquote>
<p>如果你对CSS选择器与Jquery有有所了解，那么还有个解析库可以适合你–Jquery</p>
</blockquote>
<blockquote>
<p><a href="https://pythonhosted.org/pyquery/" target="_blank" rel="noopener">官网</a><a href="https://pythonhosted.org/pyquery/" target="_blank" rel="noopener">https://pythonhosted.org/pyquery/</a></p>
</blockquote>
<h4 id="1-2-安装"><a href="#1-2-安装" class="headerlink" title="1.2 安装"></a>1.2 安装</h4><blockquote>
<p>pip install pyquery</p>
</blockquote>
<h4 id="1-3-使用方式"><a href="#1-3-使用方式" class="headerlink" title="1.3 使用方式"></a>1.3 使用方式</h4><h5 id="1-3-1-初始化方式"><a href="#1-3-1-初始化方式" class="headerlink" title="1.3.1 初始化方式"></a>1.3.1 初始化方式</h5><ul>
<li>字符串<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyquery import PyQuery as pq</span><br><span class="line">doc &#x3D; pq(str)</span><br><span class="line">print(doc(tagname))</span><br></pre></td></tr></table></figure></li>
<li>url<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyquery import PyQuery as pq</span><br><span class="line">doc &#x3D; pq(url&#x3D;&#39;http:&#x2F;&#x2F;www.baidu.com&#39;)</span><br><span class="line">print(doc(&#39;title&#39;))</span><br></pre></td></tr></table></figure></li>
<li>文件<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyquery import PyQuery as pq</span><br><span class="line">doc &#x3D; pq(filename&#x3D;&#39;demo.html&#39;)</span><br><span class="line">print(doc(tagname))</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="1-3-2-选择节点"><a href="#1-3-2-选择节点" class="headerlink" title="1.3.2 选择节点"></a>1.3.2 选择节点</h5><ul>
<li>获取当前节点<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyquery import PyQuery as pq</span><br><span class="line">doc &#x3D; pq(filename&#x3D;&#39;demo.html&#39;)</span><br><span class="line">doc(&#39;#main #top&#39;)</span><br></pre></td></tr></table></figure></li>
<li>获取子节点<ul>
<li>在doc中一层层写出来</li>
<li>获取到父标签后使用children方法<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyquery import PyQuery as pq</span><br><span class="line">doc &#x3D; pq(filename&#x3D;&#39;demo.html&#39;)</span><br><span class="line">doc(&#39;#main #top&#39;).children()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>获取父节点<ul>
<li>获取到当前节点后使用parent方法</li>
</ul>
</li>
<li>获取兄弟节点<ul>
<li>获取到当前节点后使用siblings方法<h5 id="1-3-3-获取属性"><a href="#1-3-3-获取属性" class="headerlink" title="1.3.3 获取属性"></a>1.3.3 获取属性</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyquery import PyQuery as pq</span><br><span class="line">doc &#x3D; pq(filename&#x3D;&#39;demo.html&#39;)</span><br><span class="line">a &#x3D; doc(&#39;#main #top&#39;)</span><br><span class="line">print(a.attrib[&#39;href&#39;])</span><br></pre></td></tr></table></figure>
<h5 id="1-3-4-获取内容"><a href="#1-3-4-获取内容" class="headerlink" title="1.3.4 获取内容"></a>1.3.4 获取内容</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyquery import PyQuery as pq</span><br><span class="line">doc &#x3D; pq(filename&#x3D;&#39;demo.html&#39;)</span><br><span class="line">div &#x3D; doc(&#39;#main #top&#39;)</span><br><span class="line">print(a.html())</span><br><span class="line">print(a.text())</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h5 id="1-3-5-样例"><a href="#1-3-5-样例" class="headerlink" title="1.3.5 样例"></a>1.3.5 样例</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">from pyquery import PyQuery as pq</span><br><span class="line"># 1.可加载一段HTML字符串，或一个HTML文件，或是一个url地址，</span><br><span class="line">d&#x3D;pq(&quot;&lt;html&gt;&lt;title&gt;hello&lt;&#x2F;title&gt;&lt;&#x2F;html&gt;&quot;)</span><br><span class="line">d&#x3D;pq(filename&#x3D;path_to_html_file)</span><br><span class="line">d&#x3D;pq(url&#x3D;&#39;http:&#x2F;&#x2F;www.baidu.com&#39;)注意：此处url似乎必须写全</span><br><span class="line"> </span><br><span class="line"># 2.html()和text() ——获取相应的HTML块或文本块，</span><br><span class="line">p&#x3D;pq(&quot;&lt;head&gt;&lt;title&gt;hello&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;&quot;)</span><br><span class="line">p(&#39;head&#39;).html()#返回&lt;title&gt;hello&lt;&#x2F;title&gt;</span><br><span class="line">p(&#39;head&#39;).text()#返回hello</span><br><span class="line"> </span><br><span class="line"># 3.根据HTML标签来获取元素，</span><br><span class="line">d&#x3D;pq(&#39;&lt;div&gt;&lt;p&gt;test 1&lt;&#x2F;p&gt;&lt;p&gt;test 2&lt;&#x2F;p&gt;&lt;&#x2F;div&gt;&#39;)</span><br><span class="line">d(&#39;p&#39;)#返回[&lt;p&gt;,&lt;p&gt;]</span><br><span class="line">print d(&#39;p&#39;)#返回&lt;p&gt;test 1&lt;&#x2F;p&gt;&lt;p&gt;test 2&lt;&#x2F;p&gt;</span><br><span class="line">print d(&#39;p&#39;).html()#返回test 1</span><br><span class="line"># 注意：当获取到的元素不只一个时，html()方法只返回首个元素的相应内容块</span><br><span class="line"> </span><br><span class="line"># 4.eq(index) ——根据给定的索引号得到指定元素。接上例，若想得到第二个p标签内的内容，则可以：</span><br><span class="line">print d(&#39;p&#39;).eq(1).html() #返回test 2</span><br><span class="line"> </span><br><span class="line"># 5.filter() ——根据类名、id名得到指定元素，例：</span><br><span class="line">d&#x3D;pq(&quot;&lt;div&gt;&lt;p id&#x3D;&#39;1&#39;&gt;test 1&lt;&#x2F;p&gt;&lt;p class&#x3D;&#39;2&#39;&gt;test 2&lt;&#x2F;p&gt;&lt;&#x2F;div&gt;&quot;)</span><br><span class="line">d(&#39;p&#39;).filter(&#39;#1&#39;) #返回[&lt;p#1&gt;]</span><br><span class="line">d(&#39;p&#39;).filter(&#39;.2&#39;) #返回[&lt;p.2&gt;]</span><br><span class="line"> </span><br><span class="line"># 6.find() ——查找嵌套元素，例：</span><br><span class="line">d&#x3D;pq(&quot;&lt;div&gt;&lt;p id&#x3D;&#39;1&#39;&gt;test 1&lt;&#x2F;p&gt;&lt;p class&#x3D;&#39;2&#39;&gt;test 2&lt;&#x2F;p&gt;&lt;&#x2F;div&gt;&quot;)</span><br><span class="line">d(&#39;div&#39;).find(&#39;p&#39;)#返回[&lt;p#1&gt;, &lt;p.2&gt;]</span><br><span class="line">d(&#39;div&#39;).find(&#39;p&#39;).eq(0)#返回[&lt;p#1&gt;]</span><br><span class="line"> </span><br><span class="line">#7.直接根据类名、id名获取元素，例：</span><br><span class="line">d&#x3D;pq(&quot;&lt;div&gt;&lt;p id&#x3D;&#39;1&#39;&gt;test 1&lt;&#x2F;p&gt;&lt;p class&#x3D;&#39;2&#39;&gt;test 2&lt;&#x2F;p&gt;&lt;&#x2F;div&gt;&quot;)</span><br><span class="line">d(&#39;#1&#39;).html()#返回test 1</span><br><span class="line">d(&#39;.2&#39;).html()#返回test 2</span><br><span class="line"> </span><br><span class="line"># 8.获取属性值，例：</span><br><span class="line">d&#x3D;pq(&quot;&lt;p id&#x3D;&#39;my_id&#39;&gt;&lt;a href&#x3D;&#39;http:&#x2F;&#x2F;hello.com&#39;&gt;hello&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;&quot;)</span><br><span class="line">d(&#39;a&#39;).attr(&#39;href&#39;)#返回http:&#x2F;&#x2F;hello.com</span><br><span class="line">d(&#39;p&#39;).attr(&#39;id&#39;)#返回my_id</span><br><span class="line"> </span><br><span class="line"># 9.修改属性值，例：</span><br><span class="line">d(&#39;a&#39;).attr(&#39;href&#39;, &#39;http:&#x2F;&#x2F;baidu.com&#39;)把href属性修改为了baidu</span><br><span class="line"> </span><br><span class="line"># 10.addClass(value) ——为元素添加类，例：</span><br><span class="line">d&#x3D;pq(&#39;&lt;div&gt;&lt;&#x2F;div&gt;&#39;)</span><br><span class="line">d.addClass(&#39;my_class&#39;)#返回[&lt;div.my_class&gt;]</span><br><span class="line"> </span><br><span class="line"># 11.hasClass(name) #返回判断元素是否包含给定的类，例：</span><br><span class="line">d&#x3D;pq(&quot;&lt;div class&#x3D;&#39;my_class&#39;&gt;&lt;&#x2F;div&gt;&quot;)</span><br><span class="line">d.hasClass(&#39;my_class&#39;)#返回True</span><br><span class="line"> </span><br><span class="line"># 12.children(selector&#x3D;None) ——获取子元素，例：</span><br><span class="line">d&#x3D;pq(&quot;&lt;span&gt;&lt;p id&#x3D;&#39;1&#39;&gt;hello&lt;&#x2F;p&gt;&lt;p id&#x3D;&#39;2&#39;&gt;world&lt;&#x2F;p&gt;&lt;&#x2F;span&gt;&quot;)</span><br><span class="line">d.children()#返回[&lt;p#1&gt;, &lt;p#2&gt;]</span><br><span class="line">d.children(&#39;#2&#39;)#返回[&lt;p#2&gt;]</span><br><span class="line"> </span><br><span class="line"># 13.parents(selector&#x3D;None)——获取父元素，例：</span><br><span class="line">d&#x3D;pq(&quot;&lt;span&gt;&lt;p id&#x3D;&#39;1&#39;&gt;hello&lt;&#x2F;p&gt;&lt;p id&#x3D;&#39;2&#39;&gt;world&lt;&#x2F;p&gt;&lt;&#x2F;span&gt;&quot;)</span><br><span class="line">d(&#39;p&#39;).parents()#返回[&lt;span&gt;]</span><br><span class="line">d(&#39;#1&#39;).parents(&#39;span&#39;)#返回[&lt;span&gt;]</span><br><span class="line">d(&#39;#1&#39;).parents(&#39;p&#39;)#返回[]</span><br><span class="line"> </span><br><span class="line"># 14.clone() ——返回一个节点的拷贝</span><br><span class="line"> </span><br><span class="line">#15.empty() ——移除节点内容</span><br><span class="line"> </span><br><span class="line"># 16.nextAll(selector&#x3D;None) ——返回后面全部的元素块，例：</span><br><span class="line">d&#x3D;pq(&quot;&lt;p id&#x3D;&#39;1&#39;&gt;hello&lt;&#x2F;p&gt;&lt;p id&#x3D;&#39;2&#39;&gt;world&lt;&#x2F;p&gt;&lt;img scr&#x3D;&#39;&#39; &#x2F;&gt;&quot;)</span><br><span class="line">d(&#39;p:first&#39;).nextAll()#返回[&lt;p#2&gt;, &lt;img&gt;]</span><br><span class="line">d(&#39;p:last&#39;).nextAll()#返回[&lt;img&gt;]</span><br><span class="line"> </span><br><span class="line"># 17.not_(selector) ——返回不匹配选择器的元素，例：</span><br><span class="line">d&#x3D;pq(&quot;&lt;p id&#x3D;&#39;1&#39;&gt;test 1&lt;&#x2F;p&gt;&lt;p id&#x3D;&#39;2&#39;&gt;test 2&lt;&#x2F;p&gt;&quot;)</span><br><span class="line">d(&#39;p&#39;).not_(&#39;#2&#39;)#返回[&lt;p#1&gt;]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>10. 数据提取-JsonPath</title>
    <url>/2020/01/10/python%E7%88%AC%E8%99%AB/10.%20%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96-JsonPath/</url>
    <content><![CDATA[<h3 id="1-JSON与JsonPATH"><a href="#1-JSON与JsonPATH" class="headerlink" title="1. JSON与JsonPATH"></a>1. JSON与JsonPATH</h3><p>JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成。适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。</p>
<p>JSON和XML的比较可谓不相上下。</p>
<p>Python 中自带了JSON模块，直接import json就可以使用了。</p>
<p>官方文档：<a href="http://docs.python.org/library/json.html" target="_blank" rel="noopener">http://docs.python.org/library/json.html</a></p>
<p>Json在线解析网站：<a href="http://www.json.cn/#" target="_blank" rel="noopener">http://www.json.cn/#</a></p>
<h3 id="2-JSON"><a href="#2-JSON" class="headerlink" title="2. JSON"></a>2. JSON</h3><p>json简单说就是javascript中的对象和数组，所以这两种结构就是对象和数组两种结构，通过这两种结构可以表示各种复杂的结构</p>
<ol>
<li><p>对象：对象在js中表示为{ }括起来的内容，数据结构为 { key：value, key：value, … }的键值对的结构，在面向对象的语言中，key为对象的属性，value为对应的属性值，所以很容易理解，取值方法为 对象.key 获取属性值，这个属性值的类型可以是数字、字符串、数组、对象这几种</p>
</li>
<li><p>数组：数组在js中是中括号[ ]括起来的内容，数据结构为 [“Python”, “javascript”, “C++”, …]，取值方式和所有语言中一样，使用索引获取，字段值的类型可以是 数字、字符串、数组、对象几种</p>
</li>
</ol>
<h3 id="3-Python中的json模块"><a href="#3-Python中的json模块" class="headerlink" title="3. Python中的json模块"></a>3. Python中的json模块</h3><blockquote>
<p>json模块提供了四个功能：dumps、dump、loads、load，用于字符串 和 python数据类型间进行转换</p>
</blockquote>
<h4 id="3-1-json-loads"><a href="#3-1-json-loads" class="headerlink" title="3.1 json.loads()"></a>3.1 json.loads()</h4><blockquote>
<p>把Json格式字符串解码转换成Python对象 从json到python的类型转化对照如下：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">strList &#x3D; &#39;[1, 2, 3, 4]&#39;</span><br><span class="line">strDict &#x3D; &#39;&#123;&quot;city&quot;: &quot;北京&quot;, &quot;name&quot;: &quot;范爷&quot;&#125;&#39;</span><br><span class="line">json.loads(strList) </span><br><span class="line"># [1, 2, 3, 4]</span><br><span class="line">json.loads(strDict) # json数据自动按Unicode存储</span><br><span class="line"># &#123;u&#39;city&#39;: u&#39;\u5317\u4eac&#39;, u&#39;name&#39;: u&#39;\u5927\u732b&#39;&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-2-json-dumps"><a href="#3-2-json-dumps" class="headerlink" title="3.2 json.dumps()"></a>3.2 json.dumps()</h4><blockquote>
<p>实现python类型转化为json字符串，返回一个str对象 把一个Python对象编码转换成Json字符串</p>
</blockquote>
<p>从python原始类型向json类型的转化对照如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># json_dumps.py</span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">listStr &#x3D; [1, 2, 3, 4]</span><br><span class="line">tupleStr &#x3D; (1, 2, 3, 4)</span><br><span class="line">dictStr &#x3D; &#123;&quot;city&quot;: &quot;北京&quot;, &quot;name&quot;: &quot;范爷&quot;&#125;</span><br><span class="line"></span><br><span class="line">json.dumps(listStr)</span><br><span class="line"># &#39;[1, 2, 3, 4]&#39;</span><br><span class="line">json.dumps(tupleStr)</span><br><span class="line"># &#39;[1, 2, 3, 4]&#39;</span><br><span class="line"></span><br><span class="line"># 注意：json.dumps() 序列化时默认使用的ascii编码</span><br><span class="line"># 添加参数 ensure_ascii&#x3D;False 禁用ascii编码，按utf-8编码</span><br><span class="line"></span><br><span class="line">json.dumps(dictStr) </span><br><span class="line"># &#39;&#123;&quot;city&quot;: &quot;\\u5317\\u4eac&quot;, &quot;name&quot;: &quot;\\u5927\\u5218&quot;&#125;&#39;</span><br><span class="line"></span><br><span class="line">print(json.dumps(dictStr, ensure_ascii&#x3D;False))</span><br><span class="line"># &#123;&quot;city&quot;: &quot;北京&quot;, &quot;name&quot;: &quot;范爷&quot;&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-3-json-dump"><a href="#3-3-json-dump" class="headerlink" title="3.3 json.dump()"></a>3.3 json.dump()</h4><blockquote>
<p>将Python内置类型序列化为json对象后写入文件</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">listStr &#x3D; [&#123;&quot;city&quot;: &quot;北京&quot;&#125;, &#123;&quot;name&quot;: &quot;范爷&quot;&#125;]</span><br><span class="line">json.dump(listStr, open(&quot;listStr.json&quot;,&quot;w&quot;), ensure_ascii&#x3D;False)</span><br><span class="line"></span><br><span class="line">dictStr &#x3D; &#123;&quot;city&quot;: &quot;北京&quot;, &quot;name&quot;: &quot;范爷&quot;&#125;</span><br><span class="line">json.dump(dictStr, open(&quot;dictStr.json&quot;,&quot;w&quot;), ensure_ascii&#x3D;False)</span><br></pre></td></tr></table></figure>
<h4 id="3-4-json-load"><a href="#3-4-json-load" class="headerlink" title="3.4 json.load()"></a>3.4 json.load()</h4><blockquote>
<p>读取文件中json形式的字符串元素 转化成python类型</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">strList &#x3D; json.load(open(&quot;listStr.json&quot;))</span><br><span class="line">print(strList)</span><br><span class="line"></span><br><span class="line"># [&#123;u&#39;city&#39;: u&#39;\u5317\u4eac&#39;&#125;, &#123;u&#39;name&#39;: u&#39;\u5927\u5218&#39;&#125;]</span><br><span class="line"></span><br><span class="line">strDict &#x3D; json.load(open(&quot;dictStr.json&quot;))</span><br><span class="line">print(strDict)</span><br><span class="line"># &#123;u&#39;city&#39;: u&#39;\u5317\u4eac&#39;, u&#39;name&#39;: u&#39;\u5927\u5218&#39;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-JsonPath"><a href="#4-JsonPath" class="headerlink" title="4 JsonPath"></a>4 JsonPath</h3><p>JsonPath 是一种信息抽取类库，是从JSON文档中抽取指定信息的工具，提供多种语言实现版本，包括：Javascript, Python， PHP 和 Java。</p>
<p>JsonPath 对于 JSON 来说，相当于 XPATH 对于 XML。</p>
<p>安装方法：<code>pip install jsonpath</code></p>
<p>官方文档：<a href="http://goessner.net/articles/JsonPath" target="_blank" rel="noopener">http://goessner.net/articles/JsonPath</a></p>
<h3 id="5-JsonPath与XPath语法对比"><a href="#5-JsonPath与XPath语法对比" class="headerlink" title="5 JsonPath与XPath语法对比"></a>5 JsonPath与XPath语法对比</h3><p>Json结构清晰，可读性高，复杂度低，非常容易匹配，下表中对应了XPath的用法<br>XPath | JSONPath | 描述<br>–|–|–<br>/ |    $ | 根节点<br>. |    @ | 现行节点<br>/ |    .or[] |    取子节点<br>..| n/a|     取父节点，Jsonpath未支持<br>//| .. |    就是不管位置，选择所有符合条件的条件</p>
<ul>
<li>|    * |匹配所有元素节点<br>@ |    n/a|     根据属性访问，Json不支持，因为Json是个Key-value递归结构，不需要。<br>[]| [] |    迭代器标示（可以在里边做简单的迭代操作，如数组下标，根据内容选值等）<br>| |    [,]|     支持迭代器中做多选。<br>[]| ?()|     支持过滤操作.<br>n/a| ()|     支持表达式计算<br>() |n/a|     分组，JsonPath不支持</li>
</ul>
<h4 id="6-示例"><a href="#6-示例" class="headerlink" title="6. 示例"></a>6. 示例</h4><p>我们以拉勾网城市JSON文件 <a href="http://www.lagou.com/lbs/getAllCitySearchLabels.json" target="_blank" rel="noopener">http://www.lagou.com/lbs/getAllCitySearchLabels.json</a> 为例，获取所有城市</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">from urllib.request import Request</span><br><span class="line">import jsonpath</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">url &#x3D; &#39;http:&#x2F;&#x2F;www.lagou.com&#x2F;lbs&#x2F;getAllCitySearchLabels.json&#39;</span><br><span class="line">request &#x3D;Request(url)</span><br><span class="line">response &#x3D; urlopen(request)</span><br><span class="line">html &#x3D; response.read()</span><br><span class="line"># 把json格式字符串转换成python对象</span><br><span class="line">jsonobj &#x3D; json.loads(html)</span><br><span class="line"># 从根节点开始，匹配name节点</span><br><span class="line">citylist &#x3D; jsonpath.jsonpath(jsonobj,&#39;$..name&#39;)</span><br><span class="line">print(citylist)</span><br><span class="line">print(type(citylist))</span><br><span class="line">fp &#x3D; open(&#39;city.json&#39;,&#39;w&#39;)</span><br><span class="line">content &#x3D; json.dumps(citylist, ensure_ascii&#x3D;False)</span><br><span class="line">print(content)</span><br><span class="line">fp.write(content)</span><br><span class="line">fp.close()</span><br></pre></td></tr></table></figure>

<h4 id="7-注意事项"><a href="#7-注意事项" class="headerlink" title="7. 注意事项"></a>7. 注意事项</h4><ul>
<li><p>json.loads() 是把 Json格式字符串解码转换成Python对象，如果在json.loads的时候出错，要注意被解码的Json字符的编码。<br>如果传入的字符串的编码不是UTF-8的话，需要指定字符编码的参数 encoding</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dataDict &#x3D; json.loads(jsonStrGBK);</span><br></pre></td></tr></table></figure></li>
<li><p>dataJsonStr是JSON字符串，假设其编码本身是非UTF-8的话而是GBK 的，那么上述代码会导致出错，改为对应的：</p>
   <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dataDict &#x3D; json.loads(jsonStrGBK, encoding&#x3D;&quot;GBK&quot;);</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果 dataJsonStr通过encoding指定了合适的编码，但是其中又包含了其他编码的字符，则需要先去将dataJsonStr转换为Unicode，然后再指定编码格式调用json.loads()</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dataJsonStrUni &#x3D; dataJsonStr.decode(&quot;GB2312&quot;); </span><br><span class="line">dataDict &#x3D; json.loads(dataJsonStrUni, encoding&#x3D;&quot;GB2312&quot;);</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="7-1-字符串编码转换"><a href="#7-1-字符串编码转换" class="headerlink" title="7.1 字符串编码转换"></a>7.1 字符串编码转换</h5><p>这是中国程序员最苦逼的地方，什么乱码之类的几乎都是由汉字引起的</p>
<p>其实编码问题很好搞定，只要记住一点：</p>
<p><strong>任何平台的任何编码 都能和 Unicode 互相转换</strong></p>
<p>UTF-8 与 GBK 互相转换，那就先把UTF-8转换成Unicode，再从Unicode转换成GBK，反之同理。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 这是一个 UTF-8 编码的字符串</span><br><span class="line">utf8Str &#x3D; &quot;你好地球&quot;</span><br><span class="line"></span><br><span class="line"># 1. 将 UTF-8 编码的字符串 转换成 Unicode 编码</span><br><span class="line">unicodeStr &#x3D; utf8Str.decode(&quot;UTF-8&quot;)</span><br><span class="line"></span><br><span class="line"># 2. 再将 Unicode 编码格式字符串 转换成 GBK 编码</span><br><span class="line">gbkData &#x3D; unicodeStr.encode(&quot;GBK&quot;)</span><br><span class="line"></span><br><span class="line"># 1. 再将 GBK 编码格式字符串 转化成 Unicode</span><br><span class="line">unicodeStr &#x3D; gbkData.decode(&quot;gbk&quot;)</span><br><span class="line"></span><br><span class="line"># 2. 再将 Unicode 编码格式字符串转换成 UTF-8</span><br><span class="line">utf8Str &#x3D; unicodeStr.encode(&quot;UTF-8&quot;)</span><br></pre></td></tr></table></figure>
<p>decode的作用是将其他编码的字符串转换成 Unicode 编码</p>
<p>encode的作用是将 Unicode 编码转换成其他编码的字符串</p>
<p>一句话：UTF-8是对Unicode字符集进行编码的一种编码方式</p>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>09. 数据提取-XPath</title>
    <url>/2020/01/09/python%E7%88%AC%E8%99%AB/09.%20%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96-XPath/</url>
    <content><![CDATA[<h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h3><blockquote>
<p>之前 BeautifulSoup 的用法，这个已经是非常强大的库了，不过还有一些比较流行的解析库，例如 lxml，使用的是 Xpath 语法，同样是效率比较高的解析方法。如果大家对 BeautifulSoup 使用不太习惯的话，可以尝试下 Xpath</p>
</blockquote>
<p><a href="http://lxml.de/index.html" target="_blank" rel="noopener">官网</a> <a href="http://lxml.de/index.html" target="_blank" rel="noopener">http://lxml.de/index.html</a></p>
<p><a href="http://www.w3school.com.cn/xpath/index.asp" target="_blank" rel="noopener">w3c</a> <a href="http://www.w3school.com.cn/xpath/index.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/xpath/index.asp</a></p>
<h3 id="2-安装"><a href="#2-安装" class="headerlink" title="2. 安装"></a>2. 安装</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install lxml</span><br></pre></td></tr></table></figure>

<h3 id="3-XPath语法"><a href="#3-XPath语法" class="headerlink" title="3. XPath语法"></a>3. XPath语法</h3><blockquote>
<p>XPath 是一门在 XML 文档中查找信息的语言。XPath 可用来在 XML 文档中对元素和属性进行遍历。XPath 是 W3C XSLT 标准的主要元素，并且 XQuery 和 XPointer 都构建于 XPath 表达之上</p>
</blockquote>
<h4 id="3-1-节点的关系"><a href="#3-1-节点的关系" class="headerlink" title="3.1 节点的关系"></a>3.1 节点的关系</h4><ul>
<li>父（Parent）</li>
<li>子（Children）</li>
<li>同胞（Sibling）</li>
<li>先辈（Ancestor）</li>
<li>后代（Descendant）</li>
</ul>
<h4 id="3-2-选取节点"><a href="#3-2-选取节点" class="headerlink" title="3.2 选取节点"></a>3.2 选取节点</h4><h5 id="3-2-1-常用的路径表达式"><a href="#3-2-1-常用的路径表达式" class="headerlink" title="3.2.1 常用的路径表达式"></a>3.2.1 常用的路径表达式</h5><table>
<thead>
<tr>
<th>表达式</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>nodename</td>
<td>选取此节点的所有子节点</td>
</tr>
<tr>
<td>/</td>
<td>从根节点选取</td>
</tr>
<tr>
<td>//</td>
<td>从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置</td>
</tr>
<tr>
<td>.</td>
<td>选取当前节点</td>
</tr>
<tr>
<td>..</td>
<td>选取当前节点的父节点</td>
</tr>
<tr>
<td>@</td>
<td>选取属性</td>
</tr>
</tbody></table>
<h5 id="3-2-2-通配符"><a href="#3-2-2-通配符" class="headerlink" title="3.2.2 通配符"></a>3.2.2 通配符</h5><p>XPath 通配符可用来选取未知的 XML 元素。<br>通配符 | 描述 | 举例 | 结果<br>–|– |– | – </p>
<ul>
<li>| 匹配任何元素节点|xpath(‘div/<em>‘) | 获取div下的所有子节点<br>@</em>     | 匹配任何属性节点|xpath(‘div[@*]’) |选取所有带属性的div节点<br>node() | 匹配任何类型的节点</li>
</ul>
<h5 id="3-2-3-选取若干路径"><a href="#3-2-3-选取若干路径" class="headerlink" title="3.2.3 选取若干路径"></a>3.2.3 选取若干路径</h5><p>通过在路径表达式中使用“|”运算符，您可以选取若干个路径<br>表达式 | 结果<br>–|–<br>xpath(‘//div<code>|</code>//table’)|获取所有的div与table节点</p>
<h5 id="3-2-4-谓语"><a href="#3-2-4-谓语" class="headerlink" title="3.2.4 谓语"></a>3.2.4 谓语</h5><p>谓语被嵌在方括号内，用来查找某个特定的节点或包含某个制定的值的节点</p>
<table>
<thead>
<tr>
<th>表达式</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>xpath(‘/body/div[1]’)</td>
<td>选取body下的第一个div节点</td>
</tr>
<tr>
<td>xpath(‘/body/div[last()]’)</td>
<td>选取body下最后一个div节点</td>
</tr>
<tr>
<td>xpath(‘/body/div[last()-1]’)</td>
<td>选取body下倒数第二个节点</td>
</tr>
<tr>
<td>xpath(‘/body/div[positon()&lt;3]’)</td>
<td>选取body下前丙个div节点</td>
</tr>
<tr>
<td>xpath(‘/body/div[@class]’)</td>
<td>选取body下带有class属性的div节点</td>
</tr>
<tr>
<td>xpath(‘/body/div[@class=”main”]’)</td>
<td>选取body下class属性为main的div节点</td>
</tr>
<tr>
<td>xpath(‘/body/div[price&gt;35.00]’)</td>
<td>选取body下price元素大于35的div节点</td>
</tr>
</tbody></table>
<h5 id="3-2-5-XPath-运算符"><a href="#3-2-5-XPath-运算符" class="headerlink" title="3.2.5 XPath 运算符"></a>3.2.5 XPath 运算符</h5><table>
<thead>
<tr>
<th>运算符</th>
<th>描述</th>
<th>实例</th>
<th>返回值</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
<td>计算两个节点集</td>
<td>//book</td>
</tr>
<tr>
<td>+</td>
<td>加法</td>
<td>6 + 4</td>
<td>10</td>
</tr>
<tr>
<td>–</td>
<td>减法</td>
<td>6 – 4</td>
<td>2</td>
</tr>
<tr>
<td>*</td>
<td>乘法</td>
<td>6 * 4</td>
<td>24</td>
</tr>
<tr>
<td>div</td>
<td>除法</td>
<td>8 div 4</td>
<td>2</td>
</tr>
<tr>
<td>=</td>
<td>等于</td>
<td>price=9.80</td>
<td>如果 price 是 9.80，则返回 true。如果 price 是 9.90，则返回 false。</td>
</tr>
<tr>
<td>!=</td>
<td>不等于</td>
<td>price!=9.80</td>
<td>如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。</td>
</tr>
<tr>
<td>&lt;</td>
<td>小于</td>
<td>price&lt;9.80</td>
<td>如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。</td>
</tr>
<tr>
<td>&lt;=</td>
<td>小于或等于</td>
<td>price&lt;=9.80</td>
<td>如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。</td>
</tr>
<tr>
<td>&gt;</td>
<td>大于</td>
<td>price&gt;9.80</td>
<td>如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。</td>
</tr>
<tr>
<td>&gt;=</td>
<td>大于或等于</td>
<td>price&gt;=9.80</td>
<td>如果 price 是 9.90，则返回 true。如果 price 是 9.70，则返回 false。</td>
</tr>
<tr>
<td>or</td>
<td>或</td>
<td>price=9.80 or price=9.70</td>
<td>如果 price 是 9.80，则返回 true。如果 price 是 9.50，则返回 false。</td>
</tr>
<tr>
<td>and</td>
<td>与</td>
<td>price&gt;9.00 and price&lt;9.90</td>
<td>如果 price 是 9.80，则返回 true。如果 price 是 8.50，则返回 false。</td>
</tr>
<tr>
<td>mod</td>
<td>计算除法的余数</td>
<td>5 mod 2</td>
<td>1</td>
</tr>
</tbody></table>
<h4 id="3-3-使用"><a href="#3-3-使用" class="headerlink" title="3.3 使用"></a>3.3 使用</h4><h5 id="3-3-1-小例子"><a href="#3-3-1-小例子" class="headerlink" title="3.3.1 小例子"></a>3.3.1 小例子</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from lxml import etree</span><br><span class="line">text &#x3D; &#39;&#39;&#39;</span><br><span class="line">&lt;div&gt;</span><br><span class="line">    &lt;ul&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-0&quot;&gt;&lt;a href&#x3D;&quot;link1.html&quot;&gt;first item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-1&quot;&gt;&lt;a href&#x3D;&quot;link2.html&quot;&gt;second item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-inactive&quot;&gt;&lt;a href&#x3D;&quot;link3.html&quot;&gt;third item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-1&quot;&gt;&lt;a href&#x3D;&quot;link4.html&quot;&gt;fourth item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-0&quot;&gt;&lt;a href&#x3D;&quot;link5.html&quot;&gt;fifth item&lt;&#x2F;a&gt;</span><br><span class="line">     &lt;&#x2F;ul&gt;</span><br><span class="line"> &lt;&#x2F;div&gt;</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">html &#x3D; etree.HTML(text)</span><br><span class="line">result &#x3D; etree.tostring(html)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>

<p>首先我们使用 lxml 的 etree 库，然后利用 etree.HTML 初始化，然后我们将其打印出来。</p>
<p>其中，这里体现了 lxml 的一个非常实用的功能就是自动修正 html 代码，大家应该注意到了，最后一个 li 标签，其实我把尾标签删掉了，是不闭合的。不过，lxml 因为继承了 libxml2 的特性，具有自动修正 HTML 代码的功能。</p>
<p>所以输出结果是这样的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;html&gt;&lt;body&gt;</span><br><span class="line">&lt;div&gt;</span><br><span class="line">    &lt;ul&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-0&quot;&gt;&lt;a href&#x3D;&quot;link1.html&quot;&gt;first item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-1&quot;&gt;&lt;a href&#x3D;&quot;link2.html&quot;&gt;second item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-inactive&quot;&gt;&lt;a href&#x3D;&quot;link3.html&quot;&gt;third item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-1&quot;&gt;&lt;a href&#x3D;&quot;link4.html&quot;&gt;fourth item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-0&quot;&gt;&lt;a href&#x3D;&quot;link5.html&quot;&gt;fifth item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">&lt;&#x2F;ul&gt;</span><br><span class="line"> &lt;&#x2F;div&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;body&gt;&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure>

<p>不仅补全了 li 标签，还添加了 body，html 标签。<br>文件读取</p>
<p>除了直接读取字符串，还支持从文件读取内容。比如我们新建一个文件叫做 hello.html，内容为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;div&gt;</span><br><span class="line">    &lt;ul&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-0&quot;&gt;&lt;a href&#x3D;&quot;link1.html&quot;&gt;first item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-1&quot;&gt;&lt;a href&#x3D;&quot;link2.html&quot;&gt;second item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-inactive&quot;&gt;&lt;a href&#x3D;&quot;link3.html&quot;&gt;&lt;span class&#x3D;&quot;bold&quot;&gt;third item&lt;&#x2F;span&gt;&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-1&quot;&gt;&lt;a href&#x3D;&quot;link4.html&quot;&gt;fourth item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">         &lt;li class&#x3D;&quot;item-0&quot;&gt;&lt;a href&#x3D;&quot;link5.html&quot;&gt;fifth item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">     &lt;&#x2F;ul&gt;</span><br><span class="line"> &lt;&#x2F;div&gt;</span><br></pre></td></tr></table></figure>

<p>利用 parse 方法来读取文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from lxml import etree</span><br><span class="line">html &#x3D; etree.parse(&#39;hello.html&#39;)</span><br><span class="line">result &#x3D; etree.tostring(html, pretty_print&#x3D;True)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>

<p>同样可以得到相同的结果</p>
<h5 id="3-3-2-XPath具体使用"><a href="#3-3-2-XPath具体使用" class="headerlink" title="3.3.2 XPath具体使用"></a>3.3.2 XPath具体使用</h5><p>依然以上一段程序为例</p>
<ol>
<li>获取所有的 <code>&lt;li&gt;</code> 标签</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from lxml import etree</span><br><span class="line">html &#x3D; etree.parse(&#39;hello.html&#39;)</span><br><span class="line">print (type(html))</span><br><span class="line">result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li&#39;)</span><br><span class="line">print (result)</span><br><span class="line">print (len(result))</span><br><span class="line">print (type(result))</span><br><span class="line">print (type(result[0]))</span><br></pre></td></tr></table></figure>
<p>运行结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;type &#39;lxml.etree._ElementTree&#39;&gt;</span><br><span class="line">[&lt;Element li at 0x1014e0e18&gt;, &lt;Element li at 0x1014e0ef0&gt;, &lt;Element li at 0x1014e0f38&gt;, &lt;Element li at 0x1014e0f80&gt;, &lt;Element li at 0x1014e0fc8&gt;]</span><br><span class="line"></span><br><span class="line">&lt;type &#39;list&#39;&gt;</span><br><span class="line">&lt;type &#39;lxml.etree._Element&#39;&gt;</span><br></pre></td></tr></table></figure>

<p>可见，etree.parse 的类型是 ElementTree，通过调用 xpath 以后，得到了一个列表，包含了 5 个 <code>&lt;li&gt;</code> 元素，每个元素都是 Element 类型</p>
<ol start="2">
<li>获取<code>&lt;li&gt;</code>标签的所有 class<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li&#x2F;@class&#39;)</span><br><span class="line">print (result)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>运行结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&#39;item-0&#39;, &#39;item-1&#39;, &#39;item-inactive&#39;, &#39;item-1&#39;, &#39;item-0&#39;]</span><br></pre></td></tr></table></figure>


<ol start="3">
<li>获取 <code>&lt;li&gt;</code> 标签下 href 为 link1.html 的 <code>&lt;a&gt;</code> 标签<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li&#x2F;a[@href&#x3D;&quot;link1.html&quot;]&#39;)</span><br><span class="line">print (result)</span><br></pre></td></tr></table></figure>
运行结果<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&lt;Element a at 0x10ffaae18&gt;]</span><br></pre></td></tr></table></figure></li>
<li>获取<code>&lt;li&gt;</code>标签下的所有 <code>&lt;span&gt;</code> 标签</li>
</ol>
<p><strong>注意</strong>: 这么写是不对的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li&#x2F;span&#39;)</span><br><span class="line"></span><br><span class="line">#因为 &#x2F; 是用来获取子元素的，而 &lt;span&gt; 并不是 &lt;li&gt; 的子元素，所以，要用双斜杠</span><br><span class="line">result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li&#x2F;&#x2F;span&#39;)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>运行结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&lt;Element span at 0x10d698e18&gt;]</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>获取 <code>&lt;li&gt;</code> 标签下的所有 class，不包括<code>&lt;li&gt;</code></li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li&#x2F;a&#x2F;&#x2F;@class&#39;)</span><br><span class="line">print (resul)t</span><br><span class="line">#运行结果</span><br><span class="line">[&#39;blod&#39;]</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>获取最后一个 <code>&lt;li&gt;</code> 的 <code>&lt;a&gt;</code> 的 href<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li[last()]&#x2F;a&#x2F;@href&#39;)</span><br><span class="line">print (result)</span><br></pre></td></tr></table></figure>
运行结果<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&#39;link5.html&#39;]</span><br></pre></td></tr></table></figure>


</li>
</ol>
<ol start="7">
<li><p>获取倒数第二个元素的内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">result &#x3D; html.xpath(&#39;&#x2F;&#x2F;li[last()-1]&#x2F;a&#39;)</span><br><span class="line">print (result[0].text)</span><br></pre></td></tr></table></figure>
<p>运行结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fourth item</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取 class 为 bold 的标签名</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">result &#x3D; html.xpath(&#39;&#x2F;&#x2F;*[@class&#x3D;&quot;bold&quot;]&#39;)</span><br><span class="line">print (result[0].tag)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>运行结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">span</span><br></pre></td></tr></table></figure>

<h4 id="选择XML文件中节点："><a href="#选择XML文件中节点：" class="headerlink" title="选择XML文件中节点："></a>选择XML文件中节点：</h4><ul>
<li>element（元素节点）</li>
<li>attribute（属性节点）</li>
<li>text （文本节点）</li>
<li>concat(元素节点,元素节点)</li>
<li>comment （注释节点）</li>
<li>root （根节点）</li>
</ul>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>08. 数据提取-Beautiful Soup</title>
    <url>/2020/01/08/python%E7%88%AC%E8%99%AB/08.%20%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96-Beautiful%20Soup/</url>
    <content><![CDATA[<h3 id="1-Beautiful-Soup的简介"><a href="#1-Beautiful-Soup的简介" class="headerlink" title="1. Beautiful Soup的简介"></a>1. Beautiful Soup的简介</h3><blockquote>
<p>Beautiful Soup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。</p>
</blockquote>
<blockquote>
<p>Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。</p>
</blockquote>
<blockquote>
<p>Beautiful Soup已成为和lxml、html6lib一样出色的python解释器，为用户灵活地提供不同的解析策略或强劲的速度</p>
</blockquote>
<p><a href="http://beautifulsoup.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">官网</a><a href="http://beautifulsoup.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">http://beautifulsoup.readthedocs.io/zh_CN/latest/</a></p>
<h3 id="2-Beautiful-Soup-安装"><a href="#2-Beautiful-Soup-安装" class="headerlink" title="2. Beautiful Soup 安装"></a>2. Beautiful Soup 安装</h3><blockquote>
<p>Beautiful Soup 3 目前已经停止开发，推荐在现在的项目中使用Beautiful Soup 4，不过它已经被移植到BS4了,也就是说导入时我们需要 import bs4</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install beautifulsoup4</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Beautiful Soup支持Python标准库中的HTML解析器,还支持一些第三方的解析器，如果我们不安装它，则 Python 会使用 Python默认的解析器，lxml 解析器更加强大，速度更快，推荐安装</p>
</blockquote>
<table>
<thead>
<tr>
<th>解析器</th>
<th>使用方法</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody><tr>
<td>Python标准库</td>
<td>BeautifulSoup(markup, “html.parser”)</td>
<td>1. Python的内置标准库  2. 执行速度适中 3.文档容错能力强</td>
<td>Python 2.7.3 or 3.2.2)前 的版本中文档容错能力差</td>
</tr>
<tr>
<td>lxml HTML 解析器</td>
<td>BeautifulSoup(markup, “lxml”)</td>
<td>1. 速度快 2.文档容错能力强</td>
<td>需要安装C语言库</td>
</tr>
<tr>
<td>lxml XML 解析器</td>
<td>BeautifulSoup(markup, [“lxml”, “xml”])  BeautifulSoup(markup, “xml”)</td>
<td>1. 速度快 2.唯一支持XML的解析器 3.需要安装C语言库</td>
<td></td>
</tr>
<tr>
<td>html5lib</td>
<td>BeautifulSoup(markup, “html5lib”)</td>
<td>1. 最好的容错性 2.以浏览器的方式解析文档 3.生成HTML5格式的文档 4.速度慢</td>
<td>不依赖外部扩展</td>
</tr>
</tbody></table>
<h3 id="3-创建-Beautiful-Soup-对象"><a href="#3-创建-Beautiful-Soup-对象" class="headerlink" title="3. 创建 Beautiful Soup 对象"></a>3. 创建 Beautiful Soup 对象</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line">bs &#x3D; BeautifulSoup(html,&quot;lxml&quot;)</span><br></pre></td></tr></table></figure>

<h3 id="4-四大对象种类"><a href="#4-四大对象种类" class="headerlink" title="4. 四大对象种类"></a>4. 四大对象种类</h3><blockquote>
<p>Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,所有对象可以归纳为4种:</p>
</blockquote>
<ul>
<li>Tag</li>
<li>NavigableString</li>
<li>BeautifulSoup</li>
<li>Comment</li>
</ul>
<h4 id="4-1-Tag-是什么？通俗点讲就是-HTML-中的一个个标签"><a href="#4-1-Tag-是什么？通俗点讲就是-HTML-中的一个个标签" class="headerlink" title="4.1 Tag 是什么？通俗点讲就是 HTML 中的一个个标签"></a>4.1 Tag 是什么？通俗点讲就是 HTML 中的一个个标签</h4><p>例如：<code>&lt;div&gt;</code> <code>&lt;title&gt;</code></p>
<p>使用方式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#以以下代码为例子</span><br><span class="line">&lt;title&gt;尚学堂&lt;&#x2F;title&gt;</span><br><span class="line">&lt;div class&#x3D;&#39;info&#39; float&#x3D;&#39;left&#39;&gt;Welcome to SXT&lt;&#x2F;div&gt;</span><br><span class="line">&lt;div class&#x3D;&#39;info&#39; float&#x3D;&#39;right&#39;&gt;</span><br><span class="line">    &lt;span&gt;Good Good Study&lt;&#x2F;span&gt;</span><br><span class="line">    &lt;a href&#x3D;&#39;www.bjsxt.cn&#39;&gt;&lt;&#x2F;a&gt;</span><br><span class="line">    &lt;strong&gt;&lt;!--没用--&gt;&lt;&#x2F;strong&gt;</span><br><span class="line">&lt;&#x2F;div&gt;</span><br></pre></td></tr></table></figure>
<h5 id="4-1-1-获取标签"><a href="#4-1-1-获取标签" class="headerlink" title="4.1.1 获取标签"></a>4.1.1 获取标签</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#以lxml方式解析</span><br><span class="line">soup &#x3D; BeautifulSoup(info, &#39;lxml&#39;)</span><br><span class="line">print(soup.title)</span><br><span class="line"># &lt;title&gt;尚学堂&lt;&#x2F;title&gt;</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong></p>
<blockquote>
<p>相同的标签只能获取第一个符合要求的标签</p>
</blockquote>
<h5 id="4-1-2-获取属性："><a href="#4-1-2-获取属性：" class="headerlink" title="4.1.2 获取属性："></a>4.1.2 获取属性：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#获取所有属性</span><br><span class="line">print(soup.title.attrs)</span><br><span class="line">#class&#x3D;&#39;info&#39; float&#x3D;&#39;left&#39;</span><br><span class="line"></span><br><span class="line">#获取单个属性的值</span><br><span class="line">print(soup.div.get(&#39;class&#39;))</span><br><span class="line">print(soup.div[&#39;class&#39;])</span><br><span class="line">print(soup.a[&#39;href&#39;])</span><br><span class="line">#info</span><br></pre></td></tr></table></figure>

<h4 id="4-2-NavigableString-获取内容"><a href="#4-2-NavigableString-获取内容" class="headerlink" title="4.2 NavigableString 获取内容"></a>4.2 NavigableString 获取内容</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(soup.title.string)</span><br><span class="line">print(soup.title.text)</span><br><span class="line">#尚学堂</span><br></pre></td></tr></table></figure>

<h4 id="4-3-BeautifulSoup"><a href="#4-3-BeautifulSoup" class="headerlink" title="4.3 BeautifulSoup"></a>4.3 BeautifulSoup</h4><blockquote>
<p>BeautifulSoup 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象,它支持 遍历文档树 和 搜索文档树 中描述的大部分的方法.</p>
</blockquote>
<blockquote>
<p>因为 BeautifulSoup 对象并不是真正的HTML或XML的tag,所以它没有name和attribute属性.但有时查看它的 .name 属性是很方便的,所以 BeautifulSoup 对象包含了一个值为 “[document]” 的特殊属性 .name</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(soup.name)</span><br><span class="line">print(soup.head.name)</span><br><span class="line"># [document]</span><br><span class="line"># head</span><br></pre></td></tr></table></figure>

<h4 id="4-4-Comment"><a href="#4-4-Comment" class="headerlink" title="4.4 Comment"></a>4.4 Comment</h4><blockquote>
<p>Comment 对象是一个特殊类型的 NavigableString 对象，其实输出的内容仍然不包括注释符号，但是如果不好好处理它，可能会对我们的文本处理造成意想不到的麻烦</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if type(soup.strong.string)&#x3D;&#x3D;Comment:</span><br><span class="line">    print(soup.strong.prettify())</span><br><span class="line">else:</span><br><span class="line">    print(soup.strong.string)</span><br></pre></td></tr></table></figure>

<h3 id="5-搜索文档树"><a href="#5-搜索文档树" class="headerlink" title="5 搜索文档树"></a>5 搜索文档树</h3><blockquote>
<p>Beautiful Soup定义了很多搜索方法,这里着重介绍2个: find() 和 find_all() .其它方法的参数和用法类似,请同学们举一反三</p>
</blockquote>
<h4 id="5-1-过滤器"><a href="#5-1-过滤器" class="headerlink" title="5.1 过滤器"></a>5.1 过滤器</h4><blockquote>
<p>介绍 find_all() 方法前,先介绍一下过滤器的类型 ,这些过滤器贯穿整个搜索的API.过滤器可以被用在tag的name中,节点的属性中,字符串中或他们的混合中</p>
</blockquote>
<h5 id="5-1-1-字符串"><a href="#5-1-1-字符串" class="headerlink" title="5.1.1 字符串"></a>5.1.1 字符串</h5><blockquote>
<p>最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的<div>标签</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#返回所有的div标签</span><br><span class="line">print(soup.find_all(&#39;div&#39;))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果传入字节码参数,Beautiful Soup会当作UTF-8编码,可以传入一段Unicode 编码来避免Beautiful Soup解析编码出错</p>
</blockquote>
<h5 id="5-1-2-正则表达式"><a href="#5-1-2-正则表达式" class="headerlink" title="5.1.2 正则表达式"></a>5.1.2 正则表达式</h5><p>如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 match() 来匹配内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#返回所有的div标签</span><br><span class="line">print (soup.find_all(re.compile(&quot;^div&quot;)))</span><br></pre></td></tr></table></figure>
<h5 id="5-1-3-列表"><a href="#5-1-3-列表" class="headerlink" title="5.1.3 列表"></a>5.1.3 列表</h5><blockquote>
<p>如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#返回所有匹配到的span a标签</span><br><span class="line">print(soup.find_all([&#39;span&#39;,&#39;a&#39;]))</span><br></pre></td></tr></table></figure>
<h5 id="5-1-4-keyword"><a href="#5-1-4-keyword" class="headerlink" title="5.1.4 keyword"></a>5.1.4 keyword</h5><blockquote>
<p>如果一个指定名字的参数不是搜索内置的参数名,搜索时会把该参数当作指定名字tag的属性来搜索,如果包含一个名字为 id 的参数,Beautiful Soup会搜索每个tag的”id”属性</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#返回id为welcom的标签</span><br><span class="line">print(soup.find_all(id&#x3D;&#39;welcom&#39;))</span><br></pre></td></tr></table></figure>
<h5 id="5-1-4-True"><a href="#5-1-4-True" class="headerlink" title="5.1.4 True"></a>5.1.4 True</h5><blockquote>
<p>True 可以匹配任何值,下面代码查找到所有的tag,但是不会返回字符串节点</p>
</blockquote>
<h5 id="5-1-5-按CSS搜索"><a href="#5-1-5-按CSS搜索" class="headerlink" title="5.1.5 按CSS搜索"></a>5.1.5 按CSS搜索</h5><blockquote>
<p>按照CSS类名搜索tag的功能非常实用,但标识CSS类名的关键字 class 在Python中是保留字,使用 class 做参数会导致语法错误.从Beautiful Soup的4.1.1版本开始,可以通过 class_ 参数搜索有指定CSS类名的tag</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 返回class等于info的div</span><br><span class="line">print(soup.find_all(&#39;div&#39;,class_&#x3D;&#39;info&#39;))</span><br></pre></td></tr></table></figure>
<h4 id="5-1-6-按属性的搜索"><a href="#5-1-6-按属性的搜索" class="headerlink" title="5.1.6 按属性的搜索"></a>5.1.6 按属性的搜索</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">soup.find_all(&quot;div&quot;, attrs&#x3D;&#123;&quot;class&quot;: &quot;info&quot;&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="6-CSS选择器（扩展）"><a href="#6-CSS选择器（扩展）" class="headerlink" title="6. CSS选择器（扩展）"></a>6. CSS选择器（扩展）</h3><p>soup.select(参数)</p>
<table>
<thead>
<tr>
<th>表达式</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>tag</td>
<td>选择指定标签</td>
</tr>
<tr>
<td>*</td>
<td>选择所有节点</td>
</tr>
<tr>
<td>#id</td>
<td>选择id为container的节点</td>
</tr>
<tr>
<td>.class</td>
<td>选取所有class包含container的节点</td>
</tr>
<tr>
<td>li a</td>
<td>选取所有li下的所有a节点</td>
</tr>
<tr>
<td>ul + p</td>
<td>(兄弟)选择ul后面的第一个p元素</td>
</tr>
<tr>
<td>div#id &gt; ul</td>
<td>(父子)选取id为id的div的第一个ul子元素</td>
</tr>
<tr>
<td>table ~ div</td>
<td>选取与table相邻的所有div元素</td>
</tr>
<tr>
<td>a[title]</td>
<td>选取所有有title属性的a元素</td>
</tr>
<tr>
<td>a[class=”title”]</td>
<td>选取所有class属性为title值的a</td>
</tr>
<tr>
<td>a[href*=”sxt”]</td>
<td>选取所有href属性包含sxt的a元素</td>
</tr>
<tr>
<td>a[href^=”http”]</td>
<td>选取所有href属性值以http开头的a元素</td>
</tr>
<tr>
<td>a[href$=”.png”]</td>
<td>选取所有href属性值以.png结尾的a元素</td>
</tr>
<tr>
<td>input[type=”redio”]:checked</td>
<td>选取选中的hobby的元素</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>07. 数据提取-正则表达式</title>
    <url>/2020/01/07/python%E7%88%AC%E8%99%AB/07.%20%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    <content><![CDATA[<h3 id="1-提取数据"><a href="#1-提取数据" class="headerlink" title="1. 提取数据"></a>1. 提取数据</h3><p>在前面我们已经搞定了怎样获取页面的内容，不过还差一步，这么多杂乱的代码夹杂文字我们怎样把它提取出来整理呢？下面就开始介绍一个十分强大的工具，正则表达式！</p>
<blockquote>
<p>正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。</p>
</blockquote>
<p>正则表达式是用来匹配字符串非常强大的工具，在其他编程语言中同样有正则表达式的概念，Python同样不例外，利用了正则表达式，我们想要从返回的页面内容提取出我们想要的内容就易如反掌了</p>
<p><strong>规则</strong>：</p>
<table>
<thead>
<tr>
<th>模式</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>^</td>
<td>匹配字符串的开头</td>
</tr>
<tr>
<td>$</td>
<td>匹配字符串的末尾</td>
</tr>
<tr>
<td>.</td>
<td>匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符</td>
</tr>
<tr>
<td>[…]</td>
<td>用来表示一组字符,单独列出：[amk] 匹配 ‘a’，’m’或’k’</td>
</tr>
<tr>
<td>[^…]</td>
<td>不在[]中的字符：[^abc] 匹配除了a,b,c之外的字符</td>
</tr>
<tr>
<td>re*</td>
<td>匹配0个或多个的表达式</td>
</tr>
<tr>
<td>re+</td>
<td>匹配1个或多个的表达式</td>
</tr>
<tr>
<td>re?</td>
<td>匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式</td>
</tr>
<tr>
<td>re{ n}</td>
<td></td>
</tr>
<tr>
<td>re{ n,}</td>
<td>精确匹配n个前面表达式</td>
</tr>
<tr>
<td>re{ n, m}</td>
<td>匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式</td>
</tr>
<tr>
<td>a</td>
<td>b</td>
</tr>
<tr>
<td>(re)</td>
<td>G匹配括号内的表达式，也表示一个组</td>
</tr>
<tr>
<td>(?imx)</td>
<td>正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域</td>
</tr>
<tr>
<td>(?-imx)</td>
<td>正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域</td>
</tr>
<tr>
<td>(?: re)</td>
<td>类似 (…), 但是不表示一个组</td>
</tr>
<tr>
<td>(?imx: re)</td>
<td>在括号中使用i, m, 或 x 可选标志</td>
</tr>
<tr>
<td>(?-imx: re)</td>
<td>在括号中不使用i, m, 或 x 可选标志</td>
</tr>
<tr>
<td>(?#…)</td>
<td>注释</td>
</tr>
<tr>
<td>(?= re)</td>
<td>前向肯定界定符。如果所含正则表达式，以 … 表示，在当前位置成功匹配时成功，否则失败。但一旦所含表达式已经尝试，匹配引擎根本没有提高；模式的剩余部分还要尝试界定符的右边。</td>
</tr>
<tr>
<td>(?! re)</td>
<td>前向否定界定符。与肯定界定符相反；当所含表达式不能在字符串当前位置匹配时成功</td>
</tr>
<tr>
<td>(?&gt; re)</td>
<td>匹配的独立模式，省去回溯</td>
</tr>
<tr>
<td>\w</td>
<td>匹配字母数字及下划线</td>
</tr>
<tr>
<td>\W</td>
<td>匹配非字母数字及下划线</td>
</tr>
<tr>
<td>\s</td>
<td>匹配任意空白字符，等价于 [\t\n\r\f].</td>
</tr>
<tr>
<td>\S</td>
<td>匹配任意非空字符</td>
</tr>
<tr>
<td>\d</td>
<td>匹配任意数字，等价于 [0-9]</td>
</tr>
<tr>
<td>\D</td>
<td>匹配任意非数字</td>
</tr>
<tr>
<td>\A</td>
<td>匹配字符串开始</td>
</tr>
<tr>
<td>\Z</td>
<td>匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串。c</td>
</tr>
<tr>
<td>\z</td>
<td>匹配字符串结束</td>
</tr>
<tr>
<td>\G</td>
<td>匹配最后匹配完成的位置</td>
</tr>
<tr>
<td>\b</td>
<td>匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’</td>
</tr>
<tr>
<td>\B</td>
<td>匹配非单词边界。’er\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’</td>
</tr>
<tr>
<td>\n, \t, 等.</td>
<td>匹配一个换行符。匹配一个制表符。等</td>
</tr>
<tr>
<td>\1…\9</td>
<td>匹配第n个分组的内容</td>
</tr>
<tr>
<td>\10</td>
<td>匹配第n个分组的内容，如果它经匹配。否则指的是八进制字符码的表达式</td>
</tr>
<tr>
<td>[\u4e00-\u9fa5]</td>
<td>中文</td>
</tr>
</tbody></table>
<h3 id="2-正则表达式相关注解"><a href="#2-正则表达式相关注解" class="headerlink" title="2. 正则表达式相关注解"></a>2. 正则表达式相关注解</h3><h4 id="2-1-数量词的贪婪模式与非贪婪模式"><a href="#2-1-数量词的贪婪模式与非贪婪模式" class="headerlink" title="2.1 数量词的贪婪模式与非贪婪模式"></a>2.1 数量词的贪婪模式与非贪婪模式</h4><p>正则表达式通常用于在文本中查找匹配的字符串<br>Python里数量词默认是贪婪的（在少数语言里也可能是默认非贪婪），总是尝试匹配尽可能多的字符；非贪婪的则相反，总是尝试匹配尽可能少的字符</p>
<p>例如：正则表达式”ab<em>”如果用于查找”abbbc”，将找到”abbb”。而如果使用非贪婪的数量词”ab</em>?”，将找到”a”</p>
<h4 id="2-2-常用方法"><a href="#2-2-常用方法" class="headerlink" title="2.2 常用方法"></a>2.2 常用方法</h4><ul>
<li>re.match<ul>
<li>re.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none</li>
<li>函数语法：<br>re.match(pattern, string, flags=0)</li>
</ul>
</li>
<li>re.search<ul>
<li>re.search 扫描整个字符串并返回第一个成功的匹配。</li>
<li>函数语法：<br>re.search(pattern, string, flags=0)</li>
</ul>
</li>
<li>re.sub<ul>
<li>re.sub 替换字符串<br>re.sub(pattern,replace,string)</li>
</ul>
</li>
<li>re.findall<ul>
<li>re.findall 查找全部<br>re.findall(pattern,string,flags=0)</li>
</ul>
</li>
</ul>
<h3 id="3-正则表达式修饰符-可选标志"><a href="#3-正则表达式修饰符-可选标志" class="headerlink" title="3. 正则表达式修饰符 - 可选标志"></a>3. 正则表达式修饰符 - 可选标志</h3><blockquote>
<p>正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。多个标志可以通过按位 OR(|) 它们来指定。如 re.I | re.M 被设置成 I 和 M 标志：</p>
</blockquote>
<table>
<thead>
<tr>
<th>修饰符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>re.I</td>
<td>使匹配对大小写不敏感</td>
</tr>
<tr>
<td>re.L</td>
<td>做本地化识别（locale-aware）匹配</td>
</tr>
<tr>
<td>re.M</td>
<td></td>
</tr>
<tr>
<td>re.S</td>
<td>使 . 匹配包括换行在内的所有字符</td>
</tr>
<tr>
<td>re.U</td>
<td>根据Unicode字符集解析字符。这个标志影响 \w, \W, \b, \B</td>
</tr>
<tr>
<td>re.X</td>
<td>该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>06. Requests库的用法</title>
    <url>/2020/01/06/python%E7%88%AC%E8%99%AB/06.%20Requests%E5%BA%93%E7%9A%84%E7%94%A8%E6%B3%95/</url>
    <content><![CDATA[<h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h3><blockquote>
<p>对了解一些爬虫的基本理念，掌握爬虫爬取的流程有所帮助。入门之后，我们就需要学习一些更加高级的内容和工具来方便我们的爬取。那么这一节来简单介绍一下 requests 库的基本用法</p>
</blockquote>
<h3 id="2-安装"><a href="#2-安装" class="headerlink" title="2. 安装"></a>2. 安装</h3><p>利用 pip 安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install requests</span><br></pre></td></tr></table></figure>

<h3 id="3-基本请求"><a href="#3-基本请求" class="headerlink" title="3. 基本请求"></a>3. 基本请求</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">req &#x3D; requests.get(&quot;http:&#x2F;&#x2F;www.baidu.com&quot;)</span><br><span class="line">req &#x3D; requests.post(&quot;http:&#x2F;&#x2F;www.baidu.com&quot;)</span><br><span class="line">req &#x3D; requests.put(&quot;http:&#x2F;&#x2F;www.baidu.com&quot;)</span><br><span class="line">req &#x3D; requests.delete(&quot;http:&#x2F;&#x2F;www.baidu.com&quot;)</span><br><span class="line">req &#x3D; requests.head(&quot;http:&#x2F;&#x2F;www.baidu.com&quot;)</span><br><span class="line">req &#x3D; requests.options(&quot;http:&#x2F;&#x2F;www.baidu.com&quot;)</span><br></pre></td></tr></table></figure>

<h4 id="3-1-get请求"><a href="#3-1-get请求" class="headerlink" title="3.1 get请求"></a>3.1 get请求</h4><p>参数是字典，我们也可以传递json类型的参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url &#x3D; &quot;http:&#x2F;&#x2F;www.baidu.com&#x2F;s&quot;</span><br><span class="line">params &#x3D; &#123;&#39;wd&#39;: &#39;尚学堂&#39;&#125;</span><br><span class="line">response &#x3D; requests.get(url, params&#x3D;params)</span><br><span class="line">print(response.url)</span><br><span class="line">response.encoding &#x3D; &#39;utf-8&#39;</span><br><span class="line">html &#x3D; response.text</span><br><span class="line"># print(html)</span><br></pre></td></tr></table></figure>

<h4 id="3-2-post请求"><a href="#3-2-post请求" class="headerlink" title="3.2 post请求"></a>3.2 post请求</h4><p>参数是字典，我们也可以传递json类型的参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">url &#x3D; &quot;http:&#x2F;&#x2F;www.sxt.cn&#x2F;index&#x2F;login&#x2F;login.html&quot;</span><br><span class="line">formdata &#x3D; &#123;</span><br><span class="line">    &quot;user&quot;: &quot;17703181473&quot;,</span><br><span class="line">    &quot;password&quot;: &quot;123456&quot;</span><br><span class="line">&#125;</span><br><span class="line">response &#x3D; requests.post(url, data&#x3D;formdata)</span><br><span class="line">response.encoding &#x3D; &#39;utf-8&#39;</span><br><span class="line">html &#x3D; response.text</span><br><span class="line"># print(html)</span><br></pre></td></tr></table></figure>
<h4 id="3-3-自定义请求头部"><a href="#3-3-自定义请求头部" class="headerlink" title="3.3 自定义请求头部"></a>3.3 自定义请求头部</h4><blockquote>
<p>伪装请求头部是采集时经常用的，我们可以用这个方法来隐藏：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">headers &#x3D; &#123;&#39;User-Agent&#39;: &#39;python&#39;&#125;</span><br><span class="line">r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;www.zhidaow.com&#39;, headers &#x3D; headers)</span><br><span class="line">print(r.request.headers[&#39;User-Agent&#39;])</span><br></pre></td></tr></table></figure>

<h4 id="3-4-设置超时时间"><a href="#3-4-设置超时时间" class="headerlink" title="3.4 设置超时时间"></a>3.4 设置超时时间</h4><blockquote>
<p>可以通过timeout属性设置超时时间，一旦超过这个时间还没获得响应内容，就会提示错误</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">requests.get(&#39;http:&#x2F;&#x2F;github.com&#39;, timeout&#x3D;0.001)</span><br></pre></td></tr></table></figure>
<h4 id="3-5-代理访问"><a href="#3-5-代理访问" class="headerlink" title="3.5 代理访问"></a>3.5 代理访问</h4><blockquote>
<p>采集时为避免被封IP，经常会使用代理。requests也有相应的proxies属性</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">proxies &#x3D; &#123;</span><br><span class="line">  &quot;http&quot;: &quot;http:&#x2F;&#x2F;10.10.1.10:3128&quot;,</span><br><span class="line">  &quot;https&quot;: &quot;https:&#x2F;&#x2F;10.10.1.10:1080&quot;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">requests.get(&quot;http:&#x2F;&#x2F;www.zhidaow.com&quot;, proxies&#x3D;proxies)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果代理需要账户和密码，则需这样</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">proxies &#x3D; &#123;</span><br><span class="line">    &quot;http&quot;: &quot;http:&#x2F;&#x2F;user:pass@10.10.1.10:3128&#x2F;&quot;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-6-session自动保存cookies"><a href="#3-6-session自动保存cookies" class="headerlink" title="3.6 session自动保存cookies"></a>3.6 session自动保存cookies</h4><blockquote>
<p>seesion的意思是保持一个会话，比如 登陆后继续操作(记录身份信息) 而requests是单次请求的请求，身份信息不会被记录</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建一个session对象 </span><br><span class="line">s &#x3D; requests.Session() </span><br><span class="line"># 用session对象发出get请求，设置cookies </span><br><span class="line">s.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;cookies&#x2F;set&#x2F;sessioncookie&#x2F;123456789&#39;)</span><br></pre></td></tr></table></figure>

<h4 id="3-7-ssl验证"><a href="#3-7-ssl验证" class="headerlink" title="3.7 ssl验证"></a>3.7 ssl验证</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 禁用安全请求警告</span><br><span class="line">requests.packages.urllib3.disable_warnings()</span><br><span class="line"></span><br><span class="line">resp &#x3D; requests.get(url, verify&#x3D;False, headers&#x3D;headers)</span><br></pre></td></tr></table></figure>
<h4 id="4-获取响应信息"><a href="#4-获取响应信息" class="headerlink" title="4 获取响应信息"></a>4 获取响应信息</h4><table>
<thead>
<tr>
<th>代码</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>resp.json()</td>
<td>获取响应内容（以json字符串）</td>
</tr>
<tr>
<td>resp.text</td>
<td>获取响应内容 (以字符串)</td>
</tr>
<tr>
<td>resp.content</td>
<td>获取响应内容（以字节的方式）</td>
</tr>
<tr>
<td>resp.headers</td>
<td>获取响应头内容</td>
</tr>
<tr>
<td>resp.url</td>
<td>获取访问地址</td>
</tr>
<tr>
<td>resp.encoding</td>
<td>获取网页编码</td>
</tr>
<tr>
<td>resp.request.headers</td>
<td>请求头内容</td>
</tr>
<tr>
<td>resp.cookie</td>
<td>获取cookie</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>05. URLError与Cookie</title>
    <url>/2020/01/05/python%E7%88%AC%E8%99%AB/05.%20URLError%E4%B8%8ECookie/</url>
    <content><![CDATA[<h3 id="1-Cookie"><a href="#1-Cookie" class="headerlink" title="1. Cookie"></a>1. Cookie</h3><p>为什么要使用Cookie呢？</p>
<p>Cookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密）</p>
<p>比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容是不允许的。那么我们可以利用Urllib库保存我们登录的Cookie，然后再抓取其他页面就达到目的了。</p>
<h4 id="1-1-Opener"><a href="#1-1-Opener" class="headerlink" title="1.1 Opener"></a>1.1 Opener</h4><p>当你获取一个URL你使用一个opener(一个urllib.OpenerDirector的实例)。在前面，我们都是使用的默认的opener，也就是urlopen。它是一个特殊的opener，可以理解成opener的一个特殊实例，传入的参数仅仅是url，data，timeout。</p>
<p>如果我们需要用到Cookie，只用这个opener是不能达到目的的，所以我们需要创建更一般的opener来实现对Cookie的设置</p>
<h4 id="1-2-Cookielib"><a href="#1-2-Cookielib" class="headerlink" title="1.2 Cookielib"></a>1.2 Cookielib</h4><p>cookielib模块的主要作用是提供可存储cookie的对象，以便于与urllib模块配合使用来访问Internet资源。Cookielib模块非常强大，我们可以利用本模块的CookieJar类的对象来捕获cookie并在后续连接请求时重新发送，比如可以实现模拟登录功能。该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar</p>
<h5 id="案例1：获取Cookie保存到变量"><a href="#案例1：获取Cookie保存到变量" class="headerlink" title="案例1：获取Cookie保存到变量"></a>案例1：获取Cookie保存到变量</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from urllib.request import HTTPCookieProcessor</span><br><span class="line">from urllib.request import build_opener</span><br><span class="line">from urllib.request import Request</span><br><span class="line">from http.cookiejar import CookieJar</span><br><span class="line">from urllib.parse import urlencode</span><br><span class="line">#声明一个CookieJar对象实例来保存cookie</span><br><span class="line">cookie &#x3D; CookieJar()</span><br><span class="line">#利用HTTPCookieProcessor对象来创建cookie处理器</span><br><span class="line">cookiePro &#x3D; HTTPCookieProcessor(cookie)</span><br><span class="line">#通过handler来构建opener</span><br><span class="line">opener &#x3D; build_opener(cookiePro)</span><br><span class="line">login_url &#x3D; &quot;http:&#x2F;&#x2F;www.sxt.cn&#x2F;index&#x2F;login&#x2F;login&quot;</span><br><span class="line">header &#x3D; &#123;&quot;User-Agent&quot;: &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;61.0.3163.79 Safari&#x2F;537.36&quot;&#125;</span><br><span class="line">fromdata &#x3D; &#123;</span><br><span class="line">    &quot;user&quot;: &quot;17703181473&quot;,</span><br><span class="line">    &quot;password&quot;: &quot;123456&quot;</span><br><span class="line">&#125;</span><br><span class="line">data &#x3D; urlencode(fromdata).encode()</span><br><span class="line">request &#x3D; Request(login_url, headers&#x3D;header, data&#x3D;data)</span><br><span class="line">response &#x3D; opener.open(request)</span><br><span class="line">info_url &#x3D; &#39;http:&#x2F;&#x2F;www.sxt.cn&#x2F;index&#x2F;user.html&#39;</span><br><span class="line">request_info &#x3D; Request(info_url)</span><br><span class="line">response &#x3D; opener.open(request_info)</span><br><span class="line">html &#x3D; response.read()</span><br><span class="line">print(html.decode())</span><br></pre></td></tr></table></figure>
<p>我们使用以上方法将cookie保存到变量中，然后打印出了cookie中的值，运行结果如下</p>
<p>以上程序的原理如下</p>
<p>创建一个带有cookie的opener，在访问登录的URL时，将登录后的cookie保存下来，然后利用这个cookie来访问其他网址。</p>
<h5 id="案例2：cookie保存文件的读取"><a href="#案例2：cookie保存文件的读取" class="headerlink" title="案例2：cookie保存文件的读取"></a>案例2：cookie保存文件的读取</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from urllib.request import build_opener, Request</span><br><span class="line">from urllib.request import HTTPCookieProcessor</span><br><span class="line">from http.cookiejar import MozillaCookieJar</span><br><span class="line">from urllib.parse import urlencode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_cookie():</span><br><span class="line">    # 请求头</span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &quot;User-Agent&quot;: &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;65.0.3325.181 Safari&#x2F;537.36&quot;&#125;</span><br><span class="line">    login_url &#x3D; &quot;http:&#x2F;&#x2F;www.sxt.cn&#x2F;index&#x2F;login&#x2F;login.html&quot;</span><br><span class="line">    form_data &#x3D; &#123;</span><br><span class="line">        &quot;user&quot;: &quot;17703181473&quot;,</span><br><span class="line">        &quot;password&quot;: &quot;123456&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    # 转换编码</span><br><span class="line">    f_data &#x3D; urlencode(form_data)</span><br><span class="line">    req &#x3D; Request(login_url, headers&#x3D;headers, data&#x3D;f_data.encode())</span><br><span class="line">    # 创建保存可以序列化cookie的文件对象</span><br><span class="line">    cookie &#x3D; MozillaCookieJar(&quot;cookie.txt&quot;)</span><br><span class="line">    # 构造可保存cookie的控制器</span><br><span class="line">    c_handler &#x3D; HTTPCookieProcessor(cookie)</span><br><span class="line">    # 构造opener</span><br><span class="line">    opener &#x3D; build_opener(c_handler)</span><br><span class="line">    # 发送请求 -- 登录成功 （用户名和密码 正确）</span><br><span class="line">    opener.open(req)</span><br><span class="line">    cookie.save(ignore_discard&#x3D;True, ignore_expires&#x3D;True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def use_cookie():</span><br><span class="line">    # 请求头</span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &quot;User-Agent&quot;: &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;65.0.3325.181 Safari&#x2F;537.36&quot;&#125;</span><br><span class="line"></span><br><span class="line">    info_url &#x3D; &quot;http:&#x2F;&#x2F;www.sxt.cn&#x2F;index&#x2F;user.html&quot;</span><br><span class="line">    # 创建保存可以序列化cookie的文件对象</span><br><span class="line">    cookie &#x3D; MozillaCookieJar()</span><br><span class="line">    # 加载cookie文件</span><br><span class="line">    cookie.load(&quot;cookie.txt&quot;, ignore_discard&#x3D;True, ignore_expires&#x3D;True)</span><br><span class="line">    # 构造可保存cookie的控制器</span><br><span class="line">    c_handler &#x3D; HTTPCookieProcessor(cookie)</span><br><span class="line">    # 构造opener</span><br><span class="line">    opener &#x3D; build_opener(c_handler)</span><br><span class="line">    # 构造访问个人页面请求</span><br><span class="line">    req1 &#x3D; Request(info_url, headers&#x3D;headers)</span><br><span class="line">    # 发送请求</span><br><span class="line">    resp2 &#x3D; opener.open(req1)</span><br><span class="line">    # 打印信息</span><br><span class="line">    print(resp2.read().decode())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    # get_cookie()</span><br><span class="line">    use_cookie()</span><br></pre></td></tr></table></figure>
<h3 id="2-URLError"><a href="#2-URLError" class="headerlink" title="2. URLError"></a>2. URLError</h3><p>首先解释下URLError可能产生的原因：</p>
<ul>
<li>网络无连接，即本机无法上网</li>
<li>连接不到特定的服务器</li>
<li>服务器不存在</li>
</ul>
<p>在代码中，我们需要用try-except语句来包围并捕获相应的异常,代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from urllib.request import Request, urlopen</span><br><span class="line">from urllib.error import URLError</span><br><span class="line"></span><br><span class="line">url &#x3D; &quot;http:&#x2F;&#x2F;www.sx435334t.cn&#x2F;index&#x2F;us3er.html&quot;</span><br><span class="line">try:</span><br><span class="line">    headers &#x3D; &#123;</span><br><span class="line">        &quot;User-Agent&quot;: &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;65.0.3325.181 Safari&#x2F;537.36&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    req &#x3D; Request(url, headers&#x3D;headers)</span><br><span class="line"></span><br><span class="line">    resp &#x3D; urlopen(url, timeout&#x3D;1)</span><br><span class="line"></span><br><span class="line">    print(resp.read().decode())</span><br><span class="line">except URLError as e:</span><br><span class="line">    if len(e.args) &#x3D;&#x3D; 0:</span><br><span class="line">        print(e.code)</span><br><span class="line">    else:</span><br><span class="line">        print(e.args[0])</span><br><span class="line"></span><br><span class="line">print(&quot;获取数据完毕&quot;)</span><br></pre></td></tr></table></figure>
<p>我们利用了 urlopen方法访问了一个不存在的网址，运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[Errno 11004] getaddrinfo failed</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>04. urllib库的高级用法</title>
    <url>/2020/01/04/python%E7%88%AC%E8%99%AB/04.%20urllib%E5%BA%93%E7%9A%84%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/</url>
    <content><![CDATA[<h3 id="1-伪装自己"><a href="#1-伪装自己" class="headerlink" title="1. 伪装自己"></a>1. 伪装自己</h3><p>有些网站不会同意程序直接用上面的方式进行访问，如果识别有问题，那么站点根本不会响应，所以为了完全模拟浏览器的工作</p>
<hr>
<h4 id="1-1-设置请求头"><a href="#1-1-设置请求头" class="headerlink" title="1.1 设置请求头"></a>1.1 设置请求头</h4><p>其中<code>User-Agent</code>代表用的哪个请求的浏览器</p>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line">from urllib.request import Request</span><br><span class="line"></span><br><span class="line">url &#x3D; &#39;http:&#x2F;&#x2F;www.server.com&#x2F;login&#39;</span><br><span class="line">user_agent &#x3D; &#39;Mozilla&#x2F;4.0 (compatible; MSIE 5.5; Windows NT)&#39; </span><br><span class="line">headers &#x3D; &#123; &#39;User-Agent&#39; : user_agent &#125;  </span><br><span class="line"></span><br><span class="line">request &#x3D; Request(url, headers&#x3D;headers)  </span><br><span class="line">response &#x3D; urlopen(request)  </span><br><span class="line">page &#x3D; response.read()</span><br></pre></td></tr></table></figure>

<p>对付防盗链，服务器会识别headers中的referer是不是它自己，如果不是，有的服务器不会响应，所以我们还可以在headers中加入referer</p>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">headers &#x3D; &#123; </span><br><span class="line">         &#39;User-Agent&#39; : &#39;Mozilla&#x2F;4.0 (compatible; MSIE 5.5; Windows NT)&#39;,</span><br><span class="line">         &#39;Referer&#39;:&#39;http:&#x2F;&#x2F;www.zhihu.com&#x2F;articles&#39; </span><br><span class="line">          &#125; </span><br><span class="line">&#96;&#96;&#96;       </span><br><span class="line">**提示**</span><br><span class="line">&gt;在此可以使用多个User_Agent:然后随即选择</span><br></pre></td></tr></table></figure>
<p>import urllib.request<br>import random<br>ua_list = [<br>    “Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0)”,<br>    “Mozilla/5.0 (Windows; U; Windows NT 5.2) Gecko/2008070208 Firefox/3.0.1”,<br>    “Mozilla/5.0 (Windows; U; Windows NT 5.2) AppleWebKit/525.13 (KHTML, like Gecko) Version/3.1”,<br>    “Mozilla/5.0 (Windows; U; Windows NT 5.2) AppleWebKit/525.13 (KHTML, like Gecko) Chrome/0.2.149.27”,<br>    “Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1) ;  QIHU 360EE)”<br>]<br>user_agent = random.choice(ua_list)<br>request = urllib.request.Request(“<a href="http://www.baidu.com&quot;" target="_blank" rel="noopener">http://www.baidu.com&quot;</a>)<br>request.add_header(“User-Agent”,user_agent)<br>#区分大小写<br>print(request.get_header(“User-agent”))</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 1.2 设置代理Proxy</span><br><span class="line"></span><br><span class="line">&gt; 假如一个网站它会检测某一段时间某个IP 的访问次数，如果访问次数过多，它会禁止你的访问。所以你可以设置一些代理服务器来帮助你做工作，每隔一段时间换一个代理，网站君都不知道是谁在捣鬼了，这酸爽！</span><br><span class="line"></span><br><span class="line">##### 分类：</span><br><span class="line">透明代理：目标网站知道你使用了代理并且知道你的源IP地址，这种代理显然不符合我们这里使用代理的初衷</span><br><span class="line"></span><br><span class="line">匿名代理：匿名程度比较低，也就是网站知道你使用了代理，但是并不知道你的源IP地址</span><br><span class="line"></span><br><span class="line">高匿代理：这是最保险的方式，目标网站既不知道你使用的代理更不知道你的源IP </span><br><span class="line"></span><br><span class="line">代码如下：</span><br></pre></td></tr></table></figure>
<p>from urllib.request import ProxyHandler<br>from urllib.request import build_opener</p>
<p>proxy = ProxyHandler({“http”: “119.109.197.195:80”})<br>opener = build_opener(proxy)<br>url = “<a href="http://www.baidu.com&quot;" target="_blank" rel="noopener">http://www.baidu.com&quot;</a><br>response = opener.open(url)<br>print(response.read().decode(“utf-8”))</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">### 2 使用DebugLog</span><br><span class="line">可以通过下面的方法把 Debug Log 打开，这样收发包的内容就会在屏幕上打印出来，方便调试，这个也不太常用，仅提一下</span><br></pre></td></tr></table></figure>
<p>from urllib.request import HTTPHandler<br>from urllib.request import build_opener<br>from urllib.request import Request</p>
<p>handler = HTTPHandler(debuglevel=1)<br>opener = build_opener(handler)<br>url = “<a href="http://www.sohu.com&quot;" target="_blank" rel="noopener">http://www.sohu.com&quot;</a><br>request = Request(url)<br>response = opener.open(request)<br>```</p>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>03. 爬取数据-urllib库</title>
    <url>/2020/01/03/python%E7%88%AC%E8%99%AB/03.%20%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE-urllib%E5%BA%93/</url>
    <content><![CDATA[<h3 id="1-小试牛刀"><a href="#1-小试牛刀" class="headerlink" title="1. 小试牛刀"></a>1. 小试牛刀</h3><p>怎样扒网页呢？</p>
<p>其实就是根据URL来获取它的网页信息，虽然我们在浏览器中看到的是一幅幅优美的画面，但是其实是由浏览器解释才呈现出来的，实质它是一段HTML代码，加 JS、CSS，如果把网页比作一个人，那么HTML便是他的骨架，JS便是他的肌肉，CSS便是它的衣服。所以最重要的部分是存在于HTML中的，下面我们就写个例子来扒一个网页下来</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line"> </span><br><span class="line">response &#x3D; urlopen(&quot;http:&#x2F;&#x2F;www.baidu.com&quot;)</span><br><span class="line">print(response.read().decode())</span><br></pre></td></tr></table></figure>
<p>真正的程序就两行，执行如下命令查看运行结果，感受一下</p>
<p>看，这个网页的源码已经被我们扒下来了，是不是很酸爽？</p>
<hr>
<h3 id="2-常见到的方法"><a href="#2-常见到的方法" class="headerlink" title="2. 常见到的方法"></a>2. 常见到的方法</h3><ul>
<li><p>requset.urlopen(url,data,timeout)</p>
<ul>
<li><p>第一个参数url即为URL，第二个参数data是访问URL时要传送的数据，第三个timeout是设置超时时间。</p>
</li>
<li><p>第二三个参数是可以不传送的，data默认为空None，timeout默认为 socket._GLOBAL_DEFAULT_TIMEOUT</p>
</li>
<li><p>第一个参数URL是必须要传送的，在这个例子里面我们传送了百度的URL，执行urlopen方法之后，返回一个response对象，返回信息便保存在这里面。</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>response.read()</p>
<ul>
<li>read()方法就是读取文件里的全部内容，返回bytes类型</li>
</ul>
</li>
<li><p>response.getcode()</p>
<ul>
<li>返回 HTTP的响应码，成功返回200，4服务器页面出错，5服务器问题</li>
</ul>
</li>
<li><p>response.geturl()</p>
<ul>
<li>返回 返回实际数据的实际URL，防止重定向问题</li>
</ul>
</li>
<li><p>response.info()</p>
<ul>
<li>返回 服务器响应的HTTP报头</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-Request对象"><a href="#3-Request对象" class="headerlink" title="3. Request对象"></a>3. Request对象</h3><p> 其实上面的urlopen参数可以传入一个request请求,它其实就是一个Request类的实例，构造时需要传入Url,Data等等的内容。比如上面的两行代码，我们可以这么改写</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">from urllib.request import urlopen</span><br><span class="line">from urllib.request import Request</span><br><span class="line"></span><br><span class="line">request &#x3D; Request(&quot;http:&#x2F;&#x2F;www.baidu.com&quot;)</span><br><span class="line">response &#x3D; urlopen(requst)</span><br><span class="line">print response.read().decode()</span><br></pre></td></tr></table></figure>

<p>运行结果是完全一样的，只不过中间多了一个request对象，推荐大家这么写，因为在构建请求时还需要加入好多内容，通过构建一个request，服务器响应请求得到应答，这样显得逻辑上清晰明确</p>
<hr>
<h3 id="4-Get-请求"><a href="#4-Get-请求" class="headerlink" title="4. Get 请求"></a>4. Get 请求</h3><p>大部分被传输到浏览器的html，images，js，css, … 都是通过GET方法发出请求的。它是获取数据的主要方法</p>
<p>例如：<a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a> 搜索</p>
<p>Get请求的参数都是在Url中体现的,如果有中文，需要转码，这时我们可使用</p>
<ul>
<li><p>urllib.parse.urlencode()</p>
</li>
<li><p>urllib.parse. quote()</p>
</li>
</ul>
<h3 id="5-Post-请求"><a href="#5-Post-请求" class="headerlink" title="5. Post 请求"></a>5. Post 请求</h3><p>我们说了Request请求对象的里有data参数，它就是用在POST里的，我们要传送的数据就是这个参数data，data是一个字典，里面要匹配键值对</p>
<p>发送请求/响应header头的含义：</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>Accept</td>
<td>告诉服务器，客户端支持的数据类型</td>
</tr>
<tr>
<td>Accept-Charset</td>
<td>告诉服务器，客户端采用的编码</td>
</tr>
<tr>
<td>Accept-Encoding</td>
<td>告诉服务器，客户机支持的数据压缩格式</td>
</tr>
<tr>
<td>Accept-Language</td>
<td>告诉服务器，客户机的语言环境</td>
</tr>
<tr>
<td>Host</td>
<td>客户机通过这个头告诉服务器，想访问的主机名</td>
</tr>
<tr>
<td>If-Modified-Since</td>
<td>客户机通过这个头告诉服务器，资源的缓存时间</td>
</tr>
<tr>
<td>Referer</td>
<td>客户机通过这个头告诉服务器，它是从哪个资源来访问服务器的。（一般用于防盗链）</td>
</tr>
<tr>
<td>User-Agent</td>
<td>客户机通过这个头告诉服务器，客户机的软件环境</td>
</tr>
<tr>
<td>Cookie</td>
<td>客户机通过这个头告诉服务器，可以向服务器带数据</td>
</tr>
<tr>
<td>Refresh</td>
<td>服务器通过这个头，告诉浏览器隔多长时间刷新一次</td>
</tr>
<tr>
<td>Content-Type</td>
<td>服务器通过这个头，回送数据的类型</td>
</tr>
<tr>
<td>Content-Language</td>
<td>服务器通过这个头，告诉服务器的语言环境</td>
</tr>
<tr>
<td>Server</td>
<td>服务器通过这个头，告诉浏览器服务器的类型</td>
</tr>
<tr>
<td>Content-Encoding</td>
<td>服务器通过这个头，告诉浏览器数据采用的压缩格式</td>
</tr>
<tr>
<td>Content-Length</td>
<td>服务器通过这个头，告诉浏览器回送数据的长度</td>
</tr>
</tbody></table>
<h3 id="6-响应的编码"><a href="#6-响应的编码" class="headerlink" title="6. 响应的编码"></a>6. 响应的编码</h3><p>响应状态码</p>
<p>响应状态代码有三位数字组成，第一个数字定义了响应的类别，且有五种可能取值。<br>常见状态码：</p>
<table>
<thead>
<tr>
<th>号码</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>100~199</td>
<td>表示服务器成功接收部分请求，要求客户端继续提交其余请求才能完成整个处理过程</td>
</tr>
<tr>
<td>200~299</td>
<td>表示服务器成功接收请求并已完成整个处理过程。常用200（OK 请求成功）</td>
</tr>
<tr>
<td>300~399</td>
<td>为完成请求，客户需进一步细化请求。例如：请求的资源已经移动一个新地址、常用302（所请求的页面已经临时转移至新的url）、307和304（使用缓存资源）</td>
</tr>
<tr>
<td>400~499</td>
<td>客户端的请求有错误，常用404（服务器无法找到被请求的页面）、403（服务器拒绝访问，权限不够）</td>
</tr>
<tr>
<td>500~599</td>
<td>服务器端出现错误，常用500（请求未完成。服务器遇到不可预知的情况）</td>
</tr>
</tbody></table>
<h3 id="7-Ajax的请求获取数据"><a href="#7-Ajax的请求获取数据" class="headerlink" title="7. Ajax的请求获取数据"></a>7. Ajax的请求获取数据</h3><p>有些网页内容使用AJAX加载，而AJAX一般返回的是JSON,直接对AJAX地址进行post或get，就返回JSON数据了</p>
<h3 id="8-请求-SSL证书验证"><a href="#8-请求-SSL证书验证" class="headerlink" title="8. 请求 SSL证书验证"></a>8. 请求 SSL证书验证</h3><p>现在随处可见 https 开头的网站，urllib可以为 HTTPS 请求验证SSL证书，就像web浏览器一样，如果网站的SSL证书是经过CA认证的，则能够正常访问，如：<a href="https://www.baidu.com/" target="_blank" rel="noopener">https://www.baidu.com/</a></p>
<p>如果SSL证书验证不通过，或者操作系统不信任服务器的安全证书，比如浏览器在访问12306网站如：<a href="https://www.12306.cn/mormhweb/的时候，会警告用户证书不受信任。（据说" target="_blank" rel="noopener">https://www.12306.cn/mormhweb/的时候，会警告用户证书不受信任。（据说</a> 12306 网站证书是自己做的，没有通过CA认证）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 忽略SSL安全认证</span><br><span class="line">context &#x3D; ssl._create_unverified_context()</span><br><span class="line"># 添加到context参数里</span><br><span class="line">response &#x3D; urllib.request.urlopen(request, context &#x3D; context)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>02. 工具的使用</title>
    <url>/2020/01/02/python%E7%88%AC%E8%99%AB/02.%20%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h3 id="1-常用的工具"><a href="#1-常用的工具" class="headerlink" title="1. 常用的工具"></a>1. 常用的工具</h3><ol>
<li>python</li>
<li>pycharm</li>
<li>浏览器<ol>
<li>chrome</li>
<li>火狐</li>
</ol>
</li>
<li>fiddler</li>
</ol>
<h3 id="2-fiddler的使用"><a href="#2-fiddler的使用" class="headerlink" title="2 fiddler的使用"></a>2 fiddler的使用</h3><h4 id="2-1-操作界面"><a href="#2-1-操作界面" class="headerlink" title="2.1 操作界面"></a>2.1 操作界面</h4><p><img src="https://note.youdao.com/yws/api/personal/file/04B3654AAB1F479DB305CD78C4B921D1?method=download&shareKey=1f99688aca3238dde572683cfd8117f5" alt="image"></p>
<h4 id="2-2-界面含义"><a href="#2-2-界面含义" class="headerlink" title="2.2 界面含义"></a>2.2 界面含义</h4><p>请求 (Request) 部分详解<br>名称|含义<br>—|—<br>Headers | 显示客户端发送到服务器的 HTTP 请求的,header 显示为一个分级视图，包含了 Web 客户端信息、Cookie、传输状态等<br>Textview | 显示 POST 请求的 body 部分为文本<br>WebForms | 显示请求的 GET 参数 和 POST body 内容<br>HexView | 用十六进制数据显示请求<br>Auth | 显示响应 header 中的 Proxy-Authorization(代理身份验证) 和 Authorization(授权) 信息<br>Raw  | 将整个请求显示为纯文本<br>JSON | 显示JSON格式文件<br>XML | 如果请求的 body 是 XML格式，就是用分级的 XML 树来显示它</p>
<p>响应 (Response) 部分详解<br>名称|含义<br>—|—<br>Transformer | 显示响应的编码信息<br>Headers | 用分级视图显示响应的 header<br>TextView | 使用文本显示相应的 body<br>ImageVies | 如果请求是图片资源，显示响应的图片<br>HexView | 用十六进制数据显示响应<br>WebView | 响应在 Web 浏览器中的预览效果<br>Auth | 显示响应 header 中的 Proxy-Authorization(代理身份验证) 和 Authorization(授权) 信息<br>Caching | 显示此请求的缓存信息<br>Privacy | 显示此请求的私密 (P3P) 信息<br>Raw | 将整个响应显示为纯文本<br>JSON | 显示JSON格式文件<br>XML | 如果响应的 body 是 XML 格式，就是用分级的 XML 树来显示它</p>
<h4 id="2-3-设置"><a href="#2-3-设置" class="headerlink" title="2.3 设置"></a>2.3 设置</h4><h5 id="2-3-1-如何打开"><a href="#2-3-1-如何打开" class="headerlink" title="2.3.1 如何打开"></a>2.3.1 如何打开</h5><blockquote>
<p>启动Fiddler，打开菜单栏中的 Tools &gt;Options，打开“Fiddler Options”对话框</p>
</blockquote>
<p><img src="https://note.youdao.com/yws/api/personal/file/6F19B921EB4E4C1189A60496850A508A?method=download&shareKey=c2423cc6b90a149d8a99a4b39fde6417" alt="image"></p>
<h5 id="2-3-2-设置"><a href="#2-3-2-设置" class="headerlink" title="2.3.2 设置"></a>2.3.2 设置</h5><p><img src="https://note.youdao.com/yws/api/personal/file/8AF9AC2E5CC34325AFC19D275E73072E?method=download&shareKey=c04f2cc8d703e1691bd5a77454979e17" alt="image"></p>
<ul>
<li>Capture HTTPS CONNECTs 捕捉HTTPS连接</li>
<li>Decrypt HTTPS traffic 解密HTTPS通信</li>
<li>Ignore server certificate errors 忽略服务器证书错误</li>
<li>all processes 所有进程</li>
<li>browsers onlye 仅浏览器</li>
<li>nono- browsers only 仅非浏览器</li>
<li>remote clients only 仅远程链接</li>
</ul>
<p><img src="https://note.youdao.com/yws/api/personal/file/A1DFE233350E450484719C3822589D13?method=download&shareKey=385172dd2967ab6068d6f52599d2fba8" alt="image"></p>
<ul>
<li>Trust Root Certificate(受信任的根证书) 配置Windows信任这个根证书解决安全警告</li>
</ul>
<p><img src="https://note.youdao.com/yws/api/personal/file/A14209C410524F0ABCE292F8AEB2BD05?method=download&shareKey=bc5b1a7abec2f5fc820852f1025f84e8" alt="image"></p>
<ul>
<li>Allow remote computers to connect 允许远程连接</li>
<li>Act as system proxy on startup 作为系统启动代理</li>
<li>resuse client connections 重用客户端链接</li>
</ul>
<h4 id="2-4-捕获链接-抓包"><a href="#2-4-捕获链接-抓包" class="headerlink" title="2.4 捕获链接(抓包)"></a>2.4 捕获链接(抓包)</h4><ol>
<li>安装SwitchyOmega 代理管理 Chrome 浏览器插件</li>
<li>设置代理<br><img src="https://note.youdao.com/yws/api/personal/file/EC69D3EE90AA488885A5B62468C51BC1?method=download&shareKey=c7778db1166a2e451a3a0f6755238bad" alt="image"></li>
</ol>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>01.爬虫介绍</title>
    <url>/2020/01/01/python%E7%88%AC%E8%99%AB/01.%20%E7%88%AC%E8%99%AB%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<p><img src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1517157496430&di=9d8bbd8ae0eb17809b394f8936987b1c&imgtype=0&src=http%3A%2F%2Fimg.mukewang.com%2F57466ffc00015e2f06000338-590-330.jpg" alt="图片"></p>
<h3 id="1-什么是爬虫？"><a href="#1-什么是爬虫？" class="headerlink" title="1. 什么是爬虫？"></a>1. 什么是爬虫？</h3><p><strong>网络爬虫</strong>也叫<strong>网络蜘蛛</strong>，如果把互联网比喻成一个蜘蛛网，那么蜘蛛就是在网上爬来爬去的蜘蛛，爬虫程序通过请求url地址，根据响应的内容进行解析采集数据，<br>比如：如果响应内容是html，分析dom结构，进行dom解析、或者正则匹配，如果响应内容是xml/json数据，就可以转数据对象，然后对数据进行解析。</p>
<h3 id="2-有什么作用？"><a href="#2-有什么作用？" class="headerlink" title="2. 有什么作用？"></a>2. 有什么作用？</h3><p>通过有效的爬虫手段批量采集数据，可以降低人工成本，提高有效数据量，给予运营/销售的数据支撑，加快产品发展。 </p>
<h2 id=""><a href="#" class="headerlink" title=""></a><img src="https://note.youdao.com/yws/api/personal/file/734769369243449783AC0567A0239F3F?method=download&shareKey=2965ef70a4ffeacd7d29b0a3f3b76cd0" alt="image"></h2><h3 id="3-业界的情况"><a href="#3-业界的情况" class="headerlink" title="3. 业界的情况"></a>3. 业界的情况</h3><p>目前互联网产品竞争激烈，业界大部分都会使用爬虫技术对竞品产品的数据进行挖掘、采集、大数据分析，这是必备手段，并且很多公司都设立了<code>爬虫工程师</code>的岗位</p>
<hr>
<h3 id="4-合法性"><a href="#4-合法性" class="headerlink" title="4. 合法性"></a>4. 合法性</h3><p>爬虫是利用程序进行批量爬取网页上的公开信息，也就是前端显示的数据信息。因为信息是完全公开的，所以是合法的。其实就像浏览器一样，浏览器解析响应内容并渲染为页面，而爬虫解析响应内容采集想要的数据进行存储。</p>
<hr>
<h3 id="5-反爬虫"><a href="#5-反爬虫" class="headerlink" title="5. 反爬虫"></a>5. 反爬虫</h3><p>爬虫很难完全的制止，道高一尺魔高一丈，这是一场没有硝烟的战争，码农VS码农<br>反爬虫一些手段：</p>
<ul>
<li>合法检测：请求校验(useragent，referer，接口加签名，等)</li>
<li>小黑屋：IP/用户限制请求频率，或者直接拦截</li>
<li>投毒：反爬虫高境界可以不用拦截，拦截是一时的，投毒返回虚假数据，可以误导竞品决策</li>
<li>… …</li>
</ul>
<hr>
<h3 id="6-选择一门语言"><a href="#6-选择一门语言" class="headerlink" title="6. 选择一门语言"></a>6. 选择一门语言</h3><p>爬虫可以用各种语言写, C++, Java都可以, 为什么要Python?</p>
<p>首先用C++搞网络开发的例子不多(可能是我见得太少)<br>然后由于Oracle收购了Sun, Java目前虽然在Android开发上很重要, 但是如果Google官司进展不顺利, 那么很有可能用Go语言替代掉Java来做Android开发. 在这计算机速度高速增长的年代里, 选语言都要看他爹的业绩, 真是稍不注意就落后于时代. 随着计算机速度的高速发展, 某种语言开发的软件运行的时间复杂度的常数系数已经不像以前那么重要, 我们可以越来越偏爱为程序员打造的而不是为计算机打造的语言. 比如Ruby这种传说中的纯种而又飘逸的的OOP语言, 或者Python这种稍严谨而流行库又非常多的语言, 都大大弱化了针对计算机运行速度而打造的特性, 强化了为程序员容易思考而打造的特性. 所以我选择Python</p>
<hr>
<h3 id="7-选择Python版本"><a href="#7-选择Python版本" class="headerlink" title="7. 选择Python版本"></a>7. 选择Python版本</h3><p>有2和3两个版本, 3比较新, 听说改动大. 根据我在知乎上搜集的观点来看, 我还是倾向于使用”在趋势中将会越来越火”的版本, 而非”目前已经很稳定而且很成熟”的版本. 这是个人喜好, 而且预测不一定准确. 但是如果Python3无法像Python2那么火, 那么整个Python语言就不可避免的随着时间的推移越来越落后, 因此我想其实选哪个的最坏风险都一样, 但是最好回报却是Python3的大. 其实两者区别也可以说大也可以说不大, 最终都不是什么大问题. 我选择的是Python 3</p>
<hr>
<h3 id="8-爬虫基本套路"><a href="#8-爬虫基本套路" class="headerlink" title="8. 爬虫基本套路"></a>8. 爬虫基本套路</h3><ul>
<li>基本流程<ul>
<li>目标数据</li>
<li>来源地址</li>
<li>结构分析</li>
<li>实现构思</li>
<li>操刀编码</li>
</ul>
</li>
<li>基本手段<ul>
<li>破解请求限制<ul>
<li>请求头设置，如：useragant为有效客户端</li>
<li>控制请求频率(根据实际情景)</li>
<li>IP代理</li>
<li>签名/加密参数从html/cookie/js分析</li>
</ul>
</li>
<li>破解登录授权<ul>
<li>请求带上用户cookie信息</li>
</ul>
</li>
<li>破解验证码<ul>
<li>简单的验证码可以使用识图读验证码第三方库</li>
</ul>
</li>
</ul>
</li>
<li>解析数据<ul>
<li>HTML Dom解析<ul>
<li>正则匹配，通过的正则表达式来匹配想要爬取的数据，如：有些数据不是在html 标签里，而是在html的script 标签的js变量中</li>
<li>使用第三方库解析html dom，比较喜欢类jquery的库</li>
</ul>
</li>
<li>数据字符串<ul>
<li>正则匹配(根据情景使用) </li>
<li>转 JSON/XML 对象进行解析</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="9-python爬虫"><a href="#9-python爬虫" class="headerlink" title="9. python爬虫"></a>9. python爬虫</h3><ul>
<li>python写爬虫的优势<ul>
<li>python语法易学，容易上手</li>
<li>社区活跃，实现方案多可参考</li>
<li>各种功能包丰富</li>
<li>少量代码即可完成强大功能</li>
</ul>
</li>
<li>涉及模块包<ul>
<li>请求<ul>
<li><code>urllib</code></li>
<li><code>requests</code></li>
</ul>
</li>
<li>多线程<ul>
<li><code>threading</code></li>
</ul>
</li>
<li>正则<ul>
<li><code>re</code></li>
</ul>
</li>
<li>json解析<ul>
<li><code>json</code></li>
</ul>
</li>
<li>html dom解析<ul>
<li><code>beautiful soup</code></li>
</ul>
</li>
<li>lxml<ul>
<li>xpath</li>
</ul>
</li>
<li>操作浏览器<ul>
<li><code>selenium</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
]]></content>
      <categories>
        <category>爬虫学习</category>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>navicat试用期过期问题</title>
    <url>/2019/09/08/navicat%E8%AF%95%E7%94%A8%E6%9C%9F%E8%BF%87%E6%9C%9F%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>数据库工具：Navicat<br>试用期过期处理方法</p>
<h2 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h2><p>Linux下Navicat是通过脚本启动的，因此很容易在运行之前插一个脚本，由这个脚本来重置试用期<br>注意脚本是Python3的，每次运行均会重置试用期到14天，要求64位版Navicat<br>插在Navicat启动脚本之前即可</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">import os</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 试用时间重置的正则</span></span><br><span class="line">ps = (</span><br><span class="line">        re.compile(r<span class="string">'\[Software\\\\PremiumSoft\\\\Data\\\\\&#123;[^\&#125;]*\&#125;\\\\Info\].*?\n[^\[]*'</span>),</span><br><span class="line">        re.compile(r<span class="string">'\[Software\\\\Classes\\\\CLSID\\\\\&#123;[^\&#125;]*\&#125;\\\\Info\].*?\n[^\[]*'</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># user.reg 的路径</span></span><br><span class="line">regfile = os.path.join(os.environ[<span class="string">'HOME'</span>], <span class="string">'.navicat64'</span>, <span class="string">'user.reg'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则替换</span></span><br><span class="line">with open(regfile, <span class="string">'r+'</span>) as f:</span><br><span class="line">    regstr = f.read()</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> ps:</span><br><span class="line">        regstr = p.sub(lambda m: <span class="string">''</span>, regstr)</span><br><span class="line"></span><br><span class="line">    f.seek(0, 0)</span><br><span class="line">    f.truncate()</span><br><span class="line">    f.write(regstr)</span><br></pre></td></tr></table></figure>




<h2 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h2><p>Windows用的注册表，感觉略麻烦。</p>
<ol>
<li>关闭Navicat</li>
<li>Win + R，输入regedit回车</li>
<li>删除HKEY_CURRENT_USER\Software\PremiumSoft\Data</li>
<li>展开HKEY_CURRENT_USER\Software\Classes\CLSID</li>
<li>展开每一个子文件夹，如果里面只包含一个名为Info的文件夹，就删掉它。 </li>
</ol>
<p>原文链接：<a href="https://blog.csdn.net/yyx3214/article/details/79428582" target="_blank" rel="noopener">https://blog.csdn.net/yyx3214/article/details/79428582</a></p>
]]></content>
      <categories>
        <category>解决问题</category>
        <category>工具过期</category>
      </categories>
      <tags>
        <tag>Navicat</tag>
      </tags>
  </entry>
  <entry>
    <title>基于网页开发工具Jupyter</title>
    <url>/2019/07/07/%E5%9F%BA%E4%BA%8E%E7%BD%91%E9%A1%B5%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7Jupyter/</url>
    <content><![CDATA[<p>Jupyter Notebook是基于网页的用于交互计算的应用程序。其可被应用于全过程计算：开发、文档编写、运行代码和展示结果。</p>
<p>前提：安装完成并配置好环境</p>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>打开命令行，更改到想要执行的文件夹下</p>
<p>在终端中输入以下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure>

<p>执行命令之后，在终端中将会显示一系列notebook的服务器信息，同时浏览器将会自动启动Jupyter Notebook。</p>
<p>启动过程中终端显示内容如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">D:\workspace&gt;jupyter notebook</span><br><span class="line">[I 16:20:40.367 NotebookApp] Serving notebooks from <span class="built_in">local</span> directory: D:\workspace\blog\zhaoyifanBlog</span><br><span class="line">[I 16:20:40.367 NotebookApp] The Jupyter Notebook is running at:</span><br><span class="line">[I 16:20:40.369 NotebookApp] http://localhost:8888/?token=d6f7f5a9727a9f8e33529ec7884520e93a7583233778990f</span><br><span class="line">[I 16:20:40.373 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</span><br><span class="line">[C 16:20:40.425 NotebookApp]</span><br><span class="line"></span><br><span class="line">    To access the notebook, open this file <span class="keyword">in</span> a browser:</span><br><span class="line">        file:///C:/Users/%E7%BB%8F%E5%B9%B4/AppData/Roaming/jupyter/runtime/nbserver-1652-open.html</span><br><span class="line">    Or copy and paste one of these URLs:</span><br><span class="line">        http://localhost:8888/?token=d6f7f5a9727a9f8e33529ec7884520e93a7583233778990f</span><br></pre></td></tr></table></figure>

<p>在浏览器打开它给出路径即可操作</p>
<p>注意：之后在Jupyter Notebook的所有操作，都请保持终端不要关闭，因为一旦关闭终端，就会断开与本地服务器的链接，你将无法在Jupyter Notebook中进行其他操作啦。</p>
<p>新建的python文件可以单行运行（按下enter+shift）</p>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title>关于Session和Cookies</title>
    <url>/2019/07/07/%E5%85%B3%E4%BA%8ESession%E5%92%8CCookies/</url>
    <content><![CDATA[<p>Cookie通过在客户端记录信息确定用户身份，Session通过在服务器端记录信息确定用户身份。</p>
<h2 id="Cookies"><a href="#Cookies" class="headerlink" title="Cookies"></a>Cookies</h2><p>cookie的内容主要包括：名字，值，过期时间，路径和域。路径与域一起构成cookie的作用范围。</p>
<p>1）Name 和 Value 属性由程序设定,默认值都是空引用。</p>
<p>2）Domain属性的默认值为当前URL的域名部分，不管发出这个cookie的页面在哪个目录下的。</p>
<p>3）Path属性的默认值是根目录，即 ”/” ，不管发出这个cookie的页面在哪个目录下的。可以由程序设置为一定的路径来进一步限制此cookie的作用范围。</p>
<p>4）Expires 属性，这个属性设置此Cookie 的过期日期和时间。</p>
<h2 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h2><p>session是另一种记录客户状态的机制，不同的是cookie保存在客户端浏览器中，而session保存在服务器上。客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上，这就是session。客户端浏览器再次访问时只需要从该session中查找该客户的状态就可以了。session相当于程序在服务器上建立的一份用户的档案，用户来访的时候只需要查询用户档案表就可以了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>1、cookie数据存放在客户的浏览器上，session数据放在服务器上。</p>
<p>2、cookie不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗，考虑到安全应当使用session。</p>
<p>3、session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能，考虑到减轻服务器性能方面，应当使用cookie。</p>
<p>4、单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。</p>
<p>5、可以考虑将登陆信息等重要信息存放为session，其他信息如果需要保留，可以放在cookie中</p>
]]></content>
      <categories>
        <category>知识点</category>
      </categories>
      <tags>
        <tag>cookies</tag>
        <tag>session</tag>
      </tags>
  </entry>
  <entry>
    <title>创建博客</title>
    <url>/2019/03/16/%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<p>This theme is diaspora, I download it from <a href="https://hexo.io/themes/" target="_blank" rel="noopener">this website</a>.<br>Here are some information about how to use this theme.</p>
<h2 id="New-article-template"><a href="#New-article-template" class="headerlink" title="New article template"></a>New article template</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: My awesome title</span><br><span class="line">categories: </span><br><span class="line">    - 分类1</span><br><span class="line">    - 分类2</span><br><span class="line">tags: </span><br><span class="line">    - 标签1</span><br><span class="line">    - 标签2</span><br><span class="line">mp3: http://domain.com/awesome.mp3</span><br><span class="line">cover: http://domain.com/awesome.jpg</span><br><span class="line">---</span><br></pre></td></tr></table></figure>

<h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Create-a-new-page"><a href="#Create-a-new-page" class="headerlink" title="Create a new page"></a>Create a new page</h3><p>1.create a new page, give it a name</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new page categories</span><br></pre></td></tr></table></figure>

<p>2.edit this page</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">title: name</span><br><span class="line"><span class="built_in">type</span>: <span class="string">""</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>操作</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title>解决国内安装npm速度慢的问题</title>
    <url>/2019/03/16/%E8%A7%A3%E5%86%B3%E5%9B%BD%E5%86%85%E5%AE%89%E8%A3%85npm%E9%80%9F%E5%BA%A6%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>由于node安装插件是从国外服务器下载，受网络影响大，速度慢且可能出现异常。</p>
<hr>
<p>在安装时可以手动指定从哪个镜像服务器获取资源，我们可以使用阿里巴巴在国内的镜像服务器，命令如下（以安装webpack为例）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install -gd webpack --registry=http://registry.npm.taobao.org</span><br></pre></td></tr></table></figure>

<hr>
<p>只需要使用–registry参数指定镜像服务器地址，为了避免每次安装都需要–registry参数，可以使用如下命令进行永久设置：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm config --registry=http://registry.npm.taobao.org</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>解决问题</category>
      </categories>
      <tags>
        <tag>npm</tag>
        <tag>node</tag>
      </tags>
  </entry>
  <entry>
    <title>如何写md格式的文档</title>
    <url>/2019/03/16/%E5%A6%82%E4%BD%95%E5%86%99md%E6%A0%BC%E5%BC%8F%E7%9A%84%E6%96%87%E6%A1%A3/</url>
    <content><![CDATA[<p>编辑器Markdown是一种纯文本格式的标记语言。通过简单的标记语法，它可以使普通文本内容具有一定的格式。</p>
<h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><p>标题其实和HTML中的h系列很像，想要设置为标题的文字前面加#来表示<br>一个#是一级标题，二个#是二级标题，以此类推。支持六级标题。</p>
<p><em>注：标准语法一般在#后跟个空格再写文字</em></p>
<p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 这是一级标题</span><br><span class="line">## 这是二级标题</span><br><span class="line">### 这是三级标题</span><br><span class="line">#### 这是四级标题</span><br><span class="line">##### 这是五级标题</span><br><span class="line">###### 这是六级标题</span><br></pre></td></tr></table></figure>

<h3 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h3><h4 id="加粗"><a href="#加粗" class="headerlink" title="加粗"></a>加粗</h4><p>要加粗的文字左右分别用两个*号包起来</p>
<h4 id="斜体"><a href="#斜体" class="headerlink" title="斜体"></a>斜体</h4><p>要倾斜的文字左右分别用一个*号或者用_包起来</p>
<h4 id="斜体加粗"><a href="#斜体加粗" class="headerlink" title="斜体加粗"></a>斜体加粗</h4><p>要倾斜和加粗的文字左右分别用三个*号包起来</p>
<h4 id="删除线"><a href="#删除线" class="headerlink" title="删除线"></a>删除线</h4><p>要加删除线的文字左右分别用两个~~号包起来</p>
<h4 id="缩进"><a href="#缩进" class="headerlink" title="缩进"></a>缩进</h4><p>输入法换成全角，点两个空格即可。</p>
<h4 id="文字颜色"><a href="#文字颜色" class="headerlink" title="文字颜色"></a>文字颜色</h4><p>内置HTML</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;font color&#x3D;#00ffff size&#x3D;72 face&#x3D;&quot;黑体&quot;&gt;&lt;&#x2F;font&gt;</span><br></pre></td></tr></table></figure>
<h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><p>在引用的文字前加&gt;即可。引用也可以嵌套，如加两个&gt;&gt;三个&gt;&gt;&gt;<br>加空行就在上一个文本之后打两个以上空格。</p>
<h3 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h3><p>三个或者三个以上的 - 或者 * 都可以。</p>
<h3 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h3><p>语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![图片alt](图片地址 &#39;&#39;图片title&#39;&#39;)</span><br><span class="line"></span><br><span class="line">图片alt就是显示在图片下面的文字，相当于对图片内容的解释。</span><br><span class="line">图片title是图片的标题，当鼠标移到图片上时显示的内容。title可加可不加</span><br></pre></td></tr></table></figure>

<h3 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h3><p>语法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[超链接名](超链接地址 &quot;超链接title&quot;)</span><br><span class="line">title可加可不加</span><br></pre></td></tr></table></figure>

<h3 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h3><h4 id="无序列表"><a href="#无序列表" class="headerlink" title="无序列表"></a>无序列表</h4><p>语法：<br>用 - + * 任何一种都可以</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">- 列表内容</span><br><span class="line">+ 列表内容</span><br><span class="line">* 列表内容</span><br><span class="line"></span><br><span class="line">注意：- + * 跟内容之间都要有一个空格</span><br></pre></td></tr></table></figure>

<h4 id="有序列表"><a href="#有序列表" class="headerlink" title="有序列表"></a>有序列表</h4><p>语法：<br>数字加点</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">1.列表内容</span><br><span class="line">2.列表内容</span><br><span class="line">3.列表内容</span><br><span class="line"></span><br><span class="line">注意：序号跟内容之间要有空格</span><br></pre></td></tr></table></figure>

<h4 id="嵌套列表"><a href="#嵌套列表" class="headerlink" title="嵌套列表"></a>嵌套列表</h4><p>上一级和下一级之间敲三个空格即可</p>
<h4 id="任务列表"><a href="#任务列表" class="headerlink" title="任务列表"></a>任务列表</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-  [ ] 跑步</span><br><span class="line">-  [ ] 骑车</span><br><span class="line">-  [x] 吃饭</span><br><span class="line">-  [ ] 睡觉</span><br></pre></td></tr></table></figure>
<ul>
<li><input disabled="" type="checkbox"> 跑步</li>
<li><input disabled="" type="checkbox"> 骑车</li>
<li><input checked="" disabled="" type="checkbox"> 吃饭</li>
<li><input disabled="" type="checkbox"> 睡觉</li>
</ul>
<h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">表头|表头|表头</span><br><span class="line">---|:--:|---:</span><br><span class="line">内容|内容|内容</span><br><span class="line">内容|内容|内容</span><br><span class="line"></span><br><span class="line">第二行分割表头和内容。</span><br><span class="line">- 有一个就行，为了对齐，多加了几个</span><br><span class="line">文字默认居左</span><br><span class="line">-两边加：表示文字居中</span><br><span class="line">-右边加：表示文字居右</span><br><span class="line">注：原生的语法两边都要用 | 包起来。此处省略</span><br></pre></td></tr></table></figure>

<h3 id="使用LaTeX方法"><a href="#使用LaTeX方法" class="headerlink" title="使用LaTeX方法"></a>使用LaTeX方法</h3><p>参考<a href="https://guo365.github.io/study/Markdown.html#26" target="_blank" rel="noopener">https://guo365.github.io/study/Markdown.html#26</a></p>
]]></content>
      <categories>
        <category>写法</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
</search>
